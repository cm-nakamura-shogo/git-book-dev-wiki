# TabPFN

- https://arxiv.org/abs/2207.01848v3

```
12層、埋め込みサイズ512、フィードフォワード層の隠れサイズ1024、4頭注目のPFN Transformerのみを考慮しました。我々は、線形ウォームアップとコサインアニーリング（Loshchilov and Hutter, 2017）を備えたAdam optimizer（Kingma and Ba, 2015）を使用した。各トレーニングについて、3つの学習率{.001, .0003, .0001}のセットをテストし、最終的なトレーニング損失が最も低いものを使用しました。結果として得られたモデルは25.82Mのパラメータを含んでいる。

最終的なモデルは、512データセットのバッチサイズで18 000ステップの学習を行いました。つまり、我々のTabPFNは、9 216 000の合成されたデータセットで学習される。この学習は、8つのGPU（Nvidia RTX 2080 Ti）で20時間かかります。各データセットは1024の固定サイズであり、一様にランダムに学習と検証に分割しました。一般に、学習曲線は1000万データセット程度で平坦になる傾向があり、一般に非常にノイズが多いことが確認されました。おそらく、これは我々の事前処理で多種多様なデータセットが生成されるためと思われます。

TabPFNは、学習サイズが1024を超えるデータセットには向いていません。予測に時間がかかったり、信頼性が落ちたりする可能性がある。10kサンプル以上のデータセットを実行しないことをお勧めします。マシンがクラッシュする可能性があります（TabPFNの2次関数的なメモリスケーリングのため）。フィット関数にoverwrite_warning=Trueを渡して、実行するかどうか確認してください。

TabPFNは、合成データセットで事前学習されたオープンソースのTransformerベースのモデルです。TabPFNは、多くの小規模なデータセットにおいて、木ベースのモデルよりもうまく動作することが示されています。現在、1000行未満、100特徴、10クラス以下の小規模データセットでの分類で動作しています。
```