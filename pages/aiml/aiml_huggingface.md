# HuggingFace

## Dataset

æ§˜ã€…ãªã‚½ãƒ¼ã‚¹ã‚’æ‰±ã†ã“ã¨ãŒå¯èƒ½ã€‚

### csvãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰DatasetDictã‚’ä½œæˆã™ã‚‹ä¾‹

```python
import pandas as pd
from sklearn.model_selection import train_test_split
from datasets import Dataset, DatasetDict, ClassLabel

train_base_df = pd.read_csv("train.csv")
train_df, valid_df = train_test_split(train_all_df, test_size=0.1, 
    stratify=train_base_df["label"], random_state=SEED)

# reset_indexã—ãªã„ã¨indexãŒç‰¹å¾´é‡ã¨ã—ã¦èªè­˜ã•ã‚Œã‚‹
train_df = train_df.reset_index(drop=True)
valid_df = valid_df.reset_index(drop=True)

dataset_dict = DatasetDict({
    "train": Dataset.from_pandas(train_df),
    "valid": Dataset.from_pandas(valid_df),
})

# ClassLabelã‚¯ãƒ©ã‚¹ã«å¤‰æ›
class_label = ClassLabel(num_classes=2, names=["normal", "hate"])
dataset_dict = dataset_dict.cast_column("label", class_label)
```

- å‚è€ƒ
  - åŸºæœ¬çš„ãªä½¿ã„æ–¹ã¯ä»¥ä¸‹ã‚’å‚ç…§
  - [Huggingface Datasets å…¥é–€ (2) - ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®èª­ã¿è¾¼ã¿ï½œnpakaï½œnote](https://note.com/npaka/n/n17ecbd890cd6#9ZILg)
  - DatasetDictã«ã™ã‚Œã°ã¾ã¨ã‚ã¦å‡¦ç†ã™ã‚‹ã“ã¨ã‚‚å¯èƒ½ã¨ãªã‚‹ã€‚
    - [Huggingface datasets ã‚’ä½¿ã£ã¦ ã‚ªãƒªã‚¸ãƒŠãƒ«ãƒ‡ãƒ¼ã‚¿ã§NER - Qiita](https://qiita.com/CivFractal/items/f2f7d8972fa14b152ad4)
  - ãƒ©ãƒ™ãƒ«éƒ¨åˆ†ã¯ClassLabelã«ã—ã¦ãŠãã¨å¾Œã€…ä¾¿åˆ©ã§ã‚ã‚‹ã€‚
    - [How to create custom ClassLabels? - ğŸ¤—Datasets - Hugging Face Forums](https://discuss.huggingface.co/t/how-to-create-custom-classlabels/13650)

### ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶å‡¦ç†

dataset_dictã«å¯¾ã—ã¦mapã§tokenizerã§å‡¦ç†ã™ã‚Œã°input_ids(ãƒˆãƒ¼ã‚¯ãƒ³ç³»åˆ—)ã‚„attention_maskãŒå¾—ã‚‰ã‚Œã‚‹ã€‚

```python
from transformers import AutoTokenizer

model_name = "cl-tohoku/bert-base-japanese-whole-word-masking"
max_length = 77

# ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶
tokenizer = AutoTokenizer.from_pretrained(model_name)
def tokenize(batch):
    return tokenizer(batch["text"], padding="max_length", max_length=max_length, truncation=True)
dataset_dict = dataset_dict.map(tokenize, batched=True, batch_size=None)
```

ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã®å­¦ç¿’æ™‚ã¯æ³¨æ„ãŒå¿…è¦ã€‚

- [æ–°ã—ãæ—¥æœ¬èªBERTã®ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã‚’å­¦ç¿’ã™ã‚‹ã¨ãã¯ limit_alphabet ã«æ°—ã‚’ã¤ã‘ã‚ˆã†](https://zenn.dev/hellorusk/articles/4513d7aac5b2cd)

### éš ã‚Œæ¬¡å…ƒã®ãƒ™ã‚¯ãƒˆãƒ«ã‚’å¾—ã‚‹æ–¹æ³•

Datasetã‚¯ãƒ©ã‚¹ã‚’ã†ã¾ãã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã™ã‚‹æ–¹æ³•ãŒåˆ†ã‹ã‚‰ãªã„ãŸã‚ã€æ™®é€šã«rangeã§forã‚’å›ã™ã€‚

modelã¸ã®å…¥åŠ›æ™‚ã«ã¯tensorã«å¤‰æ›ãŒå¿…è¦ã§ã€è‹¥å¹²ç…©é›‘ã¨ãªã‚‹ã€‚

modelã¯ã€AutoModelForSequenceClassificationãªã©ã§ã¯ãªãã€æ™®é€šã«AutoModelã®æ–¹ã§ã€ãƒ˜ãƒƒãƒ‰ãƒ¬ã‚¹ãªã‚‚ã®ã‚’ä½¿ç”¨ã™ã‚‹ã€‚

å‡ºåŠ›ã¯ã€pooler_outputã‹ã‚‰å¾—ã‚‹ã€‚

pooler_outputã®ä»–ã«last_hidden_stateãŒã‚ã‚‹ãŒãã®é•ã„ã¯ã€pooler_outputã¯ã€last_hidden_stateã®ç³»åˆ—å…ˆé ­ã‚’ç·šå½¢å±¤(å…¥å‡ºåŠ›åŒã˜ãƒãƒ¼ãƒ‰)ã¨tanhã‚’é€šã—ãŸã‚‚ã®ã§ã‚ã‚‹ã€‚

ã¾ãŸã€last_hidden_stateã¯ã€è¤‡æ•°ã‚ã‚‹self-attentionã®æœ€çµ‚å±¤ã¨ã„ã†æ„å‘³ã§ã‚ã‚‹ã€‚(ç³»åˆ—ã®æœ€å¾Œã¨ã„ã†æ„å‘³ã§ã¯ãªã„ã®ã§æ³¨æ„)

```python
import torch
from tqdm import tqdm
from transformers import AutoModel

model_name = "cl-tohoku/bert-base-japanese-whole-word-masking"
model = AutoModel.from_pretrained(model_name)
model.eval()

batch_size = 100
vectors = torch.empty((0, 768))

train_ds = dataset_dict["train"]

with torch.no_grad():
    for i in tqdm(range(0, len(train_ds), batch_size), ascii=True):
        output = model(
            torch.tensor(train_ds[i:i+batch_size]['input_ids']),
            torch.tensor(train_ds[i:i+batch_size]['attention_mask']),
        )
        vectors = torch.concat([vectors, output.pooler_output], axis=0)
```

- å‚è€ƒè¨˜äº‹
  - [ã€å‚™å¿˜éŒ²ã€‘PyTorchã§é»’æ©‹ç ”æ—¥æœ¬èªBERTå­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã£ã¦ã¿ã‚‹ - Seitaro Shinagawaã®é›‘è¨˜å¸³](https://snowman-88888.hatenablog.com/entry/2020/08/21/055414)

### ãƒ¢ãƒ‡ãƒ«ã®configç¢ºèª

ä»¥ä¸‹ã§ç¢ºèªã§ãã‚‹ã€‚ãƒ˜ãƒƒãƒ‰æ•°ã‚„å±¤æ•°ã€éš ã‚ŒçŠ¶æ…‹æ•°ãªã©ã‚‚ç¢ºèªå¯èƒ½ã€‚

```python
model_name = "cl-tohoku/bert-base-japanese-whole-word-masking"
model = AutoModel.from_pretrained(model_name)
model.config
```

å‡ºåŠ›ã¯ä»¥ä¸‹

```
BertConfig {
  "_name_or_path": "cl-tohoku/bert-base-japanese-whole-word-masking",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "tokenizer_class": "BertJapaneseTokenizer",
  "transformers_version": "4.23.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 32000
}
```

### EvalPrediction

compute_metricsã‚’è‡ªä½œã™ã‚‹å ´åˆã«ã‚ãŸã£ã¦ãã‚‹ã‚¯ãƒ©ã‚¹ã€‚

ä¾‹ãˆã°ä»¥ä¸‹ã®ã‚ˆã†ã«ä½¿ç”¨ã™ã‚‹ã€‚

```python
    def compute_metrics(pred: EvalPrediction):
        labels = pred.label_ids
        preds = pred.predictions.argmax(-1)
        f1 = f1_score(labels, preds)
        recall = recall_score(labels, preds)
        precision = precision_score(labels, preds, zero_division=0)
        return {"f1_score": f1, "recall": recall, "precision": precision}
    
    # ....

    trainer = Trainer(
        model=model,
        args=training_args,
        tokenizer=tokenizer,
        train_dataset=dataset_dict["train"],
        eval_dataset=dataset_dict["valid"],
        compute_metrics=compute_metrics,
    )
```

å®šç¾©ã¯ã‚³ãƒ¼ãƒ‰ã®ä»¥ä¸‹ã‚’å‚ç…§ã€‚

- [https://github.com/huggingface/transformers/blob/v4.23.1/src/transformers/trainer_utils.py#L100](https://github.com/huggingface/transformers/blob/v4.23.1/src/transformers/trainer_utils.py#L100)

### SchedulerType

ã‚³ãƒ¼ãƒ‰ã®ä»¥ä¸‹ã§ã‚ã‹ã‚‹ã€‚

- [https://github.com/huggingface/transformers/blob/v4.23.1/src/transformers/trainer_utils.py#L355](https://github.com/huggingface/transformers/blob/v4.23.1/src/transformers/trainer_utils.py#L355)

```python
class SchedulerType(ExplicitEnum):
    LINEAR = "linear"
    COSINE = "cosine"
    COSINE_WITH_RESTARTS = "cosine_with_restarts"
    POLYNOMIAL = "polynomial"
    CONSTANT = "constant"
    CONSTANT_WITH_WARMUP = "constant_with_warmup"
```

ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ã€`"linear"`ãªã®ã§æ³¨æ„ãŒå¿…è¦ã€‚

### MTEB

[https://huggingface.co/blog/mteb](https://huggingface.co/blog/mteb)

- åŸ‹ã‚è¾¼ã‚€ãƒ™ã‚¯ãƒˆãƒ«ã‚’ç”Ÿæˆå¯èƒ½ãªäº‹å‰å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡ã™ã‚‹ãŸã‚ã®å¤§è¦æ¨¡ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã«ä½¿ãˆã‚‹ã€‚
- 112ã®è¨€èªã€8ã¤ã®ã‚¿ã‚¹ã‚¯ã€58å€‹ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§è©•ä¾¡ã™ã‚‹ã€‚
- å€‹åˆ¥ã«ä½¿ãˆã‚‹ã¨ã€ãƒ–ãƒ­ã‚°ã«æ›¸ãã¨ãã¯ã‹ã©ã‚‹ã‹ã‚‚ã€‚
