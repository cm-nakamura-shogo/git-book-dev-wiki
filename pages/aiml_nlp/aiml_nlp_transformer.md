# Transformer

## 論文

- [日本語 DeBERTa V2 モデルを公開（京都大）](https://huggingface.co/ku-nlp/deberta-v2-base-japanese)
  - コーパスは Wikipedia (3.2GB), CC100 (85GB) に加え OSCAR (54GB) を使っており日本語モデルの中では最大規模
    - [https://twitter.com/nobug5c9/status/1611636869963616258](https://twitter.com/nobug5c9/status/1611636869963616258)
  - ライセンスは、CC-BY-SA-4.0なので商用（ブログ）には使えそう


## 参考

- [Transformerの最前線 〜 畳込みニューラルネットワークの先へ 〜 - Speaker Deck](https://speakerdeck.com/yushiku/20220608_ssii_transformer)