# 2019-07-18_データ指向アプリケーションデザイン

## 第I部

## 1章 信頼性、スケーラビリティ、メンテナンス性に優れたアプリケーション

### 1.1 データシステムに関する考察

### 1.2 信頼性

- フォールトがあってもシステムが正しく動作できること

### 1.3 スケーラビリティ

- 負荷が増大したとしても優れたパフォーマンスを保てるようにするための戦略を備えていること

#### 1.3.1 負荷の表現

- ファンアウトとは
  - ファンアウトは電子工学からきた言葉
  - ある論理ゲートの出力に接続されているゲート数のこと。
  - 出力側のゲートは、接続されたすべての入力を駆動できるだけの電流を供給しなければなりません。
  - トランザクション処理システムにおいては、入力されたリクエストを処理するのに必要となる他のリクエスト数を指す。
- Twitterのアーキの例は面白い
  - ユーザのホーム画面へのリクエスト毎に最新のツイートを取得。
  - ユーザ毎にキャッシュを用意し、フォロワーの書き込みリクエストでそのキャッシュを更新

#### 1.3.2 パフォーマンスの表現

- パフォーマンスの表現方法
  - スループット（処理数/時間）、あるサイズのデータのバッチジョブ時間（時間）
    - 理想的にはバッチジョブ実行時間は、あるサイズのデータをスループットで割ったもの
    - 実際にはそれよりもスキュー（ワーカー毎にデータが均等に分布しない）や低速タスクの完了待ちがあり長くなる傾向
  - レスポンスタイム
    - レイテンシとレスポンスタイムは厳密には異なる。
    - レイテンシはリクエストが待っている時間で、レスポンスタイムはネットワークやキューイングを含めた遅延。
    - レスポンスタイムは分布を持つ（一定ではない）
    - 平均値よりも中央値の方がユーザへの影響は分かりやすい（50%以上のユーザにはこのレスポンスなど）
      - 単一のリクエストに対する計測なので、ユーザアクセスの際に複数リクエストがある場合は注意。
      - このようなバックエンドサービスでは、高いパーセンタイルでの値が特に重要（最も低速なものがユーザ体験となるため）
      - テイルレイテンシの増幅と呼ばれる。
    - p95、p99、p999などが重要な指標。パーセンタイル以上のものをテイルレイテンシと呼ぶ。
- Amazonの例
  - Amazonは内部的なサービスのレスポンスタイムに対する要件を99.9パーセンタイルで示している。
  - 特に処理に時間がかかるユーザーは、大量の購入をして大量のデータを持っているユーザーであり、重要な顧客であることが多い。
  - Amazonは、レスポンスタイムが100ミリ秒大きくなれば売り上げが1%下がることを観測している。
  - その一方99.99パーセンタイル値の改善は負担が大きすぎ、Amazonの目的から見て十分な利益を生まないだろうと考えた。
- ヘッドオブラインブロッキング（HOLブロッキング）
  - 低速なリクエストが少数あるだけでそれ以降のリクエストの処理が待たされること
- パーセンタイルの計算
  - パーセンタイルの計算はソートであるため、時間がかかる。
  - 最小限のCPUとメモリのコストの下で十分な質でパーセンタイルの概算値を計算できるアルゴリズムがあるので活用する。
    - forward decay[25]、t-digest[26]、HdrHistogram[27]など

#### 1.3.3 負荷への対処のアプローチ

- エラスティックなシステム
  - 負荷の増大を検知して自動的にコンピューティングリソースを追加できるシステムのこと。
  - 負荷の予測が難しい場合に役立ちますが、手動でスケールさせるシステムのほうが単純であり、運用時に予想外のことが起こることが少ないも。
- ステートレス・ステートフル
  - ステートフルなデータシステムを単一ノードから分散環境に移行させる場合は、多くの複雑さが生じる。
  - しかし近年は分散システムのツールは抽象化が改善されるにつれて、少なくともある種のアプリケーションに関してはこの通年が改善。
- 最適なアーキテクチャとは
  - 大規模な環境下で運用されるシステムのアーキテクチャは、通常アプリケーションに固有の度合いが強くなる。
  - 1つのサイズですべてのケースに適応できる（one-size-fits-all）スケーラブルなアーキテクチャは存在しない。
  - アーキテクチャは、どういった処理が頻繁に行われ、どういった処理がまれなのかという推定、すなわち負荷のパラメータの下に構築される。
  - プロダクトの初期段階では将来の負荷に応じてスケールすることよりも、機能に関するイテレーションを素早く行えることのほうが重要。
  - 固有とはいっても、通常それは汎用的なビルディングブロックから作り上げられ、なじみのあるパターンで組み合わされる。

### 1.4 メンテナンス性

- メンテナンスの際の苦痛を最小化するためのソフトウェアを設計することは可能。
- 3つの設計原理
  - 運用性（運用チームが扱いやすいように）
  - 単純性（エンジニアがシステムを理解しやすいように）
  - 進化性（システムを変更や要求の変化に対応しやすいように）

#### 1.4.1 運用性：運用担当者への配慮

- 言いたいことがよくわからんかった
- 理解できたのは、予想しやすい挙動、可視化、標準ツールに寄せるあたりか。

#### 1.4.2 単純さ：複雑さの管理

- 複雑となる要因
  - 状態空間の爆発
  - モジュール間の密な結合
  - 依存関係のもつれ
  - 一貫性のない命名や用語
  - パフォーマンス問題を解決するためのハック
  - どこかの問題を回避するための特別なケースへの対応
- 複雑さを取り除くための最も優れた手段の1つが抽象化
- しかし優れた抽象化を見いだすのはきわめて難しいこと

#### 1.4.3 進化性：変更への配慮

- アジャイルの手法に関する議論のほとんどは、ごく小さなローカルのスケールに焦点を当てている。
- 本書で探し求めるのは、もっと大規模なレベルのデータシステムにおいてアジリティを高めるための方法
  - Twitterのホームタイムラインの実装のような

#### まとめ

## 2章 データモデルとクエリ言語

- データモデル
  - おそらくソフトウェアを開発するにあたって最も重要な部分
  - データモデルで下位レイヤの複雑さを隠蔽
  - 多くのアプリケーションは、いくつものデータモデルのレイヤーを積み重ねていくことによって構築される
- データモデルの構成
  - オブジェクトやデータ構造
  - JSONやXMLドキュメント、リレーショナルデータベースのテーブル、あるいはグラフモデルなど
  - データベースがメモリ、ディスク、ネットワーク上でバイト列として表現する方法
  - ハードウェアエンジニアがバイト列を電流や光のパルス、電磁気などで表現する方法
- 2章では2番目のレイヤ、3章では3番目のレイヤについて説明

### 2.1 リレーショナルモデルとドキュメントモデル

- 最も知られているデータモデルはおそらくSQLのデータモデル
- 1970年代にEdgar Coddが提唱したリレーショナルデータモデルに基づいている
- リレーショナルデータベースの起源は、ビジネスデータの処理にある
- 登場前は
- リレーショナルモデルへの対抗馬は、その時々に多くのブームを生み出しましたが、長く続かなかった。
- 結果的にリレーショナルデータベースは非常に汎用性が高かった

#### 2.1.1 NoSQLの誕生

- 2010 年代に入って登場したリレーショナルモデルの支配を終わらせようとする最新の試みがNoSQL
- 元々はあるミートアップで目を引きやすいTwitterのハッシュタグとして使われただけのもの
- 今では多くの興味深いデータベースシステムが#NoSQLハッシュタグに関連づけられ、さかのぼってこれはNot Only SQL として解釈
- NoSQLデータベースの採用が広がった要因
  - 巨大なデータセットや優れた書き込みのスループットを含むスケーラビリティの要求
  - フリーでオープンなソフトウェア
  - リレーショナルモデルではうまくサポートされない特殊なクエリ
  - リレーショナルなスキーマの制約に対して、もっと動的で表現力に富むデータモデルの要求

#### 2.1.2 オブジェクトとリレーショナルのミスマッチ

- オブジェクトと、テーブル、行、列といったデータベースモデルとの間に不格好な変換レイヤーが必要（インピーダンスミスマッチ）
- リレーショナルモデルではあるユーザの情報を集めるのに、複数のテーブルにクエリして結合する必要がある。
- JSONのようなドキュメントモデルでは、これが１つのクエリで済み、かつツリー関係が明示的となる。
  - ローカリティに優れている

#### 2.1.3 多対一と多対多の関係

- region及びindustryを文字列ではなくidとして与える意味
  - 名前を変更する場合にidに紐づく文字列のみを修正すればよい
  - ユーザーにはドロップダウンリストを提示することで入力を統一できる
  - ID自体は人間にとっては何も意味を持たないことから、変更する必要が生じない
  - 間にとって意味のあるものはすべて、将来どこかで変更の可能性があり、それを複製しているものも同様。
- これはJSONのようなドキュメントモデルにはうまく適合しません。
  - ドキュメントデータベースでは、一対多のツリー構造には結合は必要なく、多くの場合は結合のサポートが弱い
  - アプリケーションの初期は結合が不要なドキュメントモデルに適合したとしても、機能が追加されていくにつれて、データ同士はつながりを持つ。
  - ドキュメントモデルは、一対多の関係をうまく扱うことができますが、多対多の関係を扱うことが難しい。

#### 2.1.4 ドキュメントデータベースは歴史を繰り返すのか？

- 階層モデルは古く、1968年からIBMのInformation Management System (IMS)で使用。
- IMSは一対多の関係をうまく扱うことができますが、多対多の関係を扱うことが難しく、結合もサポートしていない
- 階層モデルの制約を解決するために、多くの方法が提唱される
  - リレーショナルモデル（これはSQLとなり、世界を制覇しました）
  - ネットワークモデル（これは初期には多くの支持を集めましたが、次第に廃れていきました）
- ドキュメントデータベースは、1 つの面では階層モデルに回帰している
- とはいえ、多対一や多対多の関係の表現という点では、リレーショナルデータベースとドキュメントデータベースに基本的な違いはなく、どちらの場合でも、関連性のあるアイテムはユニークな識別子で参照される
- 今日までは、ドキュメントデータベースはCODASYLのたどった筋道を繰り返してはいません。

#### 2.1.5 今日のリレーショナルデータベースとドキュメントデータベース

- ドキュメントデータモデルのメリット
  - スキーマの柔軟性
  - ローカリティから来る優れたパフォーマンス
  - アプリケーションによってはデータ構造がアプリケーションのものに近い
- リレーショナルデータベース
  - 結合や多対一及び多対多の関係のサポートに優れています。

##### 2.1.5.1 アプリケーションのコードをシンプルにしてくれるデータモデルは？

- アプリケーションのデータがドキュメント構造になっているならばドキュメントモデルを用いるのは良い考え
  - リレーショナルでは細分化するため、不必要にコードが複雑になる。
- ドキュメントデータベースにおける結合のサポートが貧弱なことは、アプリケーションの特性によって、問題になることも、問題にならないこともある。
  - イベントとその発生時刻を記録して使う分析アプリケーションでは、多対多の関係が必要になることはないかもしれない
- アプリケーションが現実に多対多の関係を必要とするなら、ドキュメントモデルの魅力は薄れる
  - 非正規化によって結合の必要性を下げることはできますが、その場合コードには、データの整合性を保つための処理が増える。
- データ間の関係性が強い場合、ドキュメントモデルは扱いにくく、リレーショナルモデルは許容範囲であり、グラフモデルが最も自然。

##### 2.1.5.2 ドキュメントモデルにおけるスキーマの柔軟性

- ドキュメントモデルにおけるスキーマの柔軟性
  - 多くのドキュメントデータベースやリレーショナルデータベースにおけるJSONサポートはスキーマを強制しない。
  - リレーショナルデータベースにおけるXMLサポートでは、通常はオプションとしてスキーマの検証が行える
  - スキーマレスは誤解を招く表現。実際は暗黙のスキーマがあるが、データベースにより強制されないということ。
  - もっと正確な言葉を使うなら、これはスキーマオンリードである。
  - 静的型チェックと動的型チェックの支持者たちが論争を展開してきたように、データベースにおけるスキーマの強制も物議を醸す話題
- スキーマの変更に対するアプローチの違い
  - ドキュメントは、書き込み時に新しいフィールドを追加し、古いものを読み込んだ場合の対応をアプリケーションコードに実装
  - リレーショナルな場合は、マイグレーションの実行が必要
- スキーマオンリードのアプローチは、何らかの理由でコレクション中のアイテムの構造が統一されていない場合メリットがある
  - すなわちデータがヘテロジニアスな場合
- これはたとえば、以下のような場合
  - 数多くの種類のオブジェクトがあり、個別のテーブルに保存することが現実的ではないような場合。
  - データの構造が外部のシステムによって決定され、アプリケーション側からは制御できず、いつ変更されるかも分からないような場合。

##### 2.1.5.3 クエリのためのデータローカリティ

- データローカリティ（局所性）
  - アプリケーションが多くの場合ドキュメント全体にアクセスしなければならない場合メリットがある。
  - 複数のテーブルに分割されている場合、複数回のインデックスのルックアップが必要なため多くのディスクのシークや時間が必要
  - 一部にしかアクセスしない場合は全体をロードするために無駄になる可能性がある
  - そもそもローカリティを活かすために関連性のあるデータ同士をグループ化しておくという着想は、ドキュメントモデルだけのものではない

##### 2.1.5.4 ドキュメント及びリレーショナルデータベースの融合

- ほとんどのリレーショナルデータベースシステム（MySQLを除く）は、2000 年代の中頃からXMLをサポート
- その後、PostgreSQL、MySQL、DB2は同じようなレベルでJSONドキュメントをサポート
- ドキュメントデータベースにおいては、RethinkDB がリレーショナル的な結合をクエリ言語でサポート
  - MongoDBのドライバーの中にはデータベースの参照を自動的に解決してくれるが、クライアント側での結合なため低速
- リレーショナルデータベースとドキュメントデータベースは、時間の経過と共に類似性が高まっている

### 2.2 データのためのクエリ言語

- SQLは宣言的クエリ言語。
- 宣言的なクエリ言語では、求めるデータのパターン、すなわち結果が満たすべき条件、そしてデータをどのように変換するかだけを指定
- その目標をどのように達成するかは指定せず、データベースシステムのクエリオプティマイザに任せられる
- 宣言的な言語の魅力
  - 命令型のAPIに比べて簡潔で、扱いやすい
  - データベースエンジンの実装の詳細を隠蔽
- SQLでは機能面での制約が強いからこそ、データベースによる自動的な最適化の余地が大きくなる
- 宣言型の言語は並列実行に向いていることがよくある
  - 結果のパターンを指定するだけであるため、データベースがクエリ言語の並列実装を自由に利用できる。

#### 2.2.1 Web上での宣言的クエリ

- CSSはドキュメントのスタイルの指定するための宣言的言語である
- JavaScriptでは、命令的にスタイルを指定する。
- Web ブラウザでは、宣言的なCSSのスタイル指定を使うほうが、スタイルをJavaScript で命令的に操作するよりもはるかに優れている。
  - パフォーマンスを改善してくれるかもしれない新しいAPI の利点を活かそうとすれば、コードの修正が必要
  - ブラウザのベンダーは、互換性を損なうことなくCSSやXPathのパフォーマンスを改善できる

#### 2.2.2 MapReduceでのクエリ

- MapReduce は、大量のデータを多くのマシンにまたがってまとめて処理するためのプログラミングモデル
- MongoDBやCouchDBでは、大量のドキュメントに対して読み取りのみのクエリを実行する仕組みとしてMapReduceが利用可能
- 10章で詳しく記載し、ここでは、MongoDBでのMapReduceモデルの利用方法だけを簡単に述べる
- MapReduceは、宣言的なクエリ言語でも完全に命令的なクエリAPIでもなく、その中間のどこかに位置するもの

### 2.3 グラフ型のデータモデル

- リレーションモデルは多対多の単純なケースは扱えますが、データ同士のつながりが複雑になっていくと、データをグラフとしてモデル化するほうが自然
- グラフは、頂点（ノードやエンティティとも呼ばれます）と辺（エッジや関係、弧とも呼ばれます）という2 つのオブジェクトから構成される

#### 2.3.1 プロパティグラフ

- 伝統的なリレーショナルなスキーマでは表現しにくいことが表現可能。
  - 様々な国の様々な地域の構造
  - 国の中に国があるような歴史の気まぐれ
  - 様々な粒度のデータ(居住地は市であったり州であったり、など)
- 別の関係性を同時に表現することも可能
  - リンクに別のプロパティを付ければよい
- 進化性にも優れており、機能を追加する際に、グラフを拡張することでアプリケーションのデータ構造の変化を受け入れられる

#### 2.3.2 Cypherクエリ言語

- プロパティグラフ用の宣言的なクエリ言語
- グラフデータベースのNeo4j用に作成
- たとえばアメリカからヨーロッパへ移住したすべての人々の名前を返すといったようなクエリが可能
- 宣言的言語であるため、クエリを書くときには実行の詳細を指定する必要ない

#### 2.3.3 SQLでのグラフクエリ

- 同様のものをSQLでやる場合は困難が伴う。
  - リレーショナルデータベースでは、通常事前にクエリで必要になる結合が分かっている
  - グラフデータでは、結合の数は事前には決まらない。
- SQL:1999からは、こういったものを扱うためのWITH RECURSIVEという構文があるものの、使いにくさがある
- 同じクエリをあるクエリ言語(Cyper)では4行で書くことができるのに、他の言語(SQL)で書くのには29行必要
- すなわちユースケースが異なれば適切な設計のデータモデルも異なるということを示している

#### 2.3.4 トリプルストアとSPARQL

- おおよそプロパティグラフモデルと同じで、同じ概念を異なる用語で表現
- ただし、価値のある様々なツールや言語があるためここで取り上げる
- トリプルストアでは、すべての情報は非常に3つの部分からなる言明(主語, 述語, 目的語)の形で保存
- 主語は頂点に相当する
- 目的語は以下のいずれか
  - 主語の頂点のプロパティにおけるキーと値（例えば (lucy, age, 33) など）
    - プリミティブなデータ型（文字列、数値）の場合が主にこのケース
    - 述語がキー、目的語がValueになる
  - グラフ中の別の頂点（例えば(lucy, marriedTo, alain)など）
    - 述語がラベル（辺）、目的語が別の頂点になる

##### 2.3.4.1 セマンティックWeb

- 元々は関連していたのかな？
- セマンティックWeb自体は衰退したが、トリプルはアプリケーションの内部的なデータモデルとして優れている

##### 2.3.4.2 RDFデータモデル

- 前述したTurtle言語は、RDFデータのフォーマット。

##### 2.3.4.3 SPARQLクエリ言語

- SPARQLは、RDF データモデルを使うトリプルストアのためのクエリ言語
- SPARQLはCypher以前からあったものであり、CypherのパターンマッチングはSPARQLからとったものなので似ている。
- SPARQL Protocol and RDF Query Languageの略でスパークルと発音
- Cypherよりも簡潔に記述が可能で、冗長な括弧などの記号が不要。
- RDFデータモデルは、プロパティと辺を区別しないので、プロパティのマッチに同じ構文が使用可能。

#### 2.3.5 礎となったもの：Datalog

- Datalog は、SPARQLやCypher よりもはるかに古い言語でアカデミックな分野で広く研究されてきた
- その後のクエリ言語の礎になったもので重要
- Datalog のデータモデルは、トリプルストアのモデルに似ており、やや汎用的
- トリプルは(subject, predicate, object) という形ではなく、predicate(subject,object)と書く

### まとめ

- 歴史的には、まずデータは1 つの大きなツリーとして表現されました（階層モデル）
- 階層モデルは多対多の関係をうまく表現できなかったため、リレーショナルモデルが考案
- その後リレーショナルモデルがうまく適用できないアプリケーションに対して以下の方向に発展
  - ドキュメントデータベースは、ドキュメント間の関係がそれほど存在しないようなユースケースをターゲット
  - グラフデータベースは、あらゆるもの同士に関係が存在するようなユースケースをターゲット
- ドキュメントデータベースとグラフデータベースに共通しているのは、通常それらが保存するデータにスキーマを強制しないこと
- しかし、アプリケーションにおいてデータが特定の構造を持つものと考えられるなら、これは以下のいずれかでしかない
  - スキーマが明示的（書き込み時に強制される）なのか
  - 暗黙的（読み取り時に処理される）なのかの違いでしか
- とりあげていないデータモデルの例
  - 配列相同性検索を要するゲノムデータ
  - 素粒子物理学では数百ペタバイトのデータを扱うためのカスタムソリューションが必要
  - 全文検索などの情報抽出に適したモデル

## 3章 ストレージと抽出

- 2章ではアプリケーション開発者がデータベースにデータを渡す際のフォーマットと、それを再び取り出すための仕組みを見た。
- 3章では同じことをデータベース側の観点で述べる
- アプリケーション開発者は、保存と取り出しに関するデータベースの内部動作に留意すべき
  - 利用可能な数多くのストレージエンジンの中から、自分のアプリケーションに適したエンジンを選択できなければならないため
  - ワークロードに合わせてパフォーマンスをチューニングするためには、ストレージエンジンの内部動作の大まかな理解が必要

### 3.1 データベースを駆動するデータ構造

- 世界で最も単純なデータベース
  - key, valueストアのログ追記
  - ２つのbash関数db_set, db_getのみで実装
- 追記のみが行われるデータファイルであるログが多くのデータベースの内部で使われている
- レコードが多くなるとパフォーマンスが悪化し、ルックアップのコストがO(n)となる。
- 効率的に見つけるためには、別のデータ構造、すなわちインデックスが必要になる
- インデックスは、主たるデータから導出される追加のデータ構造
- インデックスは追加・削除が可能であり、データベースの内容には影響せず、クエリのパフォーマンスのみに影響がある。
- 追加のデータ構造を管理するため、特に書き込みの際にオーバーヘッドが生じる
- いかなる種類のインデックスも通常は書き込み速度が低下する、これは書き込み時にインデックスの更新が必要であるため
- うまく選択すれば、インデックスは読み出しのクエリを高速にすることが可能
- 上記のトレードオフのため、通常デフォルトですべてにインデックスを付けるようなことはしない

#### 3.1.1 ハッシュインデックス

- メモリ上のハッシュマップでkeyとバイトオフセットを保持して、参照時にハッシュマップ経由でアクセスする。
- keyがすべてメモリ上に収まっている必要がある。
- マージのところはよくわからなかった。書き込みは少なくともマージを待たないとダメなのではないかな…？
- 追記のみのログタイプは、無駄が多いようにも見えるが以下の利点がある。
  - シーケンシャルな書き込みのみで構成されるため、ランダム書き込みよりもはるかに高速
  - クラッシュの際のリカバリは、追記のみであればシンプルとなる。
  - 古いセグメントをマージすることでデータの断片化を避けられる
- 制約は以下
  - ハッシュテーブルがメモリ内に収まる必要があるため、キーが多くなると使えない。
  - 範囲に対するクエリの効率は悪い。

#### 3.1.2 SSTableとLSMツリー

- キーでソートする。ソート済み文字列テーブル（Sorted String Table : SSTable）。
- 以下の利点がある。
  - セグメントのマージをシンプルで効率的に行える（マージソートのアルゴリズムと似たような処理が可能）
  - 特定のキーを探す際に、すべてのキーをメモリに保持しておく必要がない
    - 例えば、handiworkというキーがなくてもhandbagとhandsomeというキーのオフセットは分かっていればその間を探せばよい。
    - いくつかのキーのオフセットを知るためにインメモリインデックスが必要になるが、疎で良い。
    - 数キロバイトごとに1 つのキーが分かっていれば十分
  - 疎なキーの間のキーペアを圧縮して保持可能
    - 読み取りのリクエストの処理では、要求された範囲内にある複数のキーと値のペアをスキャンが必要
    - 書き込む際にそれらのレコードをブロックとしてグループ化し、圧縮して保持する。
    - これによりディスク領域やIOの帯域制限が可能。

##### 3.1.2.1 SSTableの構築と管理

- ソートされた構造を管理方法
  - Bツリー（ディクス上）
  - red-blackツリー、AVLツリー（メモリ上）
- これらのツリー構造は多く存在し、キーを任意の順序で挿入し、ソートされた順序で読み出しが可能
- メモリ上に保持するための手法を採用し、これがmemtableと呼ばれる。
- 問題が1つだけあり、仮にデータベースがクラッシュしてしまうと、直近の書き込み（まだディスクに書き出されていないmemtableの内容）が失われる
- この問題を生じさせないように、別個のログを持っておき、すべての書き込みを即座に追記していく。
- このログはクラッシュ時のリストアのみに使用され、SSTableに書き込み済みのデータは廃棄可能

##### 3.1.2.2 SSTableからのLSMツリーの作成

- これまで述べてきたアルゴリズムは、LevelDBやRocksDBで使用されている。
- 同様にCassandraやHBaseでも使われており、これはどちらもGoogleのBigtableの論文から着想を得ている。
  - 論文内で初めて、SSTableやmemtableが登場した。
- この構造はlog-structuredファイルシステム上に構築され、Log-Structured Merge-Tree（LSMツリー）と名づけられている。
- ElasticsearchやSolrで使われる全文検索インデックスのエンジンであるLuceneでも同様の手法が使用されている。
  - キーを語（term）、値をその語を含むすべてのドキュメントのIDのリスト（ポスティングリスト）とするキー‐バリューの構造
  - 語からポスティングリストへのマッピングはSSTable に似たソート済みのファイルに保存
  - 必要に応じてバックグラウンドでマージする

##### 3.1.2.3 パフォーマンスの最適化

- 実際には数多くの細部の最適化が必要。
- LSMツリーのアルゴリズムは存在しないキーのルックアップに時間がかかる。
- これに対応するため、ブルームフィルター（bloom filter）が使われます
  - ブルームフィルタは、集合の内容についての概要を保持するメモリ効率の良いデータ構造
  - ブルームフィルタを使うと、あるキーがデータベース中に存在しないことが分かる
- SSTable のコンパクションとマージの順序とタイミングの決定には様々な戦略がある
  - サイズごと及び階層ごとのコンパクション
- ログの以下の課題を解決
  - データセットが利用可能なメモリ量よりもはるかに大きくなっても、LSMツリーはうまく動作
  - 範囲に対するクエリ（最小値と最大値の間にあるすべてのキーのスキャン）は効率よく実行可能
  - ディスクの書き込みはシーケンシャルになるので、LSMツリーはきわめて高い書き込みのスループットを支えられる

#### 3.1.3 Bツリー

- log-structuredインデックスは最も一般的なインデックスではない。
- 最も広く使われているインデックス構造はB ツリーである
- Bツリーはほとんどすべてのリレーショナルデータベースにおいて標準的なインデックスの実装で、非リレーショナルデータベースの多くでも使用
- Bツリーはキーと値のペアをキーでソートされた状態で保持します。
- そのため、キーと値のルックアップや範囲に対するクエリを効率よく処理できるが、設計思想は大きく異なる。
- この設計は下位層のハードウェアとの関係性が高くディスクもまた固定サイズのブロックを並べることによる。
- セグメント分割の違い
  - log-structured インデックスは、データベースを通常数メガバイト以上の可変サイズのセグメントに分割、常にセグメントをシーケンシャルに書き込み
  - Bツリーはデータベースを固定サイズのブロックあるいはページに分割、別のページを参照して階層構造でたどっていく
- ほとんどのBツリーの深さは3ないし4レベルに収まる
  - ページサイズが4KBで深さが4 レベルであり、分岐係数が500 のツリーは最大で256TBを保存できます
- 新しいキーを受け入れられるだけの空き領域がない場合は、そのページを半分だけが埋まった2 つのページに分割し、親のページを配下にできた新しい2つのキーの範囲にあわせて更新

##### 3.1.3.1 Bツリーの信頼性を高める

- 上書きによってページの場所は変化しません。すなわち上書きをしてもそのページへの参照はすべてそのままで良い。
- ただし複数のページが同時に更新に必要なケースがあるため、クラッシュするとインデックスが破損する可能性がある。
- これに対応させるためには、ディスク上に追加の構造化データ、すなわちwrite-ahead ログ（WAL、あるいはredo ログと呼ばれることもあります）を持たせる
- これは追記のみが行われるファイルで、Bツリーへのすべての変更内容を、ツリーそのもののページに反映させる前に書き込む
- クラッシュ後に回復する際には、このログを使ってBツリーを整合性の取れた状態に回復させる
- 複数のスレッドが同時にBツリーにアクセスする場合、十分に注意して並行処理を制御しなければ、スレッドからツリーが整合性が保たれていない状態に見える可能性がある

##### 3.1.3.2 Bツリーの最適化

- Bツリーは長い間使われてきたので多くの最適化の手法が発展してきた

#### 3.1.4 BツリーとLSMツリーの比較

- 簡単にいえば、LSMツリーは通常書き込みを高速に処理でき、Bツリーは読み取りが高速に行える
- 実際にはワークロードの詳細の影響を受けるため、自分自身のワークロードでテストする必要がある
- 参考
  - 【DB】B-TreeとLSM-Treeを学ぶためのリンク集 - 地方エンジニアの学習日記](https://ryuichi1208.hateblo.jp/entry/2022/12/29/232726)

##### 3.1.4.1 LSMツリーの利点

- Bツリーのインデックスでは、すべてのデータを最低でも2 回書かなければならない
  - write-aheadログとツリーの２下位
  - その他にも、複数のページやページ全体の更新のオーバーヘッドがある。
  - 複数回の書き込みが生じることの影響は、書き込みの増幅と呼ばれ、書き込み回数の上限があるSSDでは特に問題となる。
- LSMツリーはBツリーに比べて書き込みのスループットを高く保つことができます。
- LSMツリーは圧縮率を高めることができるので、しばしばBツリーよりもディスク上のファイルが小さくなります。
- 実は多くのSSDは、ファームウェアが内部的にlog-structured なアルゴリズムを使っている。
- なのでランダムな書き込みであっても、下位のストレージチップではシーケンシャルな書き込みになるようにしているので、ストレージエンジンの書き込みパターンの影響は低い。
- とはいえ、書き込みの増幅を抑えてフラグメンテーションを減らすことはSSDの場合にもメリットがある。

##### 3.1.4.2 LSMツリーのマイナス面

- コンパクションの処理が実行中の読み書きのパフォーマンスに影響する場合がある
- コンパクションにより書き込みのスループットが高くなってしまう
- 設定が十分に練られていなければ、コンパクションが追いつかなくなる可能性も
  - 追いつかない場合、調べなければならないセグメントファイル数が増大するため、読み取り速度も低下する
- この状況に陥っていないかを明示的にモニタリングして検出が必要
- Bツリーの利点は、それぞれのキーがインデックス中の1 カ所にしか存在しないことだが、LSMは複数置かれる可能性がある。
- このことから、Bツリーは強いトランザクションの提供したいデータベースにとって魅力的
- 多くのリレーショナルデータベースでトランザクションの分離はキーの範囲に対するロックを使っているため、Bツリーでは直接ロックをツリーにアタッチできる
- Bツリーはデータベースのアーキテクチャにしっかりと定着し、多くのワークロードで良いパフォーマンスを発揮するため、しばらくも廃れない。
- 新しいデータストアでは、log-structured インデックスの普及が進んでいる

#### 3.1.5 その他のインデックス構造

- ここまで論じたキー‐バリューインデックスは、リレーショナルモデルにおけるプライマリーキーのようなもの
- セカンダリインデックスもまた、非常に広く使われている
- セカンダリインデックスは、効率的に結合を処理するために活用される
- セカンダリインデックスは、キー‐バリューインデックスから容易に構築できますが、主な違いは、キーがユニークではないこと。
- これに対処する方法は2つある
  - インデックス内のそれぞれの値を（全文検索インデックス中のポスティングリストのように）マッチする行の識別子のリストにする
  - それぞれのキーに行の識別子を追加してキーをユニークにする
  - いずれの方法をとるにしても、Bツリーやlog-structured なインデックスをセカンダリインデックスとして利用可能

##### 3.1.5.1 インデックスへの値の保存

- キーに対する値はデータの実体か参照かの２種類。参照の場合、実際に保存されている場所はヒープファイルと呼ばれる。
- ヒープファイルのデータは順序付けされておらず、追記だけが行われるようになっていることもある。
- 複数のセカンダリインデックスがある場合にデータが複製されないよう、参照を保持するヒープファイルが利用されている。
- 状況によっては参照を繰り返す必要があるため、読み取りのパフォーマンスに影響を与えるケースもあり、データの実体を保存する方が望ましいケースもある。
- データの実態を保存するインデックスを、クラスタ化インデックスと呼ぶ。
  - MySQLのInnnoDBでは、プライマリキーは必ずクラスタ化インデックスとなり、セカンダリはヒープではなくプライマリキーへの参照となる。
- クラスタ化インデックスと非クラスタ化インデックスの中間のものもあり、カバーリングインデックス、付加列を持つインデックスと呼ばれる
- クラスタ化インデックスやカバーリングインデックスは読み取り速度を向上させるが、ストレージは余分に必要になり、書き込みのオーバーヘッドが増加、整合性を維持するための負担も増える。

##### 3.1.5.2 複合インデックス

- 連結インデックス
  - 複数の列を扱う最も一般的なインデックスは、連結インデックス
  - 単純に複数のフィールドを付け足して１つのキーとするもの。
  - 例えば、(姓, 名)から電話番号へのインデックスとなっている旧式の紙の電話帳のようなもの
- 多次元インデックス
  - 複数の列に対するクエリを処理するための方法としてさらに汎用性が高い方法
  - 特に地理空間データにとって重要
  - 緯度と経度を同時に範囲指定する場合、標準的なBツリーやLSMツリーインデックスは効率的に答えを返すことができない。
  - その他
    - コマースのWeb サイトで(赤, 緑, 青)という次元に対する3 次元のインデックスを使って特定の範囲内にある色の製品を検索する
    - 気象観測データのデータベースに(日付, 気温) というインデックスを持たせ、2013年の気温が25度から30度の間の日をすべて検索したりする

##### 3.1.5.3 全文検索と曖昧インデックス

- ここまで見てきたインデックスでは、似ているキーを検索することはできません。こういった曖昧なクエリには、別の手法が必要。
- たとえば全文検索エンジンでは、特定の編集距離内にある語をテキストから検索できるようになっている（Lucenesの例）
- Luceneのインメモリインデックスは、trie に似たキー内の文字に対する有限状態オートマトンである。
- このオートマトンは与えられた編集距離内にある語の効率的な検索をサポートするレーベンシュタインオートマトンに変換可能
- その他の曖昧検索の手法は、ドキュメントのクラシフィケーションや機械学習の方向に向かっており、詳細は情報抽出の教科書を参照。

##### 3.1.5.4 全データのメモリでの保持

- ここまで述べてきたデータ構造は、すべてディスクの限界に対する回答
- ディスクを利用するメリットは、永続性と容量当たりのコストが低いことだが、RAMが安価になるにつれてこの利点が小さくなってきている。
- 多くのデータセットはそれほど大きくないのでメモリ上にすべてを保持することも可能で、複数のマシンに分散させることもでき、これらがインメモリデータベースの発展につながる。
- インメモリデータベースはデータを永続化するためにディスクを使用するが、読み込みはすべてメモリから行われ、書き込みはログへの追記を行う。
- インメモリデータベースの例
  - VoltDB、MemSQL、Oracle TimesTen（リレーショナルモデルを持つインメモリデータベース）
  - RAMCloudはオープンソースのインメモリキー‐バリューストアであり、永続性を持っている
  - Redis やCouchbase は、ディスクへの非同期な書き込みによる弱い耐久性を提供
- 実際には、インメモリデータベースのパフォーマンス上のメリットは、ディスクから読み取りせずにすむことによるものではない
  - ディスクベースのストレージエンジンであっても、十分なメモリがあれば直近で使われたディスク上のブロックをオペレーティングシステムがメモリにキャッシュしているため
- インメモリデータベースが高速なのは、むしろメモリ内のデータ構造をディスクに書き込める形式にエンコードするというオーバーヘッドを回避できること
- パフォーマンス以外でインメモリデータベースの興味深い領域は、実装が難しいデータモデルが提供可能であること
  - たとえばRedis は、プライオリティキューや集合といった様々なデータ構造に対するデータベースのようなインターフェースを提供
- 不揮発性メモリ（non-volatile memory、NVM）の技術が広く使われるようになれば、おそらくストレージエンジンの設計にはさらなる変化が求められるため注視が必要

### 3.2 トランザクション処理か、分析処理か？

- 初期は、通常データベースへの書き込みは商取引に関連しており、トランザクションという言葉は現代に残った。
- トランザクションは、クライアントが低レイテンシで読み書きを行うということだけで、ACID特性は持っていなくても良い。
- OLTPとは
  - クライアントからの要求を受けたコンピュータがリアルタイムでトランザクション処理を行い、特定のレスポンスを返す」というデータ処理方法
  - 特定のレスポンスとは追加/更新/削除/変更といった動作を指す
  - 通常、スキャンは小数のレコードに対して行う。
- その一方で、データベースはデータ分析での利用も増えており、大量のレコードをスキャンする必要がある。
- OLAPとは
  - 蓄積された膨大なデータに対し、複雑な(多次元的な)集計・分析を行い素早くレスポンスを返す処理
- SQLはOLTP、OLAPどちらにもうまく機能した。
- 1990年代の諸島には、OLTPシステムを分析目的で使用するのをやめて、個別のDBで分析する流れが発生

#### 3.2.1 データウェアハウス

- OLTPシステムは企業経営にとって重要なので、高い可用性と低いレイテンシでのトランザクション処理が要求
- データベース管理者はそれを守るため、アドホックな分析クエリをOLTPデータベース上で実行させたがらない
- データウェアハウスは独立したデータベースであり、様々なOLTPシステムのデータコピーがリードオンリーで配置される
- ETLと呼ばれ以下の流れでDWHにロードされる
  - OLTPデータベースから取り出し（定期的なデータダンプや連続的な更新のストリームが使用）
  - 分析に適したスキーマに変換、クリーンアップ
  - DWHへのロード
- DWHは一般的に大企業が持っていることが多く、小さな企業は普通のデータベースで扱える程度の少量のデータしか持っていないためと考えられる。
- その他、DWHを使うメリットは、分析的なアクセスパターンにエンジンを最適化することが可能。

##### 3.2.1.1 OLTPデータベースとデータウェアハウスの相違点

- データウェアハウスのデータモデルはリレーショナルであることが一般的
- SQLが分析的なクエリに適しているため
- Teradata、Vertica、SAP HANA、ParAccel といったものがデータウェアハウスのベンダーとして有名
- Amazon RedShift はParAccel のホステッドバージョンらしい
- 最近では、オープンソースで大量のSQL-on-Hadoopプロジェクトが立ち上がっており、商用のDWHとの競争を目標としている
  - Apache Hive、Spark SQL、Cloudera、Impala、Facebook Presto、Apache Tajo、Apache Drillなど

#### 3.2.2 スターとスノーフレーク：分析のためのスキーマ

- トランザクション処理の領域ではアプリケーションの要求に応じて幅広く様々なデータモデルが使われている
  - 階層モデル、リレーショナル、グラフなど
- 分析分野ではそれほどの多様性はなくありふれたスタースキーマ（ディメンションモデル）を使用
- スキーマの中心にあるのはいわゆるファクトテーブル
  - 各行は、特定の時点で生じたイベントを表現
  - 例としては、「顧客による商品の購入」や「Webサイトのページビュー」、「ユーザのクリック数」など
- 通常、ファクトは後の分析を柔軟に行えるよう、個別のイベントとして扱われますが、このためファクトテーブルは巨大になる
- Apple、Walmart、eBay のような企業では、DWHは数十ペタバイトに達し、そのほとんどはファクトテーブル
- ファクトテーブルの列の中には販売価格や仕入れ価格などが含まれるが、その他はディメンジョンテーブルと呼ばれる他のテーブルを参照する外部キー
- ディメンジョンは、そのイベントの人物、対象、日時、場所、方法、理由といったものを示す。
- スノーフレークスキーマは、スターのディメンジョンがさらにサブディメンジョンに分割されたもの。
  - スノーフレークスキーマはスタースキーマよりも正規化されている
  - 分析する際はスタースキーマの方がシンプルなので個泊まれることがある。
- データウェアハウスでは、しばしばテーブルは大量の列を持ち、100以上あったり数百に及ぶこともある
  - 分析に関係する可能性のあるすべてのメタデータなどを含むため

### 3.3 列指向ストレージ

- ファクトテーブルはしばしば100 以上の列を持ちますが、典型的なクエリが一度にアクセスするのは、そのうちの4 ないし5 列程度
  - "SELECT *"は通常不要
- 多くのOLTPデータベースでは、ストレージは行指向でレイアウトされている
- 列指向ストレージの背景となる考え方はシンプルで、列に含まれるすべての値をまとめて保存する
- これにより、クエリに必要な列だけをディスクからロードすればよい
- 列ストレージはリレーショナルデータモデルで最も理解しやすいものの、非リレーショナルモデルにも同じように適用可能
  - たとえばParquet[57] はGoogleのDremel[54] を元にしており、ドキュメントデータモデルをサポートする列ストレージフォーマット
- 列ストレージで行全体を再構成しようとすれば、個々の列のファイルからの取り出しが必要

#### 3.3.1 列の圧縮

- データを圧縮すればさらにディスクに対する負担を下げられ、しかも列指向は圧縮に適していることが多い。
- データウェアハウスで特に効果的な手法の1つがビットマップエンコーディング
  - １つの列の中のユニークな値の数は行数に比べて小さい場合に有効
- ユニーク数が大きければ、大量の0が並んでいるだけとなる疎な状態であるため、この場合はさらにランレングスエンコーディングで圧縮可能
- ビットマップ間の論理演算（ORやAND）は効率的に行える（WHERE句のORやANDなどが効率的に処理できる）
- 実際には、データの種類に応じて多彩な圧縮スキームがあるが割愛。
- Cassandra とHBase には、Bigtable から受け継いだ列ファミリという概念があるが、列指向とは呼べない。Bigtableは行指向。
  - 列ファミリ内には行のすべての列がまとめて行キーとともに保存され、列は圧縮されない

##### 3.3.1.1 メモリの帯域とベクトル化処理

- 列指向ストレージのレイアウトはCPUサイクルの効率的な利用にも適してiいる
- 例えばのL1 キャッシュにうまく収まる圧縮された列のデータのチャンクを取り、密な（関数呼び出しのない）ループ内で処理を繰り返せます。
- 関数呼び出しや条件分岐がないループは、CPUにとってはるかに高速に処理できる
- ビット単位のAND やOR といった演算子は、圧縮された列のデータのチャンクを直接処理できるように設計可能
- これをベクトル化処理（vectorized processing）と呼ぶ

#### 3.3.2 列ストレージにおけるソート順序

- 列指向でもソート順序を指定することにはメリットが多い
- 管理者は、よく使われるクエリに関する知識に基づいて、テーブルのソート順序を決める列を選択することができる
- 日付の範囲を指定することが多いのであれば、date_keyを第一のソートキーとすることは妥当
- 第二の列では、最初の列に同じ値を持つ行のソート順序を決めらる（製品IDなど）
- ソート順序を指定するもう1 つの利点は、列の圧縮を助けられる
  - ユニークな値の数が多くないのであれば、ソート後には同じ値が連続して何度も繰り返される長い並びができる
  - これはランレングスエンコーディングがより効率的に機能する
- 圧縮の効果は、第一のソートキーで最も大きくなり、ソートの優先度が下がるとランダムになるため圧縮が効きにくくなる
- とはいえ、最初の数個の列をソートするのは全体としてみれば効率的

##### 3.3.2.1 複数のソート順序

- この概念をうまく拡張して列指向のストアに導入し、採用したのが商用データウェアハウスのVertica
- クエリが異なれば役立つソート順序も変わってくるので、同じデータをいくつかの異なる方法で保存すればよいという考え
- いずれにしても障害に対応するため、データは複数のマシンにレプリケーションが必要となる
- 列指向ストアで複数のソート順序を使うことは、行指向ストアで複数のセカンダリインデックスを使うことに少し似ている
- 大きな違い
  - 行指向のストアのセカンダリインデックスは単にマッチする行へのポインタを持っているだけ
  - 列指向ストアでは参照は一切ない

#### 3.3.3 列指向ストレージへの書き込み

- DWHの負荷の大部分は大規模な読み込みであるため、これらの最適化は理にかなっている
- ただし書き込みを難しくしてしまうという欠点がある。
- Bツリーが用いるようなインプレースでの更新は、圧縮された列では不可能
  - ソートされたテーブルの途中に行を挿入したい場合には、おそらくすべての列ファイルを書き直さなければならない
  - 行は列内での位置で識別されるので、挿入に際しては一貫性を保ちながらすべての列を更新が必要
- これに対してLSMツリーは有効に機能するソリューションとなる。
  - 実際にはインメモリストアで、ディスクへの書き込みに備える
  - ある程度蓄積されたら、ディスク上の列ファイルとマージする。
- クエリはディスク上の列データとメモリ内の直近の書き込みの両方を調べる必要があるが、クエリオプティマイザが実施するためユーザからは見えない

#### 3.3.4 集計：データキューブとマテリアライズドビュー

- マテリアライズドビュー
  - DWHで使われることのおおい集計などを実体化して保持しておくビュー
  - 仮想的なビューと違い実際にディスクに書き込まれる。
  - 元々のデータが更新されたら、そのコピーであるマテリアライズドビューも更新が必要
  - OLTPデータベースではそれほど頻繁には利用されない（書き込みの負荷が高くなるため）
- マテリアライズドビューの特化したケースで広く利用されているもの
  - データキューブ、あるいはOLAP キューブというもの
  - 様々なディメンションごとにグループ化された集計値のグリッド
  - ２次元だけではなく５次元のハイパーキューブも構成可能

### まとめ

- データベースによるストレージの扱いとデータの取り出しの最下層を学んだ
- 以下のようなまとめ
  - OLTP : ユーザが利用する膨大なリクエストを処理するシステム
    - log-structed : 追記のみのストレージエンジン
      - Bitcask、SSTable、LSMツリー、LevelDB、Cassandra、HBase、Luceneなど
    - Bツリー : インプレースで更新を行う更新可能な固定サイズのページの集合
      - 多くのリレーショナルデータベースで採用（一部、非リレーショナルでも使用例あり）
    - さらに複雑なインデックス構造
      - セカンダリインデックス、複合インデックス、全文検索、曖昧インデックスなど
  - DWH : アナリストが使用する分析システム

## 第4章 エンコーディングと進化

- 時間とともにアプリケーションは変化せざるを得ない
- 新旧バージョンのコードと新旧のデータフォーマットがシステム内で共存する可能性がある
- 互換性
  - 後方互換性 : 古いコードによって書かれたデータを新しいコードが読めること。
  - 前方互換性 : 新しいコードによって書かれたデータを古いコードが読めること。
- 後方互換性を実現するのは、それほど難しくないが（新しいコード作成時に対応が可能）、前方互換性は難しいことが多い。
- 本章では、JSON、XML、Protocol Buffers、Thrift、Avro を含む、データをエンコードする様々なフォーマットを見ていく。
- 特にそれらのフォーマットがどのようにスキーマの変更を扱い、新旧が混在するシステムをサポートするかに着目。
- また、通信としては、REST、RPC、アクター、メッセージキューなどのメッセージパッシングシステムを扱う。


### 4.1 データエンコードのフォーマット

- 通常プログラムは、データを（少なくとも）2つの異なる表現で扱う。
- インメモリの表現とバイトの並びの表現で、バイトの並びへの変換をエンコーディングと呼び逆をデコーディングと呼ぶ。
  - エンコーディングは、シリアライゼーションあるいはマーシャリングとも
  - デコーディングは、パース、デシリアライゼーション、アンマーシャリングとも
  - シリアライゼーションという用語は、残念ながらトランザクションの文脈においてもまったく異なる意味で使われるため注意が必要。

#### 4.1.1 言語固有のフォーマット

- 多くのプログラミング言語にエンコードする機能が組み込まれている（Pythonのpickleなど）
- しかしこれらのエンコーディングは特定のプログラミング言語と密接に関係しており、他の言語でデータを読むのが難しくなってしまうことがある
  - これは現在のシステムを他の組織（他の言語を使っている可能性のある組織）との結合を難しくする
- セキュリティ上の問題の発生源になることが頻繁にある
  - オブジェクト型でデータをリストアしようとするなら、デコーディングは任意のクラスをインスタンス化できる必要がある。
  - 仮に攻撃者がアプリケーションに任意のバイト列をデコードさせられるなら、それは攻撃者が任意のクラスをインスタンス化できる
  - すなわち任意のコードをリモートで実行するといったひどい行為を行える
- 前方及び後方互換性の不便な問題があることが多い
  - こういったライブラリでは、多くの場合データのバージョニングが後付けになっています。
  - これらのライブラリはデータを手早く簡単にエンコードできることを意図しているため
- 効率性が悪い場合がある
  - エンコードやデコードにかかるCPU時間やエンコード後の構造のサイズ）もまた多くの場合後付けで考えられている
- 以上の理由により、ごく一時的な目的の場合を除けば、特定の言語の組み込みエンコーディングを使うのは良くない考え。

#### 4.1.2 JSON、XML、様々なバイナリフォーマット

- 標準化されたエンコーディングはJSONやXMLが候補
- JSON、XML、CSV はテキストフォーマットなので人間が読めるフォーマットではあるが、いくつかの微妙な問題がある
  - 数値のエンコードにあいまいさがある
  - XMLとCSVでは、数値とたまたま数字だけから構成される文字列とを区別できない
  - JSONは文字列と数値を区別しますが、整数値と浮動小数点数値は区別せず、精度指定もできない
  - JSONとXMLはユニコード文字列（すなわち人間が読める文字列）のサポート機能が優れいるが、バイナリ文字列をサポーしていない
    - この制限を回避するためにBase64を使ってエンコードするという手法も使われるが、データサイズが33%増加してしまう。
  - CSVにはスキーマがないので、それぞれの列や行の意味の定義はアプリケーションに任されている
  - CSV自体があいまいなフォーマットでありエスケープのルールは正式に規定されているが、すべてのパーサで正しく実装されているわけではない
  - XML[11]とJSON[12]では、それぞれのフォーマット本体とは別に、利用できるスキーマがありますが煩雑で通常利用されない（XMLは利用されている）
- 欠陥はあるものの、JSON、XML、CSVは多くの目的を十分に満たすため、広く使用されている。
- 組織間で合意が形成されていればよく、、フォーマットの美しさや効率性は通常問題にならない。
- 何かについて複数の組織が合意するということは、他のほとんどの懸念事項よりも困難なことである

##### 4.1.2.1 バイナリエンコーディング

- 一般的にバイナリエンコーディングの方が冗長性が低くなるためテラバイト級のデータでは有効となってくる。
- そのため、JSONやXML用にバイナリエンコーディングが開発された。適した場所で使われているもののJSON、XMLほど広く採用されてはいない。
  - JSONにはMessagePack、BSON、BJSON、UBJSON、BISON、Smile
  - XMLにはWBXMLやFast Infoset
- MessagePack
  - 最もシンプルな例。オブジェクトとその数を示すヘッダ、後の値の種類と長さを示すヘッダ、実際の値という形で格納される。
  - あまり圧縮はされず、例の81byteが66byteになる程度。

#### 4.1.3 ThriftとProtocol Buffers

- Protocol Buffers はGoogle で、Thrift はFacebook で開発された
- 双方ともスキーマを必要し、IDL(インターフェース定義言語)でスキーマを記述する。
- スキーマ定義を受けて様々なプログラミング言語でコードを生成し、スキーマを実装したクラスを生成してくれるツールが提供される。
- Thrift には異なる2つのエンコーディングフォーマットがある
  - BinaryProtocol
  - CompactProtocol
  - 実際にはもっと種類があるが特定の言語の見であったりするため割愛
- BinarlyProtocol
  - フィールド名を数値（フィールドタグ）に置き換える形で表現
  - 59byteになる
- CompactProtocol
  - 種類とフィールドタグを1byteに詰め、整数はその大きさにより可変長byteで圧縮して無駄を削減する
  - 34byteになる
- Protocol Buffers
  - ほぼCompactProtocolと同じような形。
  - 33byteになる。

##### 4.1.3.1 フィールドタグとスキーマの進化

- Thrift やProtocol Buffersは、どのようにして後方及び前方互換性を保ちつつスキーマの変化を扱うか？
- エンコードされたデータの意味にとってフィールドタグがきわめて重要
- エンコードされたデータがフィールド名を参照することはないので、スキーマ中のフィールド名を変更することは可能
- その一方でフィールドのタグは変更できなくなっている。
  - タグを変更するとエンコード済みの既存の全データが不正なものになってしまうため
- 新しいタグ番号を使うかぎりにおいて、スキーマには新しいフィールドを加えていくことが可能
- 前方互換
  - 古いコードは新しいデータのフィールドタグをスキップすることでが可能
- 後方互換性
  - 新しいコードは常に古いデータを読むことが可能
    - それぞれのフィールドがユニークなタグ番号を持ってさえいれば、タグ番号の意味が変わることはないため
  - 注意が必要なのは、新しく追加するフィールドは必須にできないこと
    - 後方互換性を保つためには、スキーマを最初にデプロイした後に追加するすべてのフィールドを必須としないか、初期値を指定するかしかない。
- フィールドの削除も同様ですが、後方及び前方互換性の扱いが逆となる
  - 削除できるのは、オプションとして指定されるのみ
    - 必須なフィールドは、古いコードでは必須の可能性がある（前方互換）
  - 同じタグを使いまわすことはできない。（後方互換）
    - 古いデータを誤った形で読む可能性がある。

##### 4.1.3.2 データ型とスキーマの進化

- フィールドのデータ型の変更について
- 型を変更して解釈できる場合もあるが、精度が低くなったり切り捨てられたりする可能性がある。
- Protocol Buffersの細部で興味深いのは、リストや配列のデータを持たず、スキーマ定義にrequiredとoptionalに加えてrepeatedマーカがあること。
- repeatedが指定されたフィールドは、同じフィールドタグが複数回出現する。

#### 4.1.4 Avro

- Apach Avroもバイナリのエンコーディングフォーマット
- Apache Avro は、Thrift がHadoop のユースケースにうまく適合しなかったことを受けて、2009 年にHadoop の副プロジェクトとして立ち上がった
- Avro には2 つのスキーマ言語がある
  - 1 つは人間が編集することを意図したもの（Avro IDL）
  - もう1 つはもっと機械が読み取りやすいもの（JSONベース）
- Avroにはフィールドやフィールドのデータ型を示すものがないため、スキーマの定義順でエンコードされている必要がある。

##### 4.1.4.1 ライターのスキーマとリーダーのスキーマ

- Avroが進化性を扱うためには、ライターのスキーマとリーダ―のスキーマのスキーマ缶で互換性を解決している。

##### 4.1.4.2 スキーマの進化の規則

- Avro における前方互換性
  - 新しいバージョンのスキーマをライターとして持ち、古いバージョンのスキーマをリーダーとして持てる
- Avro における後方互換性
  - 新しいバージョンのスキーマをリーダーとして持ち、古いバージョンのスキーマをライターとして持てる
- 基本的には変わらないことを言っている。

##### 4.1.4.3 そもそもライターのスキーマとは何なのか？

- あるデータがエンコードされた際のライターのスキーマを、リーダーはどのようにして知るか
  - 大量のレコードを持つ大きなファイルの場合
    - これは特にHadoop で使われる場合に一般的なAvro のユースケースであり
    - この場合、そのファイルのライターがファイルの先頭に一度だけライターのスキーマを含めておくだけよい
  - 個別にレコードが書かれるデータベースの場合
    - 異なるライターのスキーマを使って様々な時点で様々なレコードが書かれるケース
    - エンコードされたすべてのレコードの先頭にバージョン番号を含め、スキーマのバージョンのリストをデータベース中に保存しておく
    - データベースからバージョン番号に応じたライターのスキーマをフェッチする
    - Espressoがこの方法を使っている
  - ネットワーク経由でレコードを送信する場合
    - 接続の確立時にスキーマのバージョンに関するネゴシエーションを行い、接続が続く間そのスキーマを使うことがで可能
    - Avro RPCプロトコルはこの方法で動作する
- 特に、スキーマバージョンのデータベースは有用
  - ドキュメンテーションとしての役割を果たし、スキーマの互換性をチェックできるようになることから、どのようなユースケースでも役立つ

##### 4.1.4.4 動的に生成されたスキーマ

- Avroが動的に生成されたスキーマとの相性が良い
  - リレーショナルなスキーマからAvro のスキーマを容易に生成できる
  - そのスキーマを使ってデータベースの内容をエンコードし、すべての内容をAvro のオブジェクトコンテナファイルにダンプ可能
  - データベースのスキーマが変更としてもデータデーブのスキーマから再生成すればよい
- ThriftやProtocol Buffersとの比較
  - ThriftやProtocol Buffersはスキーマが変更された場合、フィールドタグの割り当てを手作業でやる必要がある
  - 自動化できる部分はある者の、過去のフィールドタグを使わないなどの細心の注意が必要となる
- これは単にスキーマの動的生成はThrift やProtocol Buffers では設計目標ではなく、一方でAvroでは設計目標だったため

##### 4.1.4.5 コード生成と動的型付き言語

- Thrift とProtocol Buffers はコード生成（コンパイル）に依存しています。
- 動的に生成されたスキーマの場合、コード生成はデータへのアクセスに対する不必要な障害となる
- Avroは動的型付けの言語と相性が良い。
  - Avro では、コード生成をまったくしなくてもうまく利用できる
  - オブジェクトコンテナファイルにスキーマが埋め込まれているため、Avro のライブラリでそのファイルをオープンして、JSONファイルを同様に内容の確認が可能
  - このファイルは、必要なメタデータがすべて含まれていることから自己記述型となる
- この特性は、Apache Pig のような動的型付き処理言語とあわせて使われる場合に非常に有益
- Apache Pigでは、単純にAvro のファイルをオープンしてその内容を分析し、導出されたデータセットをAvro フォーマットのファイルに書き出す
- これをスキーマを意識することなく行える

#### 4.1.5 スキーマのメリット

- Protocol Buffers、Thrift、Avroはいずれもスキーマを使い、これらはより規則が複雑はXML SchemaやJSON Schemaによりはるかにシンプル
- シンプルゆえにきわめて広範囲のプログラミング言語でサポートされるまでに成長した
- これらのエンコーディングが基盤としている考え方は、決して新しいものではない。
  - たとえば1984 年に初めて標準化されたスキーマ定義言語であるASN.1は、これらと多くの共通点を持つ
  - ASN.1 は様々なネットワークプロトコルの定義に利用され、そのバイナリエンコーディング（DER）は、今でもSSL証明書（X.509）のエンコードに利用されている
  - ASN.1 はProtocol Buffers やThrift のようにタグ番号を利用したスキーマの進化をサポートしている
  - とはいえASN.1は非常に複雑でドキュメントも少ないので、現在の良い選択肢ではない。
- データシステムもまた、そのデータ用に何らかの独自のバイナリエンコーディングを実装している
  - 多くのリレーショナルデータベースは、独自のネットワークプロトコルでデータベースへのクエリの送信とレスポンスの受信を行える
  - これらはデータベース特有のものであるため、デコードしてインメモリのデータ構造に変換するドライバが、データベースベンダーから提供
    - たとえばODBCやJDBC APIが利用される
- JSON、XML、CSVといったテキスト形式のデータフォーマットは広く使われているものの、スキーマに基づくバイナリエンコーディングもまた有効な選択肢
- バイナリエンコーディングの優れた性質
  - 様々な「バイナリJSON」よりもはるかにコンパクトになりうる（フィールド名が不要なため）
  - スキーマはドキュメンテーションの形態の1つとしての価値がある
    - 手作業でメンテナンスされているドキュメントは容易に実体から乖離する
  - スキーマのデータベースを管理することで、スキーマの変更の前方及び後方互換性をデプロイに先立ってチェック可能
  - 静的型付き言語のユーザーにとっては、コンパイル時の型チェックができるようになる
- まとめると、スキーマの進化はスキーマレス/スキーマオンリードのJSONが柔軟性を維持しながらデータに関する保証を高め、より優れたツールを提供可能

### 4.2 データフローの形態

- 前章に登場したエンコーディングでデータをエンコードするのは誰で、デコードするのは誰となるのか
- これらのデータフローにおける一般的なケースをこの章では見ていく

#### 4.2.1 データベース経由でのデータフロー

- データベースへの保存処理は将来の自分自身へのメッセージ送信と考えられるため、後方互換が求められるのはあきらか。
- 今日では異なるプロセスが同時にアクセスするため、前方互換が求められることも頻繁にある
  - 異なるアプリケーションやサービスの場合
  - 同じサービスの複数のインスタンスからばローリングアップグレードで新しいバージョンのコードがデプロイされるケースなど
- それ以外でも、古いコードが新しいコードのデータを更新して書き戻す問題が発生しうる。
  - その場合に新しいフィールドが失われるなど。
- これはアプリケーション側で配慮する必要がある。

##### 4.2.1.1 異なる値が異なる時刻に書き込まれるケース

- データを新しいスキーマにあわせて書き直す（マイグレーションする）ことはできますが、ほとんどのデータベースではできるだけ避けている
  - 大規模なデータセットの場合は大きな負担になるため
- リレーショナルデータベースでは、デフォルト値をnull として新しい列を追加するといったシンプルなスキーマの変更は、既存のデータを書き換えることなく可能
- 古い行が読まれる際には、ディスク上のエンコード済みのデータに欠けている列があれば、データベースがnull を補う
- LinkedIn のドキュメントデータベースであるEspresso はストレージにAvro を使い、Avro のスキーマ進化のルールを利用できる

##### 4.2.1.2 アーカイブストレージ

- データベースからは、たとえばバックアップのため、あるいはデータウェアハウスへのロードのために時おりスナップショットをとることがある.
- この場合、仮にソースデータベース中のもとのエンコーディングに様々な時期のスキーマが混在していても、通常データのダンプは最新のスキーマでエンコードされる
- データのダンプは一度に書き出され、その後は変更されることがないので、この場合Avroのオブジェクトコンテナファイルのようなフォーマットが適している
- あるいはParquetのような分析に適した列指向フォーマットも、こういったデータのエンコードには適している

#### 4.2.2 サービス経由でのデータフロー：RESTとRPC

- ネットワーク経由での実施方法で、最も一般的なやり方は、クライアントとサーバーという2つの役割を設けること
- 転送プロトコルとしてはHTTPが使われることもありますが、最上位のAPI実装はアプリケーションに固有のものであり、そのAPIの詳細についてはクライアントとサーバーの合意が必要
- サーバーそのものも他のサービスのクライアントになることがある
  - 従来サービス指向アーキテクチャ（service oriented architecture、SOA）と呼ばれていましたが、最近では洗練され、マイクロサービスアーキテクチャと呼ばれている
- いくつかの面で、サービスはデータベースに似ている
  - 通常、クライアントからのデータの投入とクエリを受け付ける
  - とはいえ、サービスが公開するのはアプリケーション固有のAPIであり、許される入出力はそのサービスのビジネスロジック（アプリケーションコード）が事前に規定しているもののみ
  - この制約により、ある程度のカプセル化ができる
- サーバークライアント間のデータのエンコーディングは、サービスAPIのバージョン間では互換性を保つ必要がある
  - マイクロサービスアーキテクチャにおいて鍵となる設計目標は、それぞれのサービスを独立してデプロイし、進化させられるようにすること
  - 言い換えれば、新旧バージョンのサーバーとクライアントが混在して動作することが予想される

##### 4.2.2.1 Webサービス

- Webサービスに関して広く使われるアプローチには、REST とSOAP という2つがある
  - れらはその哲学という点ではほぼ完全に相反しており、それぞれの支持者の間でしばしば熱い論争の題材となる
- REST
  - REST はプロトコルではなく、むしろHTTPの原理の上に構築された設計哲学
  - REST で強調されているのはリソースの識別にURL を使うシンプルなデータフォーマット
  - キャッシュの制御、認証、コンテントタイプのネゴシエーションにHTTP の機能を使うこと
  - RESTは、少なくとも組織間のサービス結合という観点ではSOAPよりも広く使われるようになってきている
  - RESTの原理に基づいて設計されたAPIはRESTful と呼ばれる
- SOAP
  - ネットワークAPIのリクエストを発行するためのXMLベースのプロトコル
  - HTTP経由で使われることが最も一般的であるが、SOAPはHTTPから独立を保ち、HTTPの機能をほぼ使わないことを目標としている
  - その代わりに、SOAPには様々な機能を追加する、広範囲にわたる複雑な関連標準が多数ある（web service framework: WS-*とよばれる）
  - API は、WSDL（Web Service Description Language）と呼ばれるXMLベースの言語で記述される
  - WSDLを使うことでコード生成が可能になり、クライアントはローカルのクラスやメソッドによりサービスにアクセス可能となる
  - これは静的型付けでは有益だが、動的型付けではそれほど役に立たない
  - WSDLは人間が読めるようには設計されていないため、ベンダーにサポートされたIDEで開発する必要がある
  - SOAPとその様々な拡張は標準化されていますが、様々なベンダーの実装間での相互運用性は、しばしば問題を引き起こしています
  - SOAPは依然として多くの大企業で使われているものの、ほとんどの小規模な会社においては支持されなくなっています。
- RESTful APIではシンプルなアプローチが好まれる傾向があり、通常はコード生成や自動化されたツールの必要性が低い
  - OpenAPIのようなAPI定義のフォーマット（Swaggerとも呼ばれます[40]）を使えば、RESTful なAPIを記述し、APIのドキュメンテーションを生成も可能。

##### 4.2.2.2 リモートプロシージャコール（RPC）の問題


- Webサービスは長い歴史の最新の技術に過ぎず、これまでに様々なモノが提案されてきた
  - Enterprise JavaBeans（EJB）やJavaのRemote Method Invocation（RMI）
    - Javaに限定されたものでした。
  - 分散コンポーネントオブジェクトモデル（Distributed Component ObjectModel、DCOM）
    - マイクロソフトのプラットフォームに限定されていました。
  - Common Object　Request Broker Architecture（CORBA）
    - あまりに複雑であり、後方及び前方互換性を提供していませんでした
- これらはすべて1970年から存在するリモートプロシージャコール（RPC）の概念に基づいている
  - RPCモデルは、リモートにあるネットワークサービスへのリクエストの発行を、同一プロセス内でのプログラミング言語における関数やメソッドの呼び出しと同じように見せようとする
  - 一見便利なように見えるものの、ネットワーク越しのリクエストは、ローカルの関数呼び出しとは大きく異なるという根本的な問題がある
- 相違点の例
  - 利用者が制御できない部分に依存して成功や失敗が決まることがある
  - 制御が返ってこない場合に、何が起きているのか知る方法がない（リクエストが到達していたのか、レスポンスが失われたのかすらわからない）
    - このため、リトライした場合に重複して処理が行われる可能性がある
  - レイテンシが変動し予測できない
- これらが示すことは、基本的にリモートサービスとローカルオブジェクトは異なるため、同じように見せようとしすぎることは意味をなさないということ。
- RESTの魅力の一部は、それ自身がネットワークプロトコルであるという事実を隠そうとしていないことにある

##### 4.2.2.3 現在のRPCの方向性

- 問題があるにもかかわらず、RPCはなくなってはいません。
  - たとえばThriftやAvro にはRPCのサポートが含まれている
  - Protocol Buffers を使ったRPCの実装としてはgRPCがある
  - Finagle はThrift を、Rest.li はHTTP上でJSONを使っています。
- 新世代のRPCフレームワーク群は、リモートリクエストがローカルの関数呼び出しとは異なることを明確にするようになっている
  - たとえばFinagle やRest.li はfuture（promise）を使って、失敗するかもしれない非同期の動作をカプセル化する
  - gRPCはストリームをサポートしており、単一のリクエストやレスポンスだけではなく、時間軸上のリクエストとレスポンスの列から構成されるものも1回の呼び出しで扱える
  - フレームワークの中にはサービスディスカバリを提供しているものもある
- パフォーマンス上はRPCプロトコルの方が優位
  - バイナリのエンコーディングフォーマットを使うカスタムのRPCプロトコルは、REST 上のJSONのように汎用的なものよりも優れたパフォーマンスを発揮できる
- とはいえRESTfulの利点は以下があるため、公開APIのスタイルとしては支配的となっている
  - 実験とデバッグに適している
    - Webブラウザやcurl のようなコマンドラインツールさえあればリクエストを発行できる
    - コード生成や何かソフトウェアをインストールする必要がない
  - 主要なプログラミング言語やプラットフォームのすべてがサポートしている
  - ツールの巨大なエコシステムがある
    - サーバー、キャッシュ、ロードバランサ、プロキシ、ファイアウォール、モニタリング、デバッギングツール、テストツールなど
- PCフレームワークの焦点は、同一組織内のサービス間のリクエストになっており、この場合のやりとりは同じデータセンター内で行われることが普通

##### 4.2.2.4 RPCのデータエンコーディングと進化

- 進化性という点ではクライアントとサーバーに変化を加えることができ、独立してデプロイされることを考える必要がある
- クライアントとサーバーの場合、シンプルにすべてのサーバーがまずアップデートされ、クライアントはその後にアップデートされると考えてもよい
- したがって、保たなければならないのは
  - リクエストの後方互換性 : 古いリクエストを新しいサーバが処理できる
  - レスポンスの前方互換性 : 新しいレスポンスを古いクライアントが処理できる
- 互換性の持たせ方
  - Thrift、gRPC（Protocol Buffers）、Avro RPCは、それぞれエンコーディングフォーマットの互換性のルールに従って進化させることが可能
  - SOAPでは、リクエストとレスポンスはXMLスキーマとあわせて指定されます。このスキーマは進化させることができますが、見落としがちな落とし穴もある
  - RESTful APIでは、レスポンスにJSON（スキーマ指定なし）を、リクエストにはJSONあるいはURIエンコード/form エンコードされたリクエストパラメータを使うことが一般的
  - 通常、オプションのリクエストパラメータの追加やレスポンスオブジェクトへの新しいフィールドの追加は互換性を保つ変更と考えられる
- APIのバージョン管理
  - APIのバージョン管理をどのようにするべきかは、合意が成されていない
  - RESTful APIの場合、URL内かHTTP上のAcceptヘッダ内でバージョン番号を使うのが一般的なアプローチです。
  - クライアントの識別にAPIキーを使うサービスの場合、クライアントが要求したAPIのバージョンをサーバーに保管し、独立した管理用のインターフェースを通じて
    選択されたバージョンを更新できるようにしておくという選択肢もある

#### 4.2.3 メッセージパッシングによるデータフロー

- 非同期のメッセージパッシングシステムは、RPCとデータベースの中間のどこかに位置するもの
  - クライアントのリクエスト（通常はメッセージ）が他のプロセスに低いレイテンシで配送されるという点ではRPCに似ている
  - メッセージが直接のネットワーク接続を通じて送信されるわけではなく、一時的にメッセージを保存するメッセージブローカーを経由するという点ではデータベースに似ている
- RPCと比較したメッセージブローカ―の利点
  - システムの信頼性が向上
    - 受信側が動作していなかったり過負荷に陥っていたりする場合に、メッセージブローカーはバッファとして働く
    - 受信側のプロセスがクラッシュした場合にメッセージを再配信する
  - 接続先の抽象化
    - メッセージブローカーを使うことで、送信側は受信側のIP アドレスやポート番号を知る必要がない
    - 特にこれはクラウド環境で有益
    - 1つのメッセージを複数の受信側に送信可能
    - 送信側と受信側が論理的な分離
      - 送信側は単にメッセージを発行するだけであり、そのメッセージの受信者のことは関知しない
- RPCと比較した際の留意点
  - 通常メッセージパッシングによるコミュニケーションは単方向になる
  - 送信側はメッセージに対するリプライを受信しないものと考えるのが普通
  - プロセスからレスポンスを送信することはできますが、それは別個のチャネルを通じて行われ、非同期となる

##### 4.2.3.1 メッセージブローカー

- 過去は、商用エンタープライズソフトウェア
  - TIBCO、IBM WebSphere、webMethods
- 最近ではOSS実装が人気
  - RabbitMQ、ActiveMQ、HornetQ、NATS、Apache Kafka
- メッセージブローカーの概略
  - まずあるプロセスが名前付きのキュー、あるいはトピックに対してメッセージを送信
  - ブローカーは、キューもしくはトピックの1 つ以上のコンシューマもしくはサブスクライバにそのメッセージが届いたことを保証
  - 1 つのトピックには、多くのプロデューサやコンシューマが関わることがある
  - トピックが提供するのは、一方通行のデータフローのみだが、コンシューマ自体も他のトピックへメッセージを展開して連鎖する
  - 展開先に、元々の送信側のリプライキューへメッセージを展開する場合がある
  - 通常メッセージブローカーは、特定のデータモデルの利用を強制しない

##### 4.2.3.2 分散アクターフレームワーク

- アクターモデル
  - 並列処理プログラミングの一つで、ロックなしで並行処理が実行できる考え方。
  - 各アクターは受信したメッセージに基づいて処理を行う。
  - 場合によってはアクターは別のアクターにメッセージを送信する。これでアプリケーション全体の処理を実現する。
  - 非同期なメッセージの送受信によって他のアクターと通信し、メッセージが配信されることは保証されない。
  - それぞれのアクターは1 つのクライアントないしはエンティティを表現し、何らかのローカルな状態を持つことがあり、それを外部と共有しない
  - アクターのスケジューリングはアクターごとに独立にフレームワークが処理する
  - 本書ではそこまで詳しく書いてないため以下などを参考にした
    - [アクターモデルとアプリケーションアーキテクチャの関係 - nkty blog](https://yunkt.hatenablog.com/entry/2020/07/26/195022)
- 分散アクターフレームワーク
  - アプリケーションを複数ノードにわたってスケールさせるためにこのプログラミングモデルが用いらる
  - メッセージパッシングの仕組みは同じで、送信側と受信側が同じノードにあるか異なるノードにあるかは問題にならない。
  - アクターモデルでは、RPCよりも場所の透過性（ローカルかリモートサーバーかを意識しなくてよい性質）がうまく働く
    - レイテンシが変わるケースはあるが、ローカルであってもそもそも非同期なのでミスマッチが少なくなる。
  - 分散アクターフレームワークは、基本的にメッセージブローカーとアクタープログラミングモデルを1 つのフレームワークに統合したもの
- 互換性について
  - アクターモデルでも前方互換と後方互換を気にする必要がある。
  - これは、新しいバージョンが動作しているノードから古いバージョンが動作しているノードへメッセージが送られたり、その逆があったりするため
- フレームワーク毎の互換性の扱い
  - Akka
    - デフォルトでは、Javaに組み込まれているシリアライズを利用するため、互換性を提供しない
    - しかし知り洗いゼーションをProtocol Buffersのようなもので置き換えることが可能で、その場合はローリングアップデートも可能
  - Orleans
    - デフォルトでカスタムのデータエンコーディングフォーマットを使いますが、ローリングアップグレードでのデプロイをサポートしていない
    - アプリケーションのデプロイは、一定の手順を踏む必要がある
  - Erlang OTP
    - スキーマに変更を加えることは驚くほど難しい
    - ローリングアップグレードは可能ですが、注意深く計画することが必要。今後改善される可能性もある。

### まとめ

- 本章ではデータのエンコーディングとそれらの互換性について論じた。
- データフローの形態についても論じました。その中で、データのエンコーディングが重要になる様々な状況を紹介
  - DB、PRCとREST API、非同期のメッセージパッシング（（メッセージブローカーあるいはアクター）

## 第II部

- 第I部では主に単一のマシンにデータが保存されるようなデータシステムについて論じた
- 第II部では、データの保存と抽出に複数のマシンが必要なケースを見ていく
- 分散するケースの利点
  - スケーラビリティ、耐障害性・高可用性、レイテンシ

### II.1 高負荷に対応するスケーリング

- 共有メモリアーキテクチャ
  - 最もシンプルなアプローチは強力なマシンに変更すること（スケールアップ）。共有メモリアーキテクチャとも呼ぶ
- 共有メモリアーキテクチャの問題
  - コストの上昇が性能の向上に対して比例以上になっていくこと
  - 耐障害性が限定的になってしまう可能性がある
    - ホットスワップが可能なコンポーネントを使っても地理的には限定されてしまう
- 共有ディスクアーキテクチャ
  - 複数のマシンを使いながら、データはマシン間で共有されるディスクアレイに高速なネットワーク経由で保存
  - DWHのアーキテクチャで使用されることがありますが、このアプローチのスケーラビリティは競合やロック処理のオーバーヘッドのために制限される

### II.1.1 シェアードナッシングアーキテクチャ

- シェアードナッシングアーキテクチャ
  - 水平スケーリング、スケールアウトとも
  - 各ノードは独立に自身の持つCPU、RAM、ディスクを利用する
  - ノード間のあらゆる調整は、通常のネットワークを通じてソフトウェアのレベルで行われる
  - 特別なハードウェアは必要ないので、最善の価格対性能比を持つマシンを自由に選択可能
  - 複数の地域にまたがってデータを分散させることも可能
    - 1つのデータセンター全体が失われても動作し続けられるように構成することが可能
- 本書の第Ⅱ部では、シェアードナッシングアーキテクチャに焦点を当てる
  - あらゆるユースケースでの最善の選択だからというわけではない
  - むしろ開発者が非常に注意しなければならないものだから
  - データが複数のノードに分散されているなら、そういった分散システムに生じる制約やトレードオフを認識していなければならない
- 分散型のシェアードナッシングアーキテクチャの課題
  - 通常はそれと同時にアプリケーションが複雑になる
  - 利用できるデータモデルの表現力に制約が生じてしまうことがある
  - 場合によっては、シングルスレッドのシンプルなプログラムの方が、100 以上のCPUコアを持つクラスターよりもはるかに高い性能を発揮することも


### II.2 レプリケーションとパーティショニング

- 複数のノードにデータを分散させる一般的な方法
  - レプリケーション
    - 同じデータのコピーを複数のノードに保持する方法で冗長性を提供する
  - パーティショニング
    - 巨大なデータベースをパーティションと呼ばれる小さなサブセットに分割する方法
    - それぞれのパーティションは別々のノードに割り当てられます（これはシャーディングと呼ばれることもある）
  - これら２つは組み合わせて使用されることもある

## 5章 レプリケーション

- レプリケーションを行う理由
  - レイテンシのため（地理的にユーザの近くに設置するなど）
  - 可用性のため
  - 読み取り処理をスケールアウトさせるため
- 前提としては、各マシンに全体をコピーしておける程度にデータセットが小さいものとする
  - これの扱いが困難な場合にはパーティショニングという話となる。
- レプリケーションの難しさは、すべてレプリケーションされたデータへの変更の扱いから生じる
  - レプリケーション自体は容易であるため
- ほとんどの分散データベースはほぼ、シングルリーダー、マルチリーダー、リーダーレスという３つのアプローチいずれか
- レプリケーションには考慮すべきトレードオフがたくさんある
  - レプリケーションを同期的に行うのか非同期的に行うのか
  - あるいは障害を起こしたレプリカの扱いをどうするのか
- 結果整合性について詳しく見ていくとともに、書き込み後読み取り（read-your-writes）やモノトニック読み取り（monotonicreads）の保証についても論じる

### 5.1 リーダーとフォロワー

- すべてのデータがすべてのレプリカに行き渡っていることを保証するには、どうすればよいか
- この問題に対処する最も一般的な解決策がリーダーベースアプリケーション
  - アクティブ／パッシブあるいはマスター‐スレーブレプリケーションと呼ばれることも
- 動作内容
  - レプリカの1 つはリーダー（マスターあるいはプライマリ）に指定され、書き込む際はクライアントはリクエストをリーダに送信する
  - 他のレプリカはフォロワー（リードレプリカ、スレーブ、セカンダリあるいはホットスタンバイ）とよばれる
  - フォロワーへの反映は、その変更データをレプリケーションログあるいは変更ストリームの一部としてすべてのフォロワーに送信
  - 読み取りたい場合は、リーダーあるいはフォロワーにクエリが可能。書き込みはリーダーのみ。
- このレプリケーションが組み込まれているもの
  - 多くのRDBに組み込まれている（PostgreSQL（version 9.0以降）、MySQL、Oracle Data Guard[2]、SQL Server AlwaysOn 可用性グループなど）
  - 非リレーショナルデータベースでも使われる（MongoDB、RethinkDB、Espresso）
  - 分散メッセージブローカーもこの仕組みを採用（。Kafka[5]やRabbitMQ高可用性キュー[6]）

#### 5.1.1 同期と非同期のレプリケーション

- レプリケーションが同期的に行われるのか、それとも非同期的に行われるのか
- 同期型のレプリケーションの特徴
  - 利点はフォロワーが持っているデータが最新であり、リーダーとの一貫性が保証されること
  - 欠点は、フォロワーが反応を返さなかった場合に書き込みが処理できなくなってしまうこと
  - この欠点のため、すべてのフォロワーを同期型にするのは現実的ではない
- 準同期型（semi-synchronous）
  - 複数のフォロワーのうちの1 つを同期型にして、残りは非同期型にする
  - 同期型のフォロワーが利用できなくなったり、その速度が低下したりしたら、非同期型のフォロワーのいずれかを同期型にする
  - これによりリーダーと1 つの同期型フォロワーという2 つのノードに最新のデータのコピーがあることが保証される
- リーダーベースのレプリケーションは、しばしば完全に非同期で構成されることもある
  - この構成では、リーダーに障害が発生し、リカバリが不可能であれば、レプリケーションされていなかった書き込みはすべて失われる
  - すなわち、クライアントに成功が返された場合であっても書き込みの永続性が保証されない
  - とはいえ、完全に非同期の構成にはすべてのフォロワーに遅延が生じていてもリーダーが書き込みの処理を継続できるという利点がある
- 非同期型のレプリケーションのシステムでが、データを失うことなく、しかも高い性能と可用性を提供できるようなレプリケーションの手法についての研究が続けられている

#### 5.1.2 新しいフォロワーのセットアップ

- 新しいフォロワーがリーダーのデータの正確なコピーを持っていることは、どのようにすれば保証できるか
- クライアントは常にデータベースに書き込みを続けており、データは変化し続けているためそれに対処が必要
- 手順
  - ある時点でのリーダーのスナップショットを取得
  - これをフォロワーにコピーする
  - フォロワーはリーダーに接続してスナップショットされた後のすべてのデータ変更を要求
    - このため、スナップショットがリーダーのレプリケーションログのどの時点のものなのかを示す正確な位置が必要
    - これはPostgreSQLではログシーケンス番号、MySQLではbinlog coordinates といったように、様々な名前で呼ばれる
  - フォロワーがスナップショット取得以降のデータ変更のバックログを処理

#### 5.1.3 ノード障害への対処

- リーダーベースのレプリケーションにおいて高可用性を達成するにはどのようにすればよいか

##### 5.1.3.1 フォロワーの障害：キャッチアップリカバリ

- リカバリは容易。フォロワーは障害発生前に最後に処理したトランザクションをログから判断可能。

##### 5.1.3.2 リーダーの障害：フェイルオーバー

- フォロワーのいずれかをリーダーに昇格させなければならないため一定の手順が必要
  - リーダーに障害が起きたことの確認
    - タイムアウトなどが使われる
  - 新しいリーダーの選出
    - 通常リーダーに最もふさわしい候補は、それまでのリーダーのデータ変更に一番最近まで追従していたレプリカ
  - 新しいリーダーを使用するためのシステムの再設定
    - システムは、古いリーダーをフォロワーにさせ、新しいリーダーを認識させなければなりません。
- フェイルオーバーには、問題になりえることがたくさんある
  - 非同期のレプリケーションを使用する場合、新しいリーダーは以前のリーダーに障害が発生した時点までのすべての書き込みを受信していない可能性がある
  - たとえばGitHubのあるインシデント例
    - 同期が遅れていたMySQLのフォロワーがリーダーに昇格
    - DBは新しい行にプライマリキーを割り当てるために自動インクリメントのカウンタを使っていた
    - そのため、新しいリーダーのカウンタは古いリーダーよりも遅れていたので、古いリーダーが割り当て済みのプライマリキーをいくつか再利用
    - プライマリキーはRedis でも使われていたので、再利用されてしまったことによってMySQLとRedis との整合性が失われる
    - それによりプライベートなデータの一部が間違ったユーザーに公開された
  - ２つのノードが共に自身をリーダーだと信じてしまうことがあり得ます。（スプリットブレイン）
  - リーダーが落ちていると宣言するまでには、どの程度のタイムアウトが適切か
    - 長く取れば、リーダーに障害があった場合のリカバリに要する時間が長くなります。とはいえタイムアウトを短くしすぎれば、不要なフェイルオーバーが生じる
- こういった問題は簡単に解決できないため、運用チームによっては自動フェイルオーバーがサポートしされている場合でも、フェイルオーバーは手動で行うことを好むケースも。

#### 5.1.4 レプリケーションログの実装

##### 5.1.4.1 ステートメントベースのレプリケーション

- 最もシンプルなケースでは、リーダーは実行するすべての書き込みリクエスト（ステートメント）をログに記録し、そのステートメントログをフォロワーに送信する
- これは妥当なやり方のように思えますが、このレプリケーションのアプローチは様々な形で破綻する可能性がある
  - NOW() や乱数を取得するRAND() といった非決定的な関数を呼ぶステートメントは、それぞれのレプリカで異なる結果を生じさせる
  - ステートメントが自動インクリメント設定されている列を使う場合、それらはすべてのレプリカ上で完全に同じ順序で実行されなければならない
  - 副作用を持つステートメント（トリガ、ストアドプロシージャ、ユーザー定義関数など）は、副作用が完全に決定的なものでなければ、各レプリカで異なる副作用となる可能性がある
- 問題を回避することは可能
  - リーダーは非決定的な関数の呼び出しを記録する際にその文を確定した返値で置き換える
  - とはいえエッジケースは非常に数多く存在するので、今日では概して他のレプリケーションの手法が好まれている

##### 5.1.4.2 write-aheadログ（WAL）の転送

- 3章ででてきたように、すべての書き込みはログに追記される
- log-structured ストレージエンジンでも、Bツリーでもログはデータベースに対するすべての書き込みを含んでいる
- リーダーは、ログをディスクに書き込むかたわら、それをネットワーク経由でフォロワーに送信する。
- この手法は、とりわけPostgreSQL とOracle で利用されています
- 主な欠点は、ログはデータを非常に低レベルで記述していること
  - WALに含まれているのは、どのディスクブロック中のどのバイトが変更されたのか、といった詳細
  - つまり、レプリケーションがストレージエンジンと密接なつながりを持つことになる。
  - ストレージフォーマットのバージョンを変更した場合、通常リーダーとフォロワーでそのデータベースの異なるバージョンを動作させられなくなる
- レプリケーションプロトコル上、フォロワーがリーダーよりも新しいバージョンのソフトウェアを使うことが許されていない場合、アップデート時にダウンタイムが発生する

##### 5.1.4.3 論理（行ベース）ログレプリケーション

- レプリケーションやストレージエンジン用に独立したログのフォーマットを使うという手法
- この種のレプリケーションログは論理ログと呼ばれ、ストレージエンジンの（物理的な）データ表現とは区別される
- リレーショナルデータベースの論理ログは、通常データベースのテーブルへの書き込みを記述するレコードの並びであり、行を単位とします。
- 複数の行を変更するトランザクションは、そのようなログのレコードを複数生成し、その後にトランザクションがコミットされたことを示すレコードを生成する
- MySQLのbinlog はこのアプローチを採用している
- 論理ログは後方互換性を保ちやすくなり、リーダとフォロワーで異なるバージョン、ストレージエンジンを動作させられる。
- またこれは外部アプリケーションにもパースしやすいため、DWHを構築する場合に変更データのキャプチャが必要な場合にも使用できる

##### 5.1.4.4 トリガベースレプリケーション

- ここまでに述べたレプリケーションのアプローチは、データベースシステムによって実装されるもの
- 環境によってはもっと柔軟なものが求められ、その場合はアプリケーションレイヤでレプリケーションが必要
  - 一部だけをレプリケーションしたい場合
  - 異なる種類のデータベース間でレプリケーションを行いたい場合
- その際は、多くのリレーショナルデータベースが持っているトリガやストアドプロシージャといった機能を使う方法がある
  - トリガには、データベースシステム中でデータの変更（書き込みのトランザクション）が生じた場合に自動的に実行されるカスタムのアプリケーションコードを登録可能
    - たとえばOracle のDatabus[20] やPostgres のBucardo[21]

### 5.2 レプリケーションラグにまつわる問題

- 読み取りスケーリングアーキテクチャでは、フォロワーを追加するだけで読み取りのみを行うリクエストの処理容量を増やせる
- とはいえこのアプローチが現実的にうまくいくのは、非同期のレプリケーションの場合のみ
- アプリケーションが非同期のフォロワーから読み取りを行う場合に見える情報は、古い可能性がある
- これは一時的であるためいずれはフォロワーが追いつく。これは、結果整合性（eventual consistency）と呼ばれる。

#### 5.2.1 自分が書いた内容の読み取り

- ユーザーが書き込みを行った直後にそのデータを見た場合、新しいデータはまだレプリカに到達していない可能性がある
- これはユーザにとってはデータが失われたように見えるため望ましくない。
- これは、read-after-write 一貫性が失われた状態と呼ばれる。
  - 他のユーザーが行った更新は、ある程度後になるまで見えないかもしれません。しかし、ユーザー自身が行った入力が正しく保存されることは保証される
- read-after-write 一貫性を実装する方法
  - ユーザーが変更したかもしれないものを読み取る場合は、リーダーから読み取りを行い、それ以外のものはフォロワーから読み取るようにする
  - 最後に更新を行った時刻を追跡しておき、最新の更新から1 分以内に行う読み取りは、リーダーから行うようにする
- デスクトップのWeb ブラウザとモバイルアプリケーションといったように、同じユーザーが複数のデバイスを使ってサービスにアクセスする場合は、また別の複雑さが生じます。
- この場合は、クロスデバイスread-after-write 一貫性を提供することになる
  - ユーザーの最終更新のタイムスタンプを記憶しておくようなアプローチの難易度は高まります。このメタデータは、どこかに集中配置しておく必要がある

#### 5.2.2 モノトニックな読み取り

- 非同期のフォロワーから読み取る際に生じる異常の2 つめの例は、過去に遡って情報がユーザーに見えてしまうというもの
  - ラグの異なるフォロワーがある場合、さきほどは見えていた情報が、その後消えたように見える
- モノトニックな読み取りは、この種の異常が生じないことを保証する
- すなわち、以前に新しいデータを読まれていたら、それよりも古いデータが読まれるこ研がないようにする
- モノトニックな読み取りを実現する方法
  - 各ユーザーが常に読み取りを同じレプリカから行うようにする
  - レプリカをランダムに決めるのではなく、ユーザーIDのハッシュに基づいて決めるなど
  - とはいえレプリカに障害が起これば、そのレプリカから読み取りを行っていたユーザーのクエリは、他のレプリカにルーティングせざるを得ない

#### 5.2.3 一貫性のあるプレフィックス読み取り

- 一貫性のあるプレフィックス読み取りが保証するのは、ある順序で一連の書き込みが行われた場合、それらの書き込みを読み取る者には必ず書き込まれた際と同じ順序でみえる
- 多くの分散データベースでは、それぞれのパーティションは独立して操作されるので、全体としては書き込みは順序づけされない
- データベースから読み取りを行った場合、データベースの古い状態の部分と新しい状態の部分が見える可能性がある
- 解決策の1 つは、互いに因果関係を持つ書き込みが必ず同じパーティションに書かれるようにすることだが、効率的に実現することが難しい場合もある
- 因果関係を明示的に追跡するようなアルゴリズムもある

#### 5.2.4 レプリケーションラグへの対処方法

- 結果整合性のシステムを扱う場合、レプリケーションのラグが数分、さらには数時間にまで大きくなった場合のアプリケーションの振る舞いについて考えておくべき
- しかし、アプリケーションの開発者がレプリケーションの微妙な問題を気にする必要なく、単純にデータベースが「適切に処理を行う」ものと信じられれば、その方が良い
- これがトランザクションの存在意義
  - トランザクションは、アプリケーションをシンプルにするために、データベースが強い保証を提供する方法
- シングルノードのトランザクションは長い間存在してきたが、分散化に伴って多くのシステムがトランザクションを諦めた
- スケーラブルなシステムでは結果整合性が不可避であるという主張もおおくされたが、これはある面では真実ですが、あまりに単純化しすぎている

### 5.3 マルチリーダーレプリケーション

- リーダーベースレプリケーションモデルの自然な拡張は、複数のノードで書き込みが受け付けられるようにすること
- レプリケーションの働きは変わりません。書き込みを処理する各ノードは、データ変更を他のすべてのノードに転送します。これはマルチリーダー構成という。
  - マスター‐マスター、あるいはアクティブ／アクティブレプリケーションと呼ばれることも
- この構成では、各リーダーは同時に他のリーダーのフォロワーとしても振る舞う

#### 5.3.1 マルチリーダーレプリケーションのユースケース

- 得られるメリットが加わる複雑さを超えることがほとんどないので、単一のデータセンター内でマルチリーダー構成を使う意味はほとんどない

##### 5.3.1.1 マルチデータセンターでの運用

- マルチリーダー構成では、リーダーをそれぞれのデータセンターに置くことが可能
  - パフォーマンス
    - 書き込みの際のレイテンシが小さくできる
    - データセンター間のネットワークの遅延はユーザーから隠蔽されるので、ユーザーが感じるパフォーマンスは向上する可能性がある
  - データセンターの障害に対する耐性
  - ネットワークの問題に対する耐性
    - 通常データセンター間のトラフィックは、公開インターネットを経由することになるので、データセンター内のローカルネットワークよりも信頼性が低い可能性がある
- マルチリーダーレプリケーションにはメリットはあるものの、大きな欠点も
  - 異なる2 つのデータセンターで並行して同じデータが変更されることがありえる
  - それらの書き込みの衝突を解決しなければならない
- マルチリーダーレプリケーションは、多くのデータベースにおいて何らかの形で後付けされた機能なため様々は問題が発生しうる
  - 自動インクリメントのキー、トリガ、整合性制約など
- そのため、しばしばマルチリーダーレプリケーションは可能であれば避けるべき危険地域と見なされる

##### 5.3.1.2 オフラインで運用されるクライアント

- マルチリーダーレプリケーションが適切な状況としては、インターネットに接続されていないときも、アプリケーションが動作し続けなければならないようなケース
- たとえば携帯電話やノートPC、あるいはその他のデバイス上でのカレンダーアプリケーション
  - この場合素bてのでばーすはリーダのようにふるまい、ローカルにデータベースをもつ
- アーキテクチャの観点から見れば、基本的にこの構成はデータセンター間でのマルチリーダーレプリケーションと同じであり、それを極端にしたものに過ぎない
- 破綻しているカレンダー同期の実装の例には事欠かないことが示すとおり、マルチリーダーレプリケーションを正しく動作させるのは難しいこと
- この種のマルチリーダー構成を容易にするためのツールもある
  - たとえばCouchDBは、こういった運用形態を念頭に置いて設計されている

##### 5.3.1.3 コラボレーティブな編集

- リアルタイムコラボレーティブ編集アプリケーションでは、複数のユーザーがドキュメントを同時に編集可能
  - たとえばEtherpad[30] やGoogle Docs[31] 
- これは先ほど述べたオフライン編集のユースケースと多くの点が共通している
- 編集の衝突がないことを保証したいなら、アプリケーションはドキュメントを変更できるようにする前にロックを取らなければならない

#### 5.3.2 書き込みの衝突の処理

- マルチリーダーレプリケーションにおける最大の問題は、書き込みが衝突しうること

##### 5.3.2.1 同期の衝突検出と非同期の衝突検出

- マルチリーダー構成ではどちらの書き込みも成功し、衝突は後のどこかの時点で非同期的にしか検出されない
- 衝突の検出を同期的に行うこともできるが、マルチリーダーレプリケーションの主たるメリットは失われる

##### 5.3.2.2 衝突の回避

- あるレコードに対する書き込みが同じリーダーに送られることをアプリケーションが保証できるなら、衝突が生じることはない
- とはいえ障害が生じた場合は、あるレコードに対するリーダーの指定を変更したいこともあるため、その場合は結局対処が必要

##### 5.3.2.3 一貫した状態への収束

- マルチリーダー構成では、書き込みに定まった順序がないので、最終的な値がどのようになるべきなのかははっきりしない
- それぞれのレプリカが単純に受け付けた順序で書き込みを適用していけば、データベースは一貫性を欠いた状態になってしまう可能性があり、これは許容されない。
- いかなるレプリケーションのスキームでも、最終的にはすべてのレプリカでデータが同じにならなければならない
- データベースは収束（convergent）する方法で衝突を解決しなければならない
- 収束的な衝突の解決方法
  - それぞれの書き込みにユニークなIDを与え、最後の書き込みを勝たせる（Last write wins : LWW）
    - このアプローチは広く使われていますが、データ損失の危険が大きい手法
  - それぞれのレプリカにユニークなIDを与え、IDの値が大きいレプリカから発行された書き込みを、IDの値が小さいレプリカからの書き込みよりも優先
    - これもデータロスの危険がある
  - 何らかの方法で値をマージする。たとえば、それらをアルファベット順に並べ、連結する
  - すべての情報を保存するデータ構造を明示的に持って衝突を記録し、後のどこかの時点で衝突を解決するようなアプリケーションコード（おそらくはユーザーに確認を求める）

##### 5.3.2.4 カスタムの衝突解決ロジック

- 衝突をどう解決するのが最も適切なのかはアプリケーションによるので、多くのツールでは衝突解決ロジックをアプリケーションのコードで書ける

##### 自動的な衝突の解決

- 衝突の自動解決については、興味深い研究がなされてきました。注目に値するものを以下に紹介しましょう
- 衝突のない複製データ型（Conflict-free replicated datatypes、CRDTs）[32, 38] 
  - 集合、マップ、順序付きリスト、カウンタといったデータ構造のファミリであり、複数のユーザーから並行して編集可能であるとともに、実用的な方法で自動的に衝突を解決する
- マージ可能な永続データ構造（Mergeable persistent data structures）
  - Git のように履歴を明示的に追跡し、3 ウェイマージ機能を使います（これに対し、CRDTは2ウェイマージ）。
- 操作変換（Operational transformation）[42]
  - Etherpad[30] やGoogle Docs[31] のようなコラボレーティブ編集アプリケーションを背後で支える衝突解決アルゴリズム
- データベースにおけるこれらのアルゴリズムの実装はまだ日が浅いものの、将来的にはレプリケーションを行うさらに多くのデータシステムに組み込まれていくことになると予想
- 将来的にはマルチリーダーのデータ同期はアプリケーションにとってはるかにシンプルに扱えるようになるかも

##### 5.3.2.5 衝突とは何か？

- 衝突には、もう少し検出しにくい種類のものもある
- たとえばミーティングルームの予約システム

#### 5.3.3 マルチリーダーレプリケーションのトポロジー

- レプリケーションのトポロジーは、あるノードから他のノードへと書き込みが伝播していく通信の経路を示すもの
- all-to-all型
  - 最も一般的なトポロジー
  - すべてのリーダーが自身で受け付けた書き込みを他のすべてのリーダーに送る
  - 最も制約の強いトポロジー
- 循環型トポロジー
  - それぞれのノードは1 つのノードから書き込みを受信し、それらの書き込みを（自身が受け付けた書き込みがあればそれも加えて）他の1 つのノードに転送
  - MySQLがサポートしているのはこのトポロジーのみ
- スター型トポロジー
  - ルートに指定された1 つのノードが、書き込みを他のすべてのノードに転送
  - スター型のトポロジーは、ツリー型に一般化できる
- 循環型やスター型のトポロジーで問題になるのは、たった1 つのノードの障害によって流れが阻害され、回復するまで他のノード同士のやりとりができなくなってしまうこと
- 高密度に接続されるトポロジーは、様々な経路でメッセージを流せることから単一障害点を持たずにすむので、耐障害性が高い
- all-to-all 型トポロジーの問題は、一部のレプリケーションメッセージが他のメッセージを「追い越し」してしまうこと
- これは、「5.2.3 一貫性のあるプレフィックス読み取り」で取り上げたものに似た、因果律の問題である
- これらのイベントを正しく順序づけするためには、本章で後ほど論じるバージョンベクトルと呼ばれる手法が利用される
- とはいえ、多くのマルチリーダーレプリケーションシステムにおける衝突検出手法の実装は貧弱
  - 本書の執筆時点ではPostgreSQL BDRは書き込みの因果関係づけの機能を提供していない
  - MySQLのTungsten Replicator は衝突を検出しようとさえしない

### 5.4 リーダーレスレプリケーション

- リーダーという概念を捨て、どのレプリカも直接クライアントからの書き込みを受け付けられるようにしているもの
- 初期のデータシステムの中にはリーダーレスだったものがったが、Amazon が自社のDynamo システムで利用した後に再び流行のデータベースアーキテクチャとなる
- Riak、Cassandra、Voldemort はDynamoから着想を得たリーダーレスレプリケーションモデルを持つオープンソースのデータストア
- そのためDynamoスタイルと呼ばれる
- リーダーレスの実装の中には、クライアントが書き込みを直接複数のレプリカに送信するものもあれば、コーディネータノードがクライアントの代わりに送信を行うものもある
- とはいえ、リーダーデータベースとは異なり、このコーディネータは書き込みの順序を強制しない
- そのため、この設計上の違いはデータベースの利用方法に大きな違いを生む

#### 5.4.1 ノードがダウンしている状態でのデータベースへの書き込み

- リーダーレス構成にはフェイルオーバーはない
- １つがアップデート中などで利用不可能なケース
  - クライアントは、書き込みを並行にすべてのレプリカに送信し、利用可能な2 つのレプリカはその書き込みを受け付け、利用不能なレプリカはその書き込みを逃す
  - 書き込みを承認するには3 つのレプリカのうち2 つで十分だとする
  - クライアントは、レプリカのうちの1つが書き込みを逃したことは単純にする
  - そのため、古いままのノードからの読み取りを行えば、そのレスポンスの内容は古い値となる
  - これを解決するために、読み取りリクエストも複数のノードに並列に送信する
  - クライアントは、すべてのノードから同じレスポンスを受け取るとは限らない
  - どちらの値が新しいのかを調べるにはバージョン番号が使用される

##### 5.4.1.1 読み取り修復と反エントロピー

- レプリケーションスキームは収束しなければならない。どのようにして逃した書き込みを取り戻すのか
- Dynamoスタイルのデータストアでは、2つの仕組みが使われる
- 読み取り修復
  - クライアントはレプリカ3 の値が古いことを知った後、新しい値をレプリカに書き戻します。
  - このアプローチは、頻繁に読み取られる値についてはうまく機能する
- 反エントロピー（Anti-entropy）処理
  - 常にレプリカ間のデータの差異を探し、欠けているデータがあればあるレプリカから他のレプリカへコピーするバックグラウンドプロセスを持つ
  - この反エントロピー処理による書き込みのコピーは順序づけされておらず、データがコピーされるまでに大きな遅延が発生しうる
  - たとえばVoldemort は、執筆点では反エントロピー処理を持っていない
  - この場合、滅多に読み出されることのない値はいくつかのレプリカで欠損しているかもしれず、耐久性が低くなる

##### 5.4.1.2 読み書きのためのクオラム（Quorum）

- 書き込みを受け付けたのが1 つだけの場合はどうするのか。
- n 個のレプリカがあるならすべての書き込みは少なくともw 個のノードで成功したものと見なされていなければならない
- また読み込みの際には最低でもr 個のノードでクエリを実行しなければならない
- w + r > n である限りにおいて、少なくとも読み取り対象のr 個のノードのうちの1 つは最新なので、最新の値が得られることが期待できる
- これらのr やw の値に従って行う読み書きのことを、クオラム読み取りあるいは書き込みと呼ぶ
- r とw は、読み取りあるいは書き込みが正当なものであるために必要な最小の投票数
- Dynamoスタイルのデータベースでは、通常パラメータのn、w、r は設定可能
  - 一般的には、n を奇数（通常は3 もしくは5）とし、w = r = (n + 1) / 2（切り上げ）とする
  - たとえば書き込みが少なく読み取りが多いのであれば、w = n、そしてr = 1とすると良い
  - こうすることで読み取りが高速になり、その一方、障害を起こしたノードが1 つあるだけで書き込みは必ず失敗する

#### 5.4.2 クオラムの一貫性の限界

- r をw + r > n となるように選択しているのであれば、すべての読み取りにおいてキーに対する最も新しい値が返される
- これは、書き込みを行ったノードの集合と、読み取りを行ったノードの集合とが必ず重なり合うため
- とはいえ古い値が返されるケースもある
  - いい加減なクオラムが使われている場合
  - 2 つの書き込みが並行して行われるなら、どちらが先に行われたのかははっきりしなくなる
  - 読み取りと並行して書き込みが行われると、一部のレプリカでのみ書き込みが反映されることになる可能性がある（？？よくわからなかった）
  - 書き込みが一部のレプリカで成功し、その他のレプリカでは失敗していて、全体として失敗の場合でも成功したレプリカはロールバックされない
  - 新しい値を持っているノードに障害があり、そのノードのデータが古い値を持っているレプリカからリストアされた場合、新しい値を持っているレプリカの数がw を下回る
  - すべてがうまく動作している場合でもタイミングによって問題が生じるようなエッジケースがある
- クオラムからの読み込みが書き込まれた最新の値を返すことを保証しているように見えても、実際にはそれほど単純ではない
- Dynamoスタイルのデータベースは、概して結果整合性で耐えられるようなユースケースに最適化されている。
- パラメータで古い値が読まれる可能性を調整することはできますが、それは絶対的な保証とは捉えない方が良い
- 強い保証を得るためには、概してトランザクションや合意が必要

##### 5.4.2.1 遅延のモニタリング

- 古い値が読み取られてもアプリケーションが耐えられる場合でも、レプリケーションの健全性は把握しておく必要がある
- リーダーレスアプリケーションの場合は書き込みが適用される順序は定められないので、モニタリングは難しくなる
- リーダーレスレプリケーションのレプリカの古さの計測と、n、w、r といったパラメータに基づいて古い値が読み取られる確率の予想についてはいくつか研究がある
- 残念ながらこれはまだ一般的な実践にはなっていない

#### 5.4.3 いい加減なクオラム（sloppy quorum）とヒント付きのハンドオフ

- リーダーレスレプリケーションは、高可用性と低レイテンシが求められ、時おり古いデータが読まれることがあっても耐えられるようなユースケースに適している
- 大規模な（n よりもはるかに多くのノードを持つような）クラスタでは、いい加減なクオラムを使用することができる
  - 指定されたn 個の「ホーム」ノードに含まれないノードを含めることが可能
  - ネットワークの障害が修復され、あるノードが他のノードの代わりに一時的に受け付けていた書き込みは、適切な「ホーム」ノードに転送される
  - これをヒント付きのハンドオフと呼ぶ
  - 任意のw ノードが利用できるかぎり、データベースは書き込みを受け付けられる、これにより書き込みの可用性を高められる
  - とはいえそれが意味するのは、あるキーに対する最新の値が読み取れるとはかぎらない
- いい加減なクオラムは、広く利用されているすべてのDynamoスタイルの実装でオプションとなっている
  - Riak ではデフォルトで有効ですが、Cassandra やVoldemort ではデフォルトで無効

##### 5.4.3.1 マルチデータセンターでの運用

- リーダーレスもまた、衝突する並行書き込み、ネットワーク障害、レイテンシのスパイクに耐えられるよう設計されていることから、マルチデータセンターでの運用に適している
  - Cassandra 及びVoldemort は、通常のリーダーレスモデル内でマルチデータセンターのサポートを実装

#### 5.4.4 並行書き込みの検出

- Dynamoスタイルのデータベースでは、複数のクライアントが同じキーに対して並行に書き込み可能
  - これは厳格なクオラムでも衝突が発生しうるということ
  - この状況はマルチリーダーレプリケーションの場合と似ているものの、読み取り修復やヒント付きハンドオフでも生じることがある
  - ネットワークの遅延のばらつきや部分的な障害で、永久的に一貫した状態とならない
- 結果整合性を実現するためには、レプリカは同じ値に収束させるためにはどうすればよいか
  - データベースに自動的に処理してほしいと考えたくなるが、多くの実装は貧弱
  - アプリケーション開発者は使用するデータベースの衝突に対する処理の内部動作についてよく把握しておく必要がある

##### 5.4.4.1 最後の書き込みを勝たせる（並行した書き込みを捨てる）

- 最後の書き込みを勝たせる（last write wins、LWW）
- 結果の収束を実現するアプローチの1 つは、各レプリカは最も「新しい」値だけを保持すれば良いとする方法
  - 何らかの方法で明確に書き込みの「新しさ」を決められさえすれば、収束できる
- 書き込みは並行して行われたのであり、その順序は定義されない。（新しさを正確には把握できないということ）
- ただし何らかの順序を強制的に与えることは可能
- この方法は、成功したとクライアントに返された場合でも書き込みが破棄されてしまう可能性がある。
  - さらにはLWWは並行して行われていない書き込みもドロップしてしまうことがあり、「「8.3.3.1 順序関係を持つイベントのタイムスタンプ」で論じる
- キャッシュのように、書き込みが失われても許容できる状況もあるが、許容できないのであれば不十分な選択肢
- その他の選択肢
  - たとえばCassandra を使う上で推奨されるのは、UUIDをキーとして使うことによってそれぞれの書き込みにユニークなキーを与える

##### 5.4.4.2 「事前発生（happens-before）」関係と並行性

- 並行性とはなにか
  - AとBが因果関係を持たず、お互いにその操作のタイミングを認識していない場合
  - 実際に同時だったかどうかは重要ではない

##### 5.4.4.3 事前発生関係の捕捉

- 単一のレプリカでのやり方で2 つの操作が並行しているのか、あるいは事前発生関係にあるのかを判断するアルゴリズムを見ていく。
- 方法は以下の通り
  - サーバーはすべてのキーについてバージョン番号を管理
    - 書き込みが行われる度にバージョン番号をインクリメント
    - クライアントがキーを読み取る際に、サーバーは上書きされていないすべての値を最新のバージョン番号とあわせて返す
    - クライアントは書き込み前にそのキーを読み取らなければならない
    - クライアントはキーを書き込む前に以前のバージョン番号を含め、その読み取りで得た値をマージしなければならない
    - バージョン番号を含む書き込みを受信すると、サーバーはそのバージョン番号もしくはそれ以下のバージョン番号を持つすべての値を上書き可能
      - それより新しいものはマージされてない可能性がある
    - バージョン番号が含まれていない書き込みをするということは、他のすべての書き込みと並行状態ということ

##### 5.4.4.4 並行で書き込まれた値のマージ

- このアルゴリズムは、通知なくデータが削除されないことを保証しますが、残念ながらクライアントの作業が増える
- 複数の操作が並行に行われると、クライアントは並行に書き込まれた値をマージすることによって、後からクリーンアップの処理が必要
  - Riakではこの並行する値をsiblingと呼んでいる
- ショッピングカートの例では、和集合を取ることによりsiblingをマージするアプローチがある
  - この場合、アイテムの削除が存在する場合、適切な結果にならないケースがある
  - この問題が生じないようにするために、削除マーカーを残す必要がある（墓石（tombstone）と呼ばれる）
- sibling のマージは複雑で間違いを起こしやすいため、自動的に行えるような試みがある
  - たとえばRiakのデータ型では、CRDTと呼ばれるデータ構造ファミリの利用がサポート[38, 39, 55]
  - sibling を賢く自動的にマージすることができ、削除操作を保持したりが可能

##### 5.4.4.5 バージョンベクトル

- レプリカが複数あり、リーダーレスの場合には、このアルゴリズムはどのように変わるか
- 依存関係を把握するためにバージョン番号を1 つ使いましたが、並行して書き込みを受け付けるレプリカが複数あるなら、これでは十分ではない
  - キーごと、そしてレプリカごとにバージョン番号を使う必要がある
  - レプリカは書き込みを処理するたびに自分のバージョン番号をインクリメントし、他のレプリカから得たバージョン番号も追跡が必要
  - すべてのレプリカから取得したバージョン番号の集合は、バージョンベクトルと呼ぶ
- バージョンベクトルの例
  - Riak 2.0[58, 59] で使われているドット付きバージョンベクトル
  - Riakではバージョンベクトルをcausal context（因果コンテキスト）と呼ぶ文字列としてエンコードする
  - データベースは、このバージョンベクトルによって上書きと並行書き込みの区別をする

### まとめ

- レプリケーション手法
  - シングルリーダーレプリケーション
    - 読み取りは任意のレプリカから行えますが、フォロワーから読み取るデータは古い可能性がある
    - 非常に理解しやすく、衝突解決を気にする必要がないため広く使われている
  - マルチリーダーレプリケーション
    - 障害やレイテンシのスパイクに頑健だが、一貫性は弱い保証しかできない
  - リーダーレスレプリケーション
    - それぞれの書き込みを複数のノードに送信し、古いデータを持つノードを修正するために読み取りを複数のノードから並列に実行
    - 障害やレイテンシのスパイクに頑健だが、一貫性は弱い保証しかできない
- いくつかの一貫性モデル
  - read-after-write 一貫性
    - ユーザーは、自分自身が投入したデータを常に見ることができる。
  - モノトニックな読み取り
    - ある時点のデータをユーザーが一度見たら、それ以前の時点のデータを見ることがない。
  - 一貫性のあるプレフィックス読み取り
    - ユーザーは、たとえば質問とその質問への回答を適切な順序でといったように、適切な因果関係を保持した状態でデータを見ることできる。


## 6章 パーティショニング

- 非常に大規模なデータセットの場合、あるいはクエリのスループットが非常に高い場合はデータをパーティションに分割する必要がある
- これはシャーディングとも呼ばれる
  - パーティションと呼んでいるものは、MongoDB、Elasticsearch、SolrCloud ではシャードと呼ばれている
  - HBaseではリージョン、Bigtableではタブレット、CassandraやRiakではvnode、CouchbaseではvBucketと呼ばれている
- 通常、パーティションではそれぞれのデータの断片（各レコード、行、あるいはドキュメント）が厳密に1 つのパーティションに属するものと定義
- データをパーティショニングする主な理由はスケーラビリティ
  - それぞれのパーティションはシェアードナッシングクラスタの別々のノードに配置可能
  - 大規模なデータセットを数多くのディスクに分散配置でき、クエリの負荷を大量のプロセッサに分散させることが可能
- パーティションの歴史
  - パーティション化されたデータベースが最初に開発されたのはTeradata やTandem NonStopSQL[1] といった製品で、1980 年代のころ
  - 近年になって、NoSQLデータベースやHadoopベースのデータウェアハウスなどで再発見された
  - パーティションは、トランザクションでも分析でもどちらのワークロードにも当てはまる

### 6.1 パーティショニングとレプリケーション

- パーティショニングはそれぞれのパーティションが複数のノードに保存されることから、レプリケーションと組み合わされる
- これは、それぞれのレコードが耐障害性を持たせるために複数のノードに保存されるかもしれないということ
- 1 つのノードには、複数のパーティションが保存されることも
  - それぞれのパーティションのリーダーは1 つのノードに割り当てられ、その他のノードはフォロワーとなる
  - それぞれのノードは、あるパーティションのリーダーであると同時に他のパーティションのフォロワーになることがある
- パーティショニングのスキームの選択は、レプリケーションのスキームの選択とはほぼ独立しているため、本章ではレプリケーションを扱わない

### 6.2 キー‐バリューデータのパーティショニング

- パーティショニングの目標は、データとクエリの負荷をノード間で均等に分散させること
- スキュー（skew）
  - パーティショニングが均等になっておらず、一部のパーティションが他に比べて多くのデータやクエリを受け持っているような状態のこと
  - 不均等な高負荷が集中しているパーティションはホットスポットと呼ばれる
- ホットスポットの発生を避ける最もシンプルなアプローチは、ノード群に対してレコードをランダムに割り当てること
  - この方法には大きなデメリットがある
  - そのアイテムのあるノードを知る方法がないので、すべてのノートで並列にクエリを実行しなければならない
- もっと良い方法
  - シンプルなキー‐バリュー型のデータモデルを仮定し、常にプライマリキーでアクセスる

#### 6.2.1 キーの範囲に基づくパーティショニング

- パーティショニングの方法の1 つは、紙の百科事典における各巻のように連続的なキーの範囲を各パーティションに割り当てること
- パーティションの境界をデータにあわせて調整しなければならない
  - これは、データの分布が均等になっているとはかぎらないため
  - 逆にキーの範囲は必ずしも均等になっている必要はない
  - 境界は、管理者が選択することもできれば、データベースに自動的に選択してもらうことも可能
- このパーティショニングの戦略は、Bigtable やBigtableを元にしたオープンソースソフトのHBase[2, 3]、あるいはRethinkDB、バージョン2.4 以前のMongoDBが採用
- それぞれのパーティション内では、キーの並びがソート順になるようにしてける
  - これにより範囲に対するスキャンが容易
- キー範囲によるパーティショニングの欠点
  - アクセスパターンによってはホットスポットが生じうる
  - キーがタイムスタンプであるなら、、すべての書き込みは同じパーティションに集中する
- この問題を回避するためには、キーの先頭の要素にタイムスタンプを使うという方法とは異なる何かが必要になる
  - たとえば、各タイムスタンプの前にセンサー名を置く
  - これによりパーティショニングがまずセンサー名で行われ、タイムスタンプはその次に来るようにするといったことが可能

#### 6.2.2 キーのハッシュに基づくパーティショニング

- スキューやホットスポットといったリスクがあることから、多くの分散データストアはキーに対するパーティションを決定する際にハッシュ関数を使う
- 優れたハッシュ関数は、スキューのあるデータを取って、一様に分散させる
- パーティショニングという観点からすれば、使用するハッシュ関数は暗号として強力である必要はない
  - たとえばCassandra とMongoDB はMD5 を使用
  - Voldemort はFowler-Noll-Vo 関数を使用
- ハッシュ関数の中にはパーティショニングに適さないものもあるため注意
  - ばJava のObject.hashCode() やRuby のObject#hash では、プロセスが異なれば同じキーに対して異なるハッシュ値が返されるため不適切
- この手法は、パーティション間で均等にキーを分散させられるという点で優れている
- 境界は均等にすることもできれば、疑似乱数的に選択する（その場合のこの手法はコンシステントハッシュ法と呼ばれます）こともできます。
- キーのハッシュによってパーティショニングをすることのデメリット
  - パーティションの優れた特性である範囲に対するクエリ実行の効率性は失われる
  - 元々は近接していたキー群もすべてのパーティションに渡ってばらばらにになり、ソート順が失われる
  - MongoDBでは、ハッシュベースのシャーディングモードを有効にすると、範囲に対するクエリは必ずすべてのパーティションに送られる
  - Riak[9]、Couchbase[10]、Voldemortでは、プライマリキーの範囲に対するクエリはサポートされていません。
- Cassandra は、2 つのパーティショニング戦略の間で妥協を成立させている
  - 複数の列から構成される複合プライマリキーを宣言できる
  - パーティションの決定にはこのキーの先頭部分だけのハッシュが使われ、他の列はSSTable 内のデータをソートするための連結インデックスとして使用
  - クエリは複合キーの先頭列における値の範囲を検索できないものの、最初の列に固定値が指定されれば、キー中の他の列に対しては範囲に対するスキャンを効率的に実行可能

#### 6.2.3 ワークロードのスキューとホットスポットの軽減

- パーティションをキーのハッシュで決定すれば、ホットスポットの発生を減らすことができるが、完全にはなくせない。
- 極端な場合には、すべての読み書きが同じキーに対して行われれば、すべてのリクエストは同じパーティションにルーティングされる
- 今日の時点で、多くのデータシステムはこういったスキューの度合いが高いワークロードに対して自動的には対処できないので、スキューを抑えることはアプリケーションの役割
- 対処例
  - シンプルな方法としてランダムな数値をキーの始めか終わりに追加するというやり方
  - 2桁のランダムな数値を加えるだけで、1 つのキーへの書き込みを100 個のキーに均等に分散させることが可能
  - とはいえ、書き込みを複数のキーに渡って分割すれば、データを読むために100 個のキーから読み取りを行い、それらを結合しなければならなくなる
  - この手法を使うと管理系の処理も増える
  - 乱数を追加するのは、負荷が集中する少数のキーに限定するのが妥当
- おそらく将来的には、データシステムが自動的にスキューの生じているワークロードを自動的に検出し、対処できるようになる

### 6.3 パーティショニングとセカンダリインデックス

- セカンダリインデックスがある場合には、状況はもっと複雑となる
- 通常セカンダリインデックスは特定の値を検索する方法の1つ
  - ユーザー123によるすべてのアクションの検索
  - hogwashという語を含むすべての記事の検索
  - 色がredの車すべての検索
- セカンダリインデックスはリレーショナルデータベースの基本であり、ドキュメントデータベースでも一般的
- 多くのキー‐バリューストア（HBase やVoldemort など）は、実装が複雑になるためセカンダリインデックスををもたない
- その一方、セカンダリインデックスはデータモデリングにおいて非常に有益なので、Riakなどはそれらを追加しはじめた
- セカンダリインデックスはSolr やElasticsearchといった検索サーバーのレーゾンデートル（存在意義）でもある
- セカンダリインデックスの問題は、それらがうまくパーティションに対応づけられないこと
- アプローチは２種類ある
  - ドキュメントベースのパーティショニング
  - 語ベースのパーティショニング

#### 6.3.1 ドキュメントによるセカンダリインデックスでのパーティショニング

- 各パーティションは、そのパーティション内のドキュメントだけをカバーする自身のセカンダリインデックスを管理する
  - 他のパーティションに保存されているデータのことは関知しない
  - そのため、ドキュメントでパーティショニングされたインデックスは、ローカルインデックスとも呼ばれる
- ドキュメントID に何か特別な仕掛けをしていないかぎり、特定の色や特定のメーカーの車すべてが同じパーティション内にある理由はない
- したがって、赤い車を検索したいのであればクエリをすべてのパーティションに送信し、得られた結果を結合しなければならない
- パーティショニングされたデータベースに対するこのクエリのアプローチは、スキャッタ／ギャザーと呼ばれ、負荷が高いものになってしまう場合がある
- ドキュメントによってパーティショニングされたセカンダリインデックスは広く使われている
  - MongoDB、Riak[15]、Cassandra[16]、Elasticsearch[17]、SolrCloud[18]、VoltDB[19] はすべてそうです
- 多くのデータベースベンダーは、セカンダリインデックスのクエリの結果が単一のパーティションから返されるようにパーティショニングのスキームを組み立てることを推奨
- しかし常に実現できるわけではない

#### 6.3.2 語によるセカンダリインデックスでのパーティショニング

- パーティションごとに個別のセカンダリインデックス（ローカルインデックス）を持たせるのではなく、すべてのパーティションのデータをカバーするグローバルインデックスを構築
- 単純に1 つのノードにグローバルインデックスを保存してしまえばそこがボトルネックになり、パーティショニングをする目的を失う。
- グローバルインデックスもパーティショニングされなければならないが、プライマリキーのインデックスとは違うやり方でパーティショニングできる
- 例
  - インデックスはa からr までの文字で始まる色はパーティション0 に、s からz で始まる色はパーティション1 になるようにパーティショニング
  - インデックスは語そのものでパーティショニングすることも、語のハッシュでパーティショニングすることも可能
- ドキュメントによってパーティショニングされたインデックスに比べて、グローバルインデックスは読み取りを効率的にできるというメリットがある
- とはいえ、グローバルインデックスには書き込みが低速で複雑になるという欠点もある
  - 1 つのドキュメントの書き込みがインデックスの複数のパーティションに影響するかもしれないため
  - つまり1 つの書き込みで影響を受けるすべてのパーティションに渡る分散トランザクションが必要に
- 実際には、グローバルなセカンダリインデックスの更新は非同期で行われる
  - たとえばAmazon DynamoDBでは
    - グローバルなセカンダリインデックスの更新は通常の状況下では1 秒よりもはるかに短い時間で更新されるものの
    - インフラストラクチャにフォールトが生じた場合は波及にもっと時間がかかることがあり得る
- 語によってパーティショニングされたグローバルなインデックスの他の事例
  - これまであげた以外にもRiak の検索機能[21] やローカルインデックスとグローバルインデックスを選択できるOracle のデータウェアハウス

> そもそも複合キーとセカンダリの使用シーンの違いがうむむとなった。
> 複合キーは、もっとcardinalityが濃い場合で、セカンダリはcardinalityが薄い場合というイメージはあるが


### 6.4 パーティションのリバランシング

- 変化が生じた場合、あるノードから別のノードへのデータやリクエストの移動が必要になります。
  - クエリのスループットが増大するため、CPUを追加して負荷に対処する。
  - データベースのサイズが大きくなるので、保存のためにディスクやRAMを追加する。
  - マシンに障害が発生し、他のマシンがそのマシンが受け持っていた処理を肩代わりすることになる。
- 負荷をクラスタ内のあるノードから別のノードへ移行するプロセスは、リバランシングと呼ばれます。
- リバランシングが満たすべき条件
  - リバランシングの終了後、負荷（データストレージ、読み書きのリクエスト）はクラスタ内のノード間で公平に分配されていなければならない。
  - リバランシングが行われている間、データベースは読み書きを受け付け続けなければならない。
  - ノード間を移動させるデータは必要最小限にとどめ、リバランシングが高速に行われ、ネットワークやディスクI/Oの負荷が最小になるようにする。

#### 6.4.1 リバランスの戦略

##### 6.4.1.1 取るべきではない方法：ハッシュの剰余

- まずなぜ単に剰余をパーティションに使わないのか？
  - mod N のアプローチの問題は、ノード数N が変化すると、ほとんどのキーはノード間で移動しなければならなくなる
  - リバランシングの時の負荷が非常に大きくなる
  - 求められるアプローチは、必要以上のデータの移動を伴わないもの

##### 6.4.1.2 パーティション数の固定

- ノード数よりもずっと多くのパーティションを作成し、1 つのノードに複数のパーティションを割り当てる
- 1 つクラスタに追加されたら、この新しいノードはパーティションの分散が再び均等になるまで既存のすべてのノードからいくつかのパーティションを盗む
- パーティション数は変化せず、キーのパーティションへの割り当ても変わらない。
- 即時に行われないため、終わるまでは古いパーティションの割り当てが利用される
- 原理的には、クラスタ内でのハードウェアの不均衡を考慮に入れることも可能
- このリバランシングのアプローチは、Riak[15]、Elasticsearch[24]、Couchbase[10]、Voldemort[25]で使用されている。
- この構成では、通常パーティション数はデータベースが最初にセットアップされたときに決められ、後から変更されることはない
  - そのため成長に十分備えられる大きな数を選択する必要がある
  - とはいえ、パーティションごとに管理のオーバーヘッドは生じるので、あまりに大きな数を選択することは逆効果
- 合計サイズが大きく変動するような場合はパーティション数を選択するのが難しくなる

##### 6.4.1.3 動的なパーティショニング

- キーの範囲によるパーティショニングを行う場合、その数と境界を固定するのは不便なことがある
  - パーティションが不適切な境界に固定されると、適切に割り振られない。
  - パーティションの境界を手動で再設定するのはとても手間がかかる
- そのため、HBase やRethinkDB などのキーの範囲によってパーティショニングされたデータベースは、パーティションを動的に作成する。
  - パーティションが設定されたサイズ以上に大きくなったら、2つのパーティションに分割され、分割後の双方にほぼ半分のデータが含まれる
    - HBaseの場合は10GBがデフォルトです
  - 逆に、大量のデータが削除され、パーティションが何らかの閾値よりも縮小したら、それを近接するパーティションにマージできる
  - この処理はBツリーのトップレベルで生じるのと似ている
  - 分割された場合、負荷をバランスさせるために分割後の2 つのパーティションの片方が他のノードへ転送される可能性がある
  - HBase の場合、パーティションファイルの転送は下位層の分散ファイルシステムであるHDFSを通じて実行される
- 動的パーティショニングの特徴
  - パーティション数をデータの総量に適合させられる
  - 空のデータベースが単一のパーティションからスタートしてしまうという落とし穴も
  - 分割されるサイズに達するまでは、すべての書き込みは単一のノードによって処理され、他のノードは遊んでいる状態
  - この問題を緩和するために、HBase やMongoDBでは空のデータベースに対して初期のパーティション群を設定可能（これは事前分割）
- ハッシュパーティショニングと動的なパーティショニング
  - 動的なパーティショニングは、ハッシュパーティショニングされたデータにも適しています。
  - バージョン2.4 以降のMongoDB はキーの範囲によるパーティショニングとハッシュパーティショニングをどちらもサポート

##### 6.4.1.4 ノード数に比例するパーティショニング

- ノードあたりのパーティション数を固定する方法もCassandraやKetamaで使われている
- ノード数に変化がなければ各パーティションのサイズはデータセットのサイズに比例
- ノード数を増やしたらパーティションは小さくなる
- 新しいノードがクラスタに加わると、そのノードは一定数の既存のパーティションを分割し、片方を自分のものとし、他方のパーティションはそのまま元のノードに置く
- パーティションの境界をランダムに選択するためには、ハッシュベースのパーティショニングが使われていなければない
- 実際のところ、このアプローチはコンシステントハッシュ法の元々の定義に最も近い

#### 6.4.2 運用：自動のリバランスと手動のリバランス

- リバランスを自動で行うのか、それとも手動で行うのか
- 完全に自動化されたリバランシングと、完全に手動のリバランシングとの間には勾配がある
  - Couchbase、Riak、Voldemort はパーティションの推奨割り当てを自動的に生成しますが、それが有効になるのは管理者による確認の後
- 自動化は、自動的な障害検出と組み合わさると危険になることがある
  - ノードが過負荷になり、一時的にリクエストへのレスポンス速度が落ちた
  - 他のノードは過負荷になったノードが落ちたと結論し、自動的にクラスタをリバランスする
  - そうなれば過負荷に陥ったノードやその他のノード、そしてネットワークにさらなる負荷がかかる
  - 状況を悪化させ、カスケード障害が引き起こされる可能性
- そのため、リバランシングの処理のどこかには人を介在させると良いと考えられる

### 6.5 リクエストのルーティング

- リクエストを発行するときクライアントは適切な接続先のノードをどのように判断すればよいか
  - パーティションはリバランシングされるので、パーティションのノードへの割り当ては変化する
- これは、一般的なサービスディスカバリと呼ばれる問題の例であり、サービスディスカバリはデータベースだけの問題ではない
- ソフトウェアの中にネットワーク経由でアクセスできる部分があり、とりわけそのソフトウェアが高可用性を指向していれば、この問題が存在する
- おおきく３つのアプローチがある
  - クライアントが任意のノードに接続でき、それぞれのノードにパーティションへのノードの割り当てに関する情報があり、転送される
  - クライアントからのすべてのリクエストはまずルーティング層に送信し、転送される
  - クライアントに割り当てを認識させておき、直接適切なノードへ接続する方法
- いずれの場合でもルーティングの判断をするコンポーネントが、どのようにして割り当てが変化したことを知るか
- 多くの分散データシステムは、このクラスタのメタデータの追跡をZooKeeper のような独立した協調サービスに依存する
  - たとえばLinkedIn のEspresso はクラスタ管理にHelix を使っており、HelixはZooKeeperに依存している
  - HBase、SolrCloud、そしてKafka もZooKeeper でパーティションの割り当てを追跡する
  - MongoDBは同様のアーキテクチャであるものの、独自に実装されたconfig server とルーティング層のmongosで動作する
- クラスタの状態の変化を広めるためにノード間でgossipプロトコルを使用する方法もある
  - Cassandra とRiakで採用されている

#### 6.5.1 パラレルクエリの実行

- ここまでは、単一のキーの読み書きだけを行うきわめて単純なクエリに焦点を当ててきました。
- これは、ほとんどのNoSQL分散データストアがサポートしているアクセスのレベルです。
- しかし、しばしば分析処理に使われる大規模並列処理（massively parallel processing、MPP）リレーショナルデータベースは、はるかに洗練されている
  - データウェアハウスのクエリには、複数の結合、フィルタリング、グループ化、集計といった処理が含まれる
  - MPPのクエリオプティマイザはこの複雑なクエリをいくつもの実行ステージとパーティションに分割
  - その多くはデータベースクラスタ内の複数のノード上で並列に処理できます。
- データウェアハウスのクエリの高速な並列実行は専門的な話題であり、ビジネスにおける分析処理の重要性から商業的に大きな関心が寄せられており「10章」で述べる

### まとめ

- パーティショニング方法
  - キーの範囲によるパーティショニング
  - ハッシュパーティショニング
- パーティショニングとセカンダリインデックス
  - ドキュメントによってパーティショニングされたインデックス（ローカルインデックス）
  - 語によってパーティショニングされたインデックス（グローバルインデックス）
- リバランシング
- ルーティングする手法

## 7章 トランザクション

- トランザクションは、アプリケーションが複数の読み書きを論理的な単位としてまとめる方法
- トランザクションは全体として成功（コミット（commit））もしくは失敗（中断（abort）、ロールバック（rollback））します。
- トランザクションがあれば、エラー処理はアプリケーションにとってはるかにシンプルなもの
  - 部分的に成功したり失敗するということを気にしなくて良くなる
- トランザクションは自然法則ではなく、データベースにアクセスするアプリケーションのためのプログラミングモデルをシンプルにするという目的を持って生み出されたもの
- トランザクションはあらゆるアプリケーションが必要とするわけではない
  - トランザクションの保証を弱くしたり、トランザクションの保証を一切しなかったりすることによってメリットも
    - たとえばパフォーマンスの向上や可用性の向上
- トランザクションが必要かどうかを知るために、トランザクションが提供できる安全性の保証とそれに伴うコストを正確に理解する

### 7.1 トランザクションというとらえどころのない概念

- 多くのRDBのトランザクションは最初のSQLデータベースであるIBMのSystem Rが1975 年に導入したスタイルを踏襲
  - 大まかな発想は40年にわたって事実上変わっていない
  - MySQL、PostgreSQL、Oracle、SQL Serverなどのトランザクションサポートは、System Rに似ている
- 非リレーショナル（NoSQL）データベースの人気が高まった際に多くの新世代のデータベースがトランザクションを放棄あるいや弱くした
- しかしトランザクションはスケーラビリティに対立するものというのは誇張である
  - 真実は、それほど単純なものではありません。
  - これ以外のあらゆる技術的な設計の選択と同様に、トランザクションにはメリットと限界があります。

#### 7.1.1 ACIDの意味

- トランザクションが提供する安全性の保証は、しばしばよく知られている短縮語であるACIDで示される
  - 原子性（Atomicity）、一貫性（Consistency）、分離性（Isolation）、永続性（Durability）
- 実際には、ACIDの実装はデータベースごとに異なり、あいまいな部分も
  - 分離性の意味には曖昧な部分が多々ある
  - あるシステムが「ACID準拠」だと主張しているときに、実際にどういった保証を期待できるかははっきりしない
- ACID の条件を満たさないシステムはBASE と呼ばれることがある
  - 基本的に利用可能（Basically Available）、厳密ではない状態遷移（Soft state）、結果整合性（Eventual consistency）
  - これはACIDの定義よりもさらにあいまいで、妥当なものは「ACIDではない」というものだけ
  - すなわちほとんど好きなように解釈可能

##### 7.1.1.1 原子性

- 原子（アトミック）はそれ以上小さな部分に分割できないものを指して使われる言葉
- 並行処理におけるアトミックとは異なる
  - たとえばマルチスレッドのプログラミングにおいては、あるスレッドがアトミックな処理を実行していると表現される
  - それは他のスレッドからはその処理の半分だけ完了した途中の状態を見る方法が存在しないことを意味する
  - ACIDの文脈の原子性は並行性とは関係ない（これはACIDのIの分離性となる）
- ACID の原子性が示すもの
  - あるクライアントが複数の書き込みを行おうとして、いくつかの書き込みが処理された後に障害が発生した際におこること
  - 複数の書き込みがアトミックなトランザクションにグループ化されていれば、完了しなかった場合は書き込みがすべて破棄されて元の状態に戻る
  - トランザクションが中断されたら、アプリケーションは何も変更していないことが確実なので、リトライしても安全
  - エラーの際にトランザクションを中断し、そのトランザクションのすべての書き込みを破棄できることが、ACIDの原子性を決定づける特徴

##### 7.1.1.2 一貫性

- 残念ながら、同じ言葉が少なくとも4つの異なる意味で使われている
  - 5 章では、レプリカの一貫性と、非同期にレプリケーションを行うシステムで生じる結果整合性の問題について論じた
  - コンシステントハッシュ法は、リバランシングのためにいくつかのシステムで利用されているパーティショニングのアプローチ
  - CAP定理（9 章参照）においては、一貫性という言葉は線形化可能性の意味で使われる
  - ACIDの文脈における一貫性は、データベースが「良い状態」にあることを示すアプリケーション固有の概念
- ACID における一貫性という概念は、データについて常に真でなければならない何らかの言明があるということ
  - たとえば会計システムの場合、すべてのアカウントでまとめれば常に貸方と借方は等しくならなければならない
- 一貫性を保つようにトランザクションを適切に定義することはアプリケーションの責任
  - これはデータベースが保証できることではない
  - ある種の不変性の中には、データベースがチェックできるものもある
    - たとえば外部キーの制約やユニーク制約などを使う
    - しかし一般的には、データが適正かどうかはアプリケーションが決めることであり、データベースはデータを保存するだけ
- 原子性、分離性、永続性はデータベースの特性ですが、一貫性はアプリケーションの特性
- アプリケーションは一貫性を保つ上で、データベースの原子性や分離性といった特性を頼りにできるが、データベースだけの責任ではない

##### 7.1.1.3 分離性

- 複数のクライアントからアクセスされ同じレコードにアクセスするのであれば、並行性の問題（レース条件［race condition］）が生じる可能性がある
- 例
  - 2回のインクリメントが行われているのでカウンタは42から44にならなければなりませんが、実際にはレース条件のために43となることが生じうる
- ACID における分離性とは、並行して実行されたトランザクションがお互いから分離されており、お互いのつま先を踏みつけあうようなことがないという意味
- 古典的なデータベースの教科書では、分離性を直列化可能性（serializability）として形式化している
- 直列化可能性とは、それぞれのトランザクションがデータベース全体の中で実行されている唯一のトランザクションであるかのように振る舞うことを意味する
- 実際には、パフォーマンス上のペナルティがあることから直列化可能な分離性が用いられることはほとんどない
  - Oracle 11g のように、広く使われているデータベースの中にはそれを実装していないものさえある
  - Oracle には「直列化可能（serializable）」という分離レベルがありますが、実際にはそれが実装しているのはスナップショット分離と呼ばれるもの
  - これは直列化可能性よりも弱い保証

##### 7.1.1.4 永続性

- データベースシステムが目的とするのは、データを失う恐れなく保存できる安全な場所を提供すること
- 永続性は、トランザクションのコミットが成功したら、仮にハードウェアの障害やデータベースのクラッシュがあったとしても、データは失われないことを約束するもの
- p.6「1.2 信頼性」で論じたように、完全な永続性というものは存在しない

#### 7.1.2 単一オブジェクトと複数オブジェクトの操作

- 原子性・分離性はマルチオブジェクトトランザクションの時に必要
  - ユーザーが複数のオブジェクトを同時に変更する処理
  - 分離性により中途半端な状態が他のユーザから参照されることを防ぐ
- マルチオブジェクトのトランザクションでは、どの読み書き操作が同じトランザクションに属しているのかを判断する方法が必要がある
  - この判断は通常クライアントからデータベースサーバーへのTCP接続に基づいて行われる
  - ある接続上でBEGIN TRANSACTION文からCOMMIT文の間に行われたすべての操作は、同じトランザクションの一部と見なされる
- 一方で、多くの非リレーショナルデータベースは操作をグループ化するような方法を持っていない
  - マルチオブジェクトAPI がある場合でも、必ずしもトランザクションの性格を持っているとは限らない
  - つまり、データベースが部分的に更新された状態のままになるということもありえる


##### 7.1.2.1 単一のオブジェクトへの書き込み

- 原子性と分離性は、単一のオブジェクトが変更される場合にも適用される
- たとえば20KBのJSONドキュメントをデータベースに書き込む事例
  - 10KBが送信された時点でネットワーク接続が切断されたら、データベースはパースできない10KBのJSONの断片を保存するか
  - データベースがディスク上の以前の値を書き換えている途中で電源に障害があったら、新旧の値が継ぎ合わされた状態になるか
  - 書き込みが行われている途中に他のクライアントがそのドキュメントを読んだら、部分的に更新された値が見えることになるか
- これらの問題はひどい混乱を引き起こす
- そのため、ストレージエンジンはほとんど必ず単一ノードにおける単一オブジェクト（たとえばキー‐バリューペア）のレベルで原子性と分離性を提供することを目指している
  - 原子性はクラッシュリカバリ用のログを使って実装できる
  - 分離性はそれぞれのオブジェクトでのロックを使って実装できる
- データベースの中には、インクリメント操作などのさらに複雑なアトミック操作を提供することによって、read-modify-write のサイクルを必要としないものもある
- 同様に広く使われているのはcompare-and-set の操作で、これは値が並行に他者によって変更されていないときにのみ書き込みが行われるようにするもの
- これらの単一オブジェクトの操作は、複数のクライアントが並行に同じオブジェクトへの書き込みを行おうとしたときに更新が失われることをなくしてくれる
- とはいえこれらは通常の言葉の意味としてのトランザクションではありません。
  - compare-and-set やその他の単一オブジェクトの操作は、「軽量トランザクション」
  - あるいはマーケティング上の理由から「ACID」と称されることさえありますが[20, 21, 22]、そういった表現は誤解を招く
- 通常トランザクションとは、複数のオブジェクトに対する複数の操作を1つの実行単位としてグループ化する仕組み

##### 7.1.2.2 複数オブジェクトのトランザクションの必要性

- 多くの分散データストアは、複数オブジェクトのトランザクションを放棄している
  - パーティションをまたいで実装することが難しい
  - きわめて高い可用性やパフォーマンスが求められるような状況における難題になってしまう
- 根本的には分散データベースでトランザクションを実現することが不可能な理由はなく、分散トランザクションの実装については9 章で論じる
- そもそも複数オブジェクトのトランザクションは必要なものなのでしょうか？
- 他の多くのユースケースでは、複数の異なるオブジェクトへの書き込みを協調させなければならない
  - 外部キー参照を持津場合、複数オブジェクトのトランザクションがあれば、これらの参照を適切な状態にあることを保証できる
  - ドキュメントデータモデルにおいては、更新しなければならないフィールド群がドキュメント内にある場合はトランザクションは不要
  - 非正規化された情報を更新しなければならない場合には、一度に複数のドキュメントを更新する必要があり、トランザクションが有効
  - セカンダリインデックスを持つデータベースでは、インデックスも更新しなければならない
  - トランザクションの分離性がなければ、あるレコードが1 つのインデックスには現れているにもかかわらず、他のインデックスには現れていないということが起こる
- 原子性がなければエラー処理ははるかに複雑なものとなる
- 分離性がなければ並行性の問題が生じる

##### 7.1.2.3 エラーと中断の処理

- トランザクションの重要な機能は、エラーが生じた際に中断でき、しかも安全にリトライこと
- 途中まで進んだ状態のままにすることなく、完全に破棄するという哲学を基盤としている
- システムによってはこの哲学に従わないものもある
  - リーダーレスレプリケーションを行うデータストアはベストエフォートのことが多い
  - つまり、データベースはできる限り処理を進め、もしもエラーが起きた場合でもそれまでに終わった処理を取り消すことはしない
  - エラーからのリカバリはアプリケーションが受け持つ
- たとえばRails のActiveRecord やDjango といった広く使われているORMフレームワークは、中断されたトランザクションのリトライを行わない
- 中断されたトランザクションのリトライは完ぺきではない
  - サーバーが成功したコミットの承認をクライアントに通知する際に障害が発生すると、成功しているにもかかわらずクライアント側では失敗したと見なされる
    - この場合、アプリケーションレベルで重複を排除する仕組みが必要
  - 過負荷によるエラーの場合、リトライにより状況が悪化する（この場合exponential backoffなどを用いる）
  - リトライは一時的なエラーにのみ対応すべきで、恒久的なエラーはリトライするのはmとはずれ
  - トランザクションがデータベース外に副作用を持っているなら、そういった副作用はトランザクションが中断された場合にも生じている可能性がある
    - 例えばメールの送信など。この場合２相コミットなどが役立つ可能性がある
  - クライアントのプロセスがリトライ中に落ちたら、そのプロセスがデータベースに書こうとしていたデータはすべて失われる

### 7.2 弱い分離レベル

- 並行性の問題（レース条件）が問題になる場合は以下の２つ
  - 1 つのトランザクションが他のトランザクションから並行して変更されているデータを読み取る場合
  - 2つのトランザクションが同時に同じデータを変更しようとする場合
- トランザクションの分離性の提供
  - 並行性のバグはタイミングが悪い場合にのみ生じるものなので、テストで見つけることが困難
  - 大量のユーザーがアクセスするのであれば、どのデータがいつ予想外に変更されるかもしれないことから困難
  - その困難性のためトランザクションの分離性を提供することによってアプリケーションの開発者から隠そうとしてきた
- 実際の分離性の実現
  - 直列化可能な分離性はパフォーマンス上の負担になるので、多くのデータベースはその負担を割けている
  - もっと弱いレベルの分離性の提供、すなわちすべての並行性の問題ではなくある種の並行性の問題に対する保護だけを行うようなシステムが一般的
  - このレベルの分離性は理解することがかなり難しく、微妙なバグにつながりますが、利用されている。
  - ACIDなデータベースでも弱い分離性である
    - 広く使われている多くのリレーショナルデータベースシステム（これは通常「ACID」だと見なされています）でさえ利用しているのは弱い分離性
  - 何も考えずにツールを信じて依存するのではなく、並行性の問題を把握し回避するためにはどうすればよいかを理解する必要がある

#### 7.2.1 Read Committed

- トランザクションの分離性において最も基本的なレベルがread committed
- このレベルが保証すること
  - ダーティリードは生じない：データベースからの読み取りを行った際に見えるデータは、コミットされたもののみであること
  - ダーティライトは生じない：データベースへの書き込みを行う場合、上書きするのはコミットされたデータのみであること

##### 7.2.1.1 ダーティリードが生じないこと

- すなわち、あるトランザクションによる書き込みが他のトランザクションから見えるようになるのは、書き込みを行ったトランザクションがコミットされた後
- そしてその場合、そのトランザクションで行われたすべての書き込みが一斉に見えるようになる
- ダーティリードの回避が有益である理由
  - トランザクションが複数のオブジェクトを更新しなければならない場合、ダーティリードが原因で他のトランザクションから不正に見える
  - トランザクションが中断されている場合に後でロールバックされるのかもしれないデータが、他のトランザクションから見えてしまう。

##### 7.2.1.2 ダーティライトが生じないこと

- 並行してデータベース中の同じオブジェクトを更新しようとする際の話
- 通常、2 番目の書き込みは最初の書き込みのトランザクションがコミットもしくは中断されるまで遅らせられることにより実現する
- しかしread committedでは、た2回のカウンタのインクリメントで生じるレース条件を防いでくれない
  - 2 回目の書き込みはダーティライトではありません。にもかかわらず別の理由で正しくなくなる
  - これについては、「7.2.3 更新のロストの回避」で論じる

##### 7.2.1.3 read committedの実装

- read committedは非常に広く使われている
  - Oracle 11g、PostgreSQL、SQL Server 2012、MemSQLあるいはその他多くのデータベースにおけるデフォルトの設定
- ダーティライトを避けるやり方
  - 行レベルロックを使う方法がほぼ一般的
- ダーティリードを避けるやり方
  - 単純には、短期間だけロックを取得し、読み取りが終わったらすぐにそのロックを解放するという方法
    - read committed 分離レベルのためにロックを使っている主要なデータベースは、IBM DB2 とread_committed_snapshot=off が設定されたMicrosoft SQL Server のみ
  - しかしこれは実際にはうまくいかず、長時間にわたる書き込みトランザクションがあると、多くの読み取りのみのトランザクションが待たないといけなくなる
  - そのため、ほとんどのデータベースは書き込みが行われたすべてのオブジェクトに関して以下を保持しておく
    - コミット済みの古い値
    - 書き込みロックを取得しているトランザクションが設定した新しい値

#### 7.2.2 スナップショット分離とリピータブルリード

- read committed 分離を表面的に見たら、トランザクションがやらなければならないことはすべてやってくれるようにも見える
- 銀行残高の例では、口座の合計が減ったように見える。
  - 合計$1,000を持っている
  - 口座１の残高は$500を確認
  - 口座２から口座１への書き込みトランザクションを実施、コミット官僚
  - 口座２の残高は$400を確認、減っているように見える
- これを、はnonrepeatable readあるいは読み取りスキュー（read skew）と呼ばれる
- この問題は長く続くものではなく、ロードし直せば一貫した状態になるが、場合によってはこういった一時的な不整合が許されない状況もある
- 不整合が許されない状況
  - バックアップ
    - バックアップの処理が行われている間もデータベースへの書き込みがあるため、古いものと新しいものが混ざる場合がある
    - そういったバックアップからのリストアをする場合、不整合が恒久的になる
  - 分析的なクエリと整合性のチェック
    - 分析する場合は、データベースの大部分をスキャンするようなクエリを実行する
    - そういった場合は、不整合なデータによっては意味のない結果となる
- スナップショット分離は[28]、この問題に対する最も一般的な解決方法
  - それぞれのトランザクションがデータベースの一貫性のあるスナップショットから読み取りを行う
  - トランザクションが読み取るデータは、すべてそのトランザクションの開始時点のデータベースにコミット済みのものだけ
  - スナップショット分離は、バックアップや分析的なクエリのように長時間実行される読み取りだけを行うクエリに有益
- スナップショット分離は広く利用されている機能
  - PostgreSQL、ストレージエンジンとしてInnoDB を利用するMySQL、Oracle、SQL Server などでサポート

##### 7.2.2.1 スナップショット分離の実装

- read committed 分離と同様に、スナップショット分離の実装においてはダーティライトを避けるために通常は書き込みロックが
- 一方、読み取りのロックを必要としない
- スナップショット分離の重要な原則
  - 読み取りが書き込みをブロックすることはなく、書き込みが読み取りをブロックすることもない
- 実装には、ダーティリードを回避する仕組みを汎用化したものが使われる
- MVCC（multi-version concurrency control、マルチバージョン並行性制御）
  - はあるオブジェクトについて複数のバージョンを並べて管理すること
  - データベースはオブジェクトの複数のコミット済みバージョンを保持しなければならない
- スナップショットの取り方の違い
  - read committed ではクエリごとに個別のスナップショットを使用
  - スナップショット分離では、トランザクション全体にわたって同じスナップショットを使う
- PostgreSQL においてスナップショット分離の実装例
  - トランザクションが開始されると、そのトランザクションにはユニークなトランザクションID が割り当て
  - トランザクションIDはインクリメントされる
  - 更新は内部的には、古いレコードにdeleted_byにトランザクションIDを入れ、新しいレコードにcreated_byにトランザクションIDを入れる形で実行
  - 並行して実行するトランザクションIDより新しいcreated byを読まなくすることで分離される
  - その後どこかの時点で、削除されたデータにアクセスするトランザクションが存在しないことが確実なときに、ガベージコレクションのプロセスがその領域を開放


##### 7.2.2.2 一貫したスナップショットを見るための可視化ルール

- ほぼ上で書いた通りだった

##### 7.2.2.3 インデックスとスナップショット分離

- マルチバージョンデータベースにおいては、インデックスはどのように動作するか
  - インデックスは単純にオブジェクトの全バージョンを指すようにする
  - インデックスを使うクエリは、現在のトランザクションからは見えないバージョンのオブジェクトをフィルタリングする
- 実際には、多くの実装の細部によってMVCC のパフォーマンスは決まる
  - PostgreSQLでは、同じオブジェクトの様々なバージョンが同じページに格納されるのであればインデックスの更新を避ける
  - CouchDB、Datomic、LMDBでは別のアプローチが使われ、append-only のBツリーとなっているため、書き込みトランザクションが新しいBツリーのルートを作成する
    - このアプローチでも、コンパクションとガベージコレクションのためのバックグラウンドプロセスは必要で

##### 7.2.2.4 リピータブルリードと名前の混乱

- スナップショット分離レベルを実装している多くのデータベースにおいて、この分離レベルに別の名前が付けられている
  - Oracle ではSERIALIZABLE
  - PostgreSQLやMySQLではリピータブルリード
- この混乱が生じた理由
  - SQL標準にスナップショット分離という概念がないため
  - SQL標準がSystem Rの1975 年の分離レベルに基づいており[2]、その時点ではスナップショット分離が発明されていなかったことから来ている
  - SQL標準で定義されているのがリピータブルリードで、PostgreSQLやMySQLでは準拠を主張するためにそう呼んでいる
- 実際にはSQL標準における分離レベルの定義には欠陥がある
  - 曖昧であり、精密でなく、標準というには実装依存になり過ぎている
  - 建前上は標準化されているにもかかわらず、それらが実際に提供している保証には大きな違いがある
  - リピータブルリードの正式な定義は、研究文献中でなされてきましたが[29, 30]、ほとんどの実装はこの正式な定義を満たしていない
- さらに、IBM DB2Iは「リピータブルリード」という言葉を直列化可能性の意味で使っている

#### 7.2.3 更新のロストの回避

- 「ダーティライトが生じないこと」以外の書き込み間で生じうる衝突
- 最もよく知られているのは更新のロスト問題
  - 2 つの並行するカウンタのインクリメントの例
  - この問題は、アプリケーションが何らかの値をデータベースから読み取り、その値を変更して書き戻す場合に生じることがある
  - このケースを、read-modify-write サイクルと呼ぶ
- このパターンは、様々な状況下で生じる
  - カウンタを更新したり、口座の残高を更新したりする場合
  - 複雑な値の一部を変更する場合。たとえばJSONドキュメント内のリストに対する要素の追加
  - 2人のユーザーが同時に同じwiki のページを編集し、それぞれが変更分の更新処理としてページ全体の内容をサーバーに送信し、上書きする場合

##### 7.2.3.1 アトミックな書き込み操作

- 多くのデータベースでは、アトミックな更新処理が提供される
- これにより、アプリケーションのコードでread-modify-write サイクルを実装しなくてもすむ
- 提供状況
  - ほとんどのリレーショナルデータベースにおいてアトミックな操作を提供
  - MongoDBなどのドキュメントデータベースはJSONドキュメントの一部に局所的な変更を行うためのアトミックな操作を提供
  - Redis は優先順位付きキューのようなデータ構造を変更するためのアトミックな操作を提供し
- 必ずしもすべての書き込みがアトミックな操作によって簡単に表現できるわけではない
  - たとえばwiki ページの更新にはどういったテキスト編集が含まれるか分からない
  - 通常１クエリで書くことが難しいって話かな？
- アトミックな操作の実装
  - オブジェクトからの読み取りの際に排他ロックを取り、更新の適用が終わるまで他のトランザクションがそのオブジェクトを読めないようにする
  - この手法はカーソル固定（cursor stability）と呼ばれることもある
  - もう1つの選択肢としては、単純にすべてのアトミックな操作を単一のスレッドで実行するという方法も
- ORMフレームワークを使う際の懸念
  - ORMを使う場合、データベースが提供するアトミックな操作ではなく、安全ではないread-modify-write サイクルを実行してしまうコードを意図せず書いてしまいやすくなる
  - 認識できていれば問題はありませんが、これはテストでは見つけにくい微妙なバグの発生源になる可能性がある

##### 7.2.3.2 明示的なロック

- 更新のロストを防ぐもう1 つの選択
- 仮にデータベースに組み込まれたアトミックな操作の機能では不十分な場合で使う
  - 更新対象のオブジェクトをアプリケーションが明示的にロックするという方法
  - そうすればread-modify-write サイクルはアプリケーションが実行できる
  - 他のトランザクションが同じオブジェクトから並行に読み取りを行おうとしても、そのトランザクションは先行するread-modify-write サイクルが完了するまで待たされる
- たとえば複数のプレイヤーが同じキャラクターを並行して動かすようなマルチプレイヤーゲーム
  - この場合、プレイヤーの操作がゲームのルールに沿っていることを保証しなければならず、そのためにはデータベースのクエリではうまく実装できないようなロジックが必要
  - アトミックな操作だけでは不十分な可能性がある
  - その代わりに、ロックを使って他のプレイヤーが並行に同じキャラクターを動かせないようにする
  - FOR UPDATE節は、このクエリが返すすべての行に対するロックをデータベースが取得しなければならないことを示す。
- 正しく行うためにはアプリケーションのロジックを慎重に考慮が必要
  - コード中のどこかで必要なロックを追加するのを忘れ、レース条件を発生させてしまうことは簡単に起こる

##### 7.2.3.3 更新のロストの自動検出

- アトミックな操作とロックは、複数のread-modify-write サイクルがシーケンシャルに処理されるよう強制することによって更新のロストを防ぐ
- 別の手段として、トランザクションマネージャが更新のロストを検出する方法がある
  - read-modify-writeサイクルの並列実行は許す
  - 更新のロストを検出したらトランザクションを中断し、そのトランザクションのread-modify-write サイクルをリトライさせる
- このアプローチの利点は、スナップショット分離と併用することでデータベースがこのチェックを効率的に行える
- 提供
  - PostgreSQL のリピータブルリード、Oracle のserializable、SQL Server のスナップショット分離レベルは更新のロストが発生したことを自動的検出し、中断させる
  - しかしMySQL/InnoDB のリピータブルリードは更新のロストを検出しない
  - スナップショット分離を提供しているとするためには、更新のロストを回避できなければならないという主張もある
- 更新のロストの検出は、アプリケーションのコードから特別なデータベースの機能を使う必要もないので、素晴らしい機能

##### 7.2.3.4 compare-and-set

- トランザクションを提供していないデータベースには、しばしばアトミックなcompare-and-set操作がある
- この操作の目的は、更新が行われるのを最後の読み取り時から値が変化していない場合に限定することによって、更新のロストを防ぐこと
- 現在の値が以前の読み取り時と同じでなければ更新は行われず、read-modify-write サイクルはリトライしなければない
- 仮に古いスナップショットからの読み取りができるようなデータベースであれば、データが以前の読み取り時と同じか確認できているか注意が必要
  - compare-and-set 操作を頼りにする前に、それが安全なものかを調べてください。

##### 7.2.3.5 衝突の解決とレプリケーション

- レプリケーションされているデータベースでは、更新のロストの回避を別の角度からも見る必要がある
- 同じデータのコピーが複数のノード上にあるため、データが別々のノード上で並行して変更されるかもしれないので、更新のロストを避けるのに必要な手順が増える
- ロックやcompare-and-set 操作は、データの最新のコピーが1つしかないことを前提としている
- マルチリーダーやリーダーレスレプリケーションのデータベースでは通常複数の書き込みが並行に行われ、そのレプリケーションが非同期に行われる
- この場合は、ロックやcompare-and-set に基づく手法は適用できない
- その代わりに、レプリケーションされているデータベースでは、以下のアプローチが一般的であった
  - 並行する書き込みによって1つの値について衝突する複数バージョンを許す
  - アプリケーションのコードもしくは特別なデータ構造を使って事後にそれらのバージョンの衝突解決やマージを行う
- アトミックな操作はレプリケーションが行われる環境でもうまく働く
  - これは特に、それらの操作が交換可能である場合に言えることです
    - （すなわちそれらの操作の適用順序がレプリカごとに変わっても結果が同じになるような場合）
  - Riak 2.0 のデータ型はレプリカ間での更新のロストを防いでくれますが、その背景となっているのがこの概念
- 一方で、LWW（last write wins、最後の書き込みを勝たせる）を衝突解決の方法として利用する場合には更新のロストが生じやすくなる
- 残念ながら、レプリケーションを行う多くのデータベースではLWWがデフォルトになっている

#### 7.2.4 書き込みスキューとファントム

- 前セクションで2 種類のレース条件としてダーティライトとダーティリードを見た
- 並行する書き込みの間で生じうるレース条件がこれだけというわけではなく、本セクションでは、もっと微妙な衝突の例を見ていく

##### 7.2.4.1 書き込みスキューとは何か

- 2 つのトランザクションが更新しているのは別々のオブジェクトにもかかわらず、その集約値などによりレース条件が発生するケース
- 2 つのトランザクションが同じオブジェクト群からの読み取りを行い、それらのいくつかを更新する場合に生じうるもの
- この場合に取れる選択肢は限られている
  - 複数のオブジェクトが関わっているので、単一オブジェクトに対するアトミックな操作は役に立たない
  - スナップショット分離の実装に見られる更新ロストの自動検出も役に立たない、自動的に回避するためには、本当の直列化可能分離が必要
  - データベースによってはトリガやマテリアライズドビューを使ってそういった制約を実装できるかも
- 次善の選択肢は、依存する行を明示的にロックする方法（`FOR UPDATE`を使う）

##### 7.2.4.2 書き込みスキューの他の例

- 会議室予約システム
  - スケジュールが重ならないことを保証するためには、直列化可能分離レベルが必要
- マルチプレイヤーゲーム
  - 同じキャラクターを同時に動かせないようにはできますが、それぞれの異なるキャラクターを同じ位置に動かすことは防げない。
  - 強制したいルールによってはユニーク制約を使えるかも
- 利用するユーザー名の要求
  - 同じユーザで同時にアカウントを作成するようなケース
  - この場合はユニーク制約がシンプルな解決策になりえる
- ２重支払いの防止
  - ２つの出費が同時に挿入されて残高がマイナスになってしまう可能性も

##### 7.2.4.3 書き込みスキューを生じさせるファントム

- 1)読み込み、2)書き込み、3)読み込みとした場合に、1)と3)が同じクエリでも結果が変わってしまうケース
- あるトランザクションでの書き込みが他のトランザクション中の検索クエリの結果を変化させてしまうこの効果はファントムと呼ばれる
- ロックをかけるレコードが存在しない場合もある
  - 1)読み込みが、存在しないことをチェックする場合など
  - こうするとロックで回避することが難しい

##### 7.2.4.4 衝突の実体化

- ロックするものがない場合はそれを作ればよいという話。
- 会議室予約のケース
  - 時間と会議室のスロットからなるテーブルを作成する
  - 注意すべきは、追加されたテーブルは予約の情報を保存するためには使用せず、純粋に同じ会議室の時間帯が並行して修正されないようにするためのロックの集合
- これを衝突の実体化と呼ぶ
- 衝突の実体化は他の方法がない場合の最後の手段として考えるべき
  - 並行性の制御の仕組みをアプリケーションのデータモデルに漏れ出させてしまうのは不格好なこと
- 多くのケースでは、直列化可能分離レベルの方がはるかに望ましい

### 7.3 直列化可能性

- 分離レベルを理解することは難しく、データベースごとの実装には一貫性がない
- アプリケーションのコードを見てみても、それを特定の分離レベルで実行して安全なのかを判断することは難しい
- レース条件の検出を支援してくれる優れたツールはない
  - 原理的には静的解析は役に立つかもしれませんが[26]、それらを実用化する研究手法はまだ見いだされていない
  - 通常、並行性の問題は非決定的（nondeterministic）であるためテストするのが難しい
- 研究者たちが答えとしてきたのは最初からシンプルで、直列化可能分離レベルを使おう、ということ
- 直列化可能分離レベルは、通常最も強い分離レベルとされている
- 直列化可能分離レベルがその他様々な弱い分離レベルよりもはるかに優れているというなら、なぜ誰もがそれを使わないのか？
- 現時点では、これらの手法は主に単一ノードのデータベースの観点で論じる

#### 7.3.1 完全な順次実行

- 並行性の問題を回避する最もシンプルな方法は、並行性を完全に排除してしまうこと
- ある時点で実行するトランザクションは1 つだけにして、単一スレッドで順次トランザクションを処理
- 当然のことであるものの、単一スレッドのループによるトランザクションの実行が現実的だとデータベース設計者が判断するようになったのはごく最近（2007 年頃）
  - マルチスレッドの並行性がこの30 年間にわたって高性能を得るために必須と考えられていた
- この再考が促されたのは、2つの進歩による
  - RAMの価格が下がり、多くのユースケースでアクティブなデータセット全体をメモリに保持しておくことが十分現実的になった
  - データベースの設計者が、OLTPトランザクションは通常短く、少数の読み書きしかしないのを理解した
  - これに対し、長時間にわたって実行される分析的なクエリは通常読み取りのみを行うので、順次実行のループ外で実行できる
- トランザクションを順次実行していくアプローチは、VoltDB/H-Store、Redis、Datomicで実装されている
- シングルスレッドでの実行用に設計されたシステムは、並行性をサポートするシステムよりも高い性能を発揮することがあります。
  - これは、ロックによる調整のオーバーヘッドを避けられるため
- とはいえ、こういったシステムのスループットは単一のCPUコアのスループットが上限になる
- シングルスレッドの性能を最大限に発揮させるためには、トランザクションを旧来の形とは異なる構成にする必要がある

##### 7.3.1.1 ストアドプロシージャへのトランザクションのカプセル化

- 歴史
  - データベースの初期の時代には、データベースのトランザクションはユーザーの活動の流れすべてを包含できるものとして意図されていた
    - 航空券の予約は複数の段階を含むプロセスです（経路、運賃、利用可能なシートの検索、旅程表の作成、旅程表の各フライトの予約、乗客の詳細情報の登録、支払い
    - これらが一つのトランザクションであることを意図
  - しかし人間の意思決定や反応は非常に低速で、トランザクションがユーザーからの入力を待たなければならないとしたら、非効率的です。
  - そのため、トランザクションは短くなるよう変化した。
  - いずれ人間はこの処理から外されたにもかかわらず、トランザクションはクライアント/サーバー形式のやりとりの中で、1 回のやりとりで1 つの文を実行するままとなっていた。
  - そのためトランザクション内では今度はアプリケーションとデータベース間でのネットワーク通信に時間が消費されている。
- ストアドプロシージャ
  - そのため単一スレッドの順次実行では、外部とのやりとりを行う複数文を含むトランザクションを禁止
  - その代わりに、アプリケーションはトランザクションのコード全体を事前にストアドプロシージャとしてデータベースに登録
  - トランザクションが必要とするデータがすべてメモリ中にあるならストアドプロシージャはネットワークやディスクI/Oを待つことなくきわめて高速に実行できる

##### 7.3.1.2 ストアドプロシージャの長所と短所

- リレーショナルデータベースには、これまでかなりの間ストアドプロシージャがあったが様々な理由から多少の悪評が寄せられてきました。
  - 各データベースベンダーが独自の言語をストアドプロシージャに使っている
  - データベース上で動作するコードの管理の難しさ（デバッグが難しい、バージョン管理が面倒、など）
  - うまく書かれていないストアドプロシージャ（たとえばメモリやCPU時間の消費が大きい）がデータベース内にある場合は、パフォーマンスが重視されるDBでは重大な問題になる
- 現代的なストアドプロシージャの実装はこれらの問題を解決できる
  - 汎用的なプログラミング言語を使うなど

##### 7.3.1.3 パーティショニング

- 書き込みのスループットが高いアプリケーションの場合、単一スレッドでのトランザクション処理は重大なボトルネックになる可能性がある
- 複数のCPUコア、そして複数のノードにスケールさせるためには、データをパーティショニングする（6 章参照）こともできる
  - これはVoltDBでサポート
- 複数のパーティションにアクセスしなければならないトランザクションの場合、すべてのパーティションにわたるトランザクションを調整しなければなりません。
- システム全体にわたって直列化可能性を保証するために、ストアドプロシージャはすべてのパーティションで歩調を揃えて実行する必要がある
- トランザクションがパーティションをまたぐなら、調整のオーバーヘッドが加わることになり、単一パーティション上でのトランザクションに比べて極端に速度が落ちる
- トランザクションを単一パーティションで実行できるかどうかは、アプリケーションが使用するデータの構造に強く依存
  - 複数のセカンダリインデックスを持つようなデータの場合は、おそらくパーティションをまたぐ調整が大量に必要になる

##### 7.3.1.4 順次実行のまとめ

- 順次実行は、以下のような制約の下では直列化可能分離レベルを実現するための現実的な方法
  - すべてのトランザクションが小さくて高速
  - アクティブなデータセットがメモリに収まるようなユースケースであること
  - 書き込みスループットが単一のCPUコアで十分処理できる程度に低いこと
  - またはパーティションをまたぐ調整なしにトランザクションをパーティショニングできること
  - パーティションにまたがるトランザクションも実行できますが、それを使うためにハードリミットを設けられる場合

#### 7.3.2 ツーフェーズ（2相）ロック（2PL）

- 30 年ほどの間、データベースにおいて直列化可能分離レベルのために広く使われていた唯一のアルゴリズムがツーフェーズロック（2PL）
  - 2PL は2PC ではありません
  - ツーフェーズロック（2PL）は2 相コミット（2PC）と非常によく似ているように聞こえますが、これらはまったく別のもの
  - 2PCについては9 章で論じる
- ツーフェーズロックはダーティライトの防止と似ているが、ロックの要求がさらに強い
- オブジェクトへの書き込み（変更もしくは削除）をしようとする場合には、排他的アクセスが必要となる
- 2PLでは、ライターがブロックするのは他のライターだけではなく、他のリーダーもブロックする
  - スナップショット分離レベルの教義は「リーダーはライターをブロックせず、ライターはリーダーをブロックせず」
- ツーフェーズロックは直列化可能性を提供するので、更新のロストや書き込みスキューを含むこれまで議論してきたすべてのレース条件に対する保護を提供

##### 7.3.2.1 ツーフェーズロックの実装

- PLはMySQL（InnoDB）及びSQL Serverの直列化可能分離レベル、そしてDB2のrepeatable read 分離レベルで使用
- 実装
  - 各オブジェクトにロックを持たせることによって実装されます。
  - このロックは共有（shared）モードもしくは排他（exclusive）モードを取ることができる
    - 共有モード
      - 共有モードのロックは読み取り時に必要なロックで、複数のトランザクションが同時に取得可能
      - 排他ロックされている場合はその解放を待つ必要がある。
    - 排他モード
      - 排他モードのロックは書き込み時に必要なロックで、一つのトランザクションが取得可能
      - ロックされている場合はその解放を待つ必要がある
  - 読み取りと書き込みを行う場合は、共有モードを排他モードに格上げできる（解放は待つ必要がある）
  - ロックを取得したトランザクションは、トランザクションの終了（コミットもしくは中断）までそのロックを保持する
- デッドロック
  - 大量のトランザクションが使われることから、お互いの解放を待つことも容易に生じ、デッドロックと呼ばれる
  - データベースはトランザクション間のデッドロックを自動的に検出し、いずれかのトランザクションを中断させて他のトランザクションが処理を進められるようにします
  - この場合アプリケーション側でリトライが必要

##### 7.3.2.2 ツーフェーズロックのパフォーマンス

- 1970 年代からツーフェーズロックが広く一般に使われるようにならなかった理由はパフォーマンス
- 旧来のリレーショナルデータベースはトランザクションの長さを制限していません。
- これはそれらのデータベースが人間からの入力を待つインタラクティブなアプリケーション用に設計されているため
- つまり、システムを停止状態に追い込むには、たった1 つの低速なトランザクション、あるいは大量のデータで多くのロックを取得するトランザクションが1 つあればよい
- 頑健な運用が求められる場合、この不安定性は問題になる
- デッドロックはロックベースのread committed分離レベルでも生じるが、2PLの直列化可能分離レベルでははるかに生じやすい

##### 7.3.2.3 述語（predicate）ロック

- ファントムの問題がある中でどのようにロックを取得するかという話
- 概念的には、述語ロックが必要になります
- 鍵となる概念は、述語ロックはデータベース中にまだ存在していないものの、将来追加されるかもしれないオブジェクト（ファントム）に対してさえも適用できる
- フェーズロックが述語ロックも含むのであれば、そのデータベースはあらゆる形態の書き込みスキューやその他のレース条件を排除でき、その分離レベルは直列化可能分離レベル

##### 7.3.2.4 インデックス範囲（range）ロック

- 残念ながら、述語ロックのパフォーマンスは良くない
- アクティブなトランザクションによって多くのロックが取られていると、マッチするロックを調べるのに時間がかかる
- そのため、2PL を持つほとんどのデータベースが実装しているのはインデックス範囲ロック（next-key ロックとも呼ばれます）
- これは述語ロックを単純化した近似的なもの
  - room_idと時間帯でロックを確認するのが述語ロック（インデックス以外のすべての条件を見る）
  - room_idのみでロックを確認する（インデックスのみを見る）
- 近似的な検索条件が1 つのインデックスに付けられている仕組み
- ロックの対象となるオブジェクトの範囲は直列化可能性を保つのに厳密に必要な範囲よりも大きくなる
- 範囲ロックを与えるのに適したインデックスがない場合、テーブル全体に対する共有ロックにフォールバックする
- これはパフォーマンス面では良くありませんが、安全なフォールバック先ではある

#### 7.3.3 直列化可能なスナップショット分離（SSI）

- ここまで見てきたように直列化可能分離レベルと優れたパフォーマンスとは、基本的に相容れないのでしょうか？
- 直列化可能スナップショット分離（serializablesnapshot isolation、SSI）と呼ばれるアルゴリズムには、大きな期待が持てます。
- これは完全な直列化可能性を提供しながら、スナップショット分離レベルに比べるとパフォーマンス上のペナルティが少なくて済む
- SSI は非常に新しいもので、最初に言及されたのは2008 年
- 今日、SSI は以下で使われている
  - 単一ノードのデータベース（バージョン9.1 以降のPostgreSQL の直列化可能分離レベル[41]）
  - 分散データベース（FoundationDBが同様のアルゴリズムを使っています）
- SSI は他の並行性制御の仕組みに比べるととても若いので、現場のパフォーマンスはまだ検証されている段階
- 将来的には新しいデフォルトになるだけの速度を持っている

##### 7.3.3.1 悲観的な並行性制御と楽観的な並行性制御

- ツーフェーズロックは、いわゆる悲観的（pessimistic）な並行性制御の仕組み
  - どんなことでもおかしくなる可能性があることを原理とする
  - 状況が安全になるのを待ってから処理を進める方が良いとする
  - これは、マルチスレッドプログラミングにおいてデータ構造を保護するのに使われる相互排他に似ている
- 直列化可能スナップショット分離は楽観的（optimistic）な並行性制御の仕組み
- 楽観的というのは潜在的に危険なことが起きた場合、トランザクションの実行をブロックする代わりにそのまま継続してしまい、結局は問題がなかったことになるのを期待する
  - トランザクションがコミットしようとした場合、データベースは問題が生じていないかを確認する
  - もし問題があればトランザクションは中断され、リトライが必要
  - コミットできるのは、直列化可能な状態で実行できたトランザクションのみ
- 楽観的な並行性制御は古い概念でありその長所も短所も長い間にわたって議論されてきた
- 激しい競合がある場合はパフォーマンスが悪くなる
  - 中断しなければならないトランザクションの比率が上がるため
- しかし、十分な容量があり、トランザクション環境の競合がそれほど高くない場合は、楽観的な並行性制御の手法のパフォーマンスは悲観的な手法に比べて高くなる
- 競合は、交換可能なアトミックな操作によって減少させることが可能
- その名が示すとおり、SSI はスナップショット分離に基づいている
  - すなわち、トランザクション内のすべての読み取りは一貫したデータベースのスナップショットから行われる
  - これは、これまでの楽観的並行性制御の手法との主な違い
  - SSI は、スナップショット分離の上に書き込み間の直列化可能性の衝突を検出するアルゴリズムを加え、中断すべきアルゴリズムを判断する

##### 7.3.3.2 古くなったプレミスに基づく判断

- プレミスとは、トランザクション開始時点では真だった事実。たとえば「現時点では2 人の当直医がいる」など
- アプリケーションがクエリを発行する時点では、データベースはアプリケーションのロジックがそのクエリの結果をどのように使うのかをしらない。
- データベースはそのクエリの結果（プレミス）に変化があれば、そのトランザクションにおける書き込みは無効になるかもしれないという前提を考慮する必要がある
- 直列化可能分離レベルを提供するためには、データベースはトランザクションが古くなったプレミスを元に処理を行った状況を検出し、中断させなければならない
- クエリの結果が変化したかもしれないことを、データベースはどのように知る？
  - 古くなったバージョンのMVCCオブジェクトを読み取った場合
  - 以前の読み取りに影響する書き込みが行われた場合

##### 7.3.3.3 古くなったMVCCの読み取りの検出

- スナップショット分離レベルは、通常マルチバージョン並行性制御（MVCC、図7-10 参照）によって実装
- あるトランザクションが他のトランザクションによる書き込みをMVCCの可視性の規則に従って無視したことを追跡する
- トランザクションをコミットしようとするときには、データベースは無視された書き込みのいずれかがこの時点でコミットされていないかを調べます。
- もしコミットされたものがあれば、トランザクションを中断する必要がある
- 読み取りを検出した時点ですぐにを中断すればよいわけではない
  - 読み取りのみを行うトランザクションであれば、書き込みスキューのリスクはないので中断する必要がない
  - 読み取りを行った時点では、データベースはどのトランザクションが後に書き込みを行うかわからない

##### 7.3.3.4 先行する読み取りに影響する書き込みの検出

- あるトランザクションが読み取ったデータを後に他のトランザクションが変更するケース
- インデックスが張られているなら、データベースはそのインデックスエントリを使ってトランザクションデータを読んだということを記録できる
- 7.3.3.3との違いは実装方法？？よくわからなかった。

##### 7.3.3.5 直列化可能スナップショット分離のパフォーマンス

- 実際にどれだけうまく動作するかは、数多くのエンジニアリング上の細部に影響
- たとえばトレードオフの1 つにトランザクションの読み書きを追跡する粒度
  - きわめて詳細に追跡するなら、どのトランザクションを中断させなければならないのかは精密に決定できる、その代わりに追跡作業のオーバーヘッドが増加
  - 逆であれば追跡は高速にできるが、厳密には必要のなかったトランザクションの中断が発生する
- 直列化可能スナップショット分離には他のトランザクションが保持しているロックを待っているトランザクションが必ずしもブロックされないという大きな利点がある
- スナップショット分離レベルの場合のように、ライターはリーダーをブロックせず、リーダーもライターをブロックしない
- 読み取りのみを行うクエリはロックを必要とせずに一貫したスナップショット上で実行でき、これは読み取り負荷の高いワークロードにとって大きな魅力
- 順次実行と比較した場合、直列化可能スナップショット分離は単一のCPUコアのスループットに限定されない
  - FoundationDBは直列化の競合の検出を複数マシンに分配することによって、きわめて高いスループットにまでスケールできる
- データが複数のマシンにまたがってパーティショニングされている場合も、トランザクションは直列化可能分離レベルを保証したまま複数のパーティションのデータを読み書き可能
- それでも、長期間にわたってデータを読み書きするトランザクションは衝突を起こして中断する可能性が高い
- そのためSSI では読み書きを行うトランザクションはごく短くすることが求めらるが、ツーフェーズロックや順次実行に比べると影響は低い

### まとめ

- トランザクションが回避の役に立つような問題の例を数多く見ましたが、必ずしもすべてのアプリケーションがこれらすべての問題の影響を受けやすいわけではない
- しかし、アクセスパターンがもっと複雑な場合には、トランザクションは考慮が必要な潜在的エラーケースの数を大幅に減らす
- トランザクションがない場合、様々なエラーのシナリオででデータの一貫性が失われうる
- 本章では、特に並行性制御の問題に深く踏み込みました。
  - ダーティリード、ダーティリード
  - 読み取りスキュー
  - 更新のロスト
  - 書き込みスキュー
  - ファントムリード
- 広く利用されているいくつかの分離レベル、中でもread committed、スナップショット分離、直列化可能といたレベルについて論じた
- 弱い分離レベルは、これらの異常のいくつかを防いでくれますが、それ以外はアプリケーション開発者に対処させる必要がある
- これらすべての問題に対する保護を提供してくれるのは直列化可能分離レベルのみ
  - 文字通りにトランザクションを順次実行する
  - ツーフェーズロック
  - 直列化可能スナップショット分離（SSI、serializable snapshot isolation）
- トランザクションはデータモデルがどういったものであるかにかかわらず、価値あるデータベースの機能

## 8章 分散システムの問題

- これまでフォールトについて多くを語ってきたとはいえ、これまでの数章はあまりに楽観的過ぎました
- 現実はもっと暗く、ここからは最大限に悲観的になり、おかしくなるかもしれないことは必ずいつかおかしくなるものと考える
- 分散システムを扱うということは、単一のコンピュータ用にソフトウェアを書くこととは根本的に異なる
- そしてその主な違いは、驚くべき問題が新たに、そして大量に現れるということ

### 8.1 フォールトと部分障害

- 単一のコンピュータ上で動作するソフトウェアが当てにならないものになる根源的な理由はない。
- ハードウェアが正しく動作していれば、同じ処理は常に同じ結果を生成する（決定的である）
- 優れたソフトウェアが搭載された個々のコンピュータは、通常完全に動作するか完全に壊れているかのどちらかであり、その中間の状態にはならない。
- ネットワークで接続された複数のコンピュータ上で動作するソフトウェアを書く場合、状況は根本的に異なる
- 分散システムは、理想的なシステムモデルの中で動作するものではなく、物理的な世界の雑然とした現実に直面せざるを得ない
- 部分障害
  - 分散システムでは、システムの他の部分は問題なく動作しているにもかかわらず、ある部分は予想外の何らかの形で破損していることが十分あり得る
  - 部分障害は非決定的であるため困難を伴う（うまく動作することもあれば、予想外に失敗することも）
  - メッセージがネットワークをわたっていくのにかかる時間もまた非決定的なので、何かが成功したかどうかを知ることができないことさえある

#### 8.1.1 クラウドコンピューティングとスーパーコンピューティング

- 大規模なコンピューティングシステムの構築方法の哲学にはスペクトラムが存在する
  - 一端にはハイパフォーマンスコンピューティング（HPC）
    - た演算集中型の科学計算タスクに使われる
  - もう一端にはクラウドコンピューティング
    - 複数テナントのデータセンター、IP ネットワーク（多くの場合イーサネット）で接続された一般的なコンピュータ群
    - エラスティック（柔軟に増減できる）／オンデマンドでのリソース割り当て
    - 計測に基づく課金など
  - 伝統的なエンタープライズデータセンターは、この両端の間のどこか
- こういった哲学の下で、フォールトを扱うためのアプローチには大きな違いが生じる
  - スーパーコンピュータの場合
    - 通常ジョブは時おり演算の状況をチェックポイント処理し、永続性のあるストレージに保存
    - ノード障害の一般的な対応策は、ワークロード全体の停止
    - 障害の起きたノードが修理できれば、演算処理は最後のチェックポイントから再開
    - これは単一ノードのコンピュータに近い
- 本書で焦点を当てるのはインターネット上のサービスを実装するシステムであり、スパコンと大きく異なる
  - 高可用性
    - インターネットに関係するアプリケーションの多くはオンラインで動作し、常に低レイテンシでユーザーに対応が必要。
    - 修理するためにサービスが利用できなくなることは受け入れられない、一方スパコンのジョブは停止してから再開しても影響は小さい
  - ハードウェアの信頼性
    - スパコンは特化したハードウェアで構築し、信頼性も高い
    - クラウドサービスのノードはコモディティマシンで構築されており、障害の発生率も高い
  - 通信方式
    - スパコンは既知の通信パターンを持つHPCのワークロードにおいて優れたパフォーマンスを発揮する、ネットワークトポロジーを利用（多次元メッシュやトーラスなど）
  - 規模
    - 数千ノードを持つシステムにおいては何かが常に故障しているという前提に立つのが妥当
  - 耐障害性
    - 障害を起こしたノードがあってもシステムが耐えられ、全体としては処理を継続できるのであれば、それは運用やメンテナンスという面で非常に有益
  - 地理的な違い
    - 地理的に分散している動作環境はインターネットを経由することになる
    - スーパーコンピュータの場合、概してすべてのノードは近くにあるものと考えられる
- 分散システムをうまく動作させたいのであれば、部分障害の可能性を受け入れ、フォールトトレランスの仕組みを組み込む必要性
- 言い換えれば、信頼性のないコンポーネントから信頼性のあるシステムを構築しなければならない
  - 完全な信頼性などというものは存在しないので、私たちは現実的に約束できることの限界を理解しなければない
- フォールトの処理はソフトウェアの設計の一部でなければならず、ソフトウェアの運用者はフォールトの発生時にソフトウェアの振る舞いがどうなるのかを知っておく必要がある

#### 信頼性のないコンポーネントから信頼性のあるシステムを構築すること

- 直感的には、システムの信頼性は最も信頼性の低いコンポーネント（すなわち最弱リンク）と同程度にしかならないように思えるが、これは誤り。
- 実際のところ、コンピューティングの分野においては、信頼性の低い基盤上にもっと信頼性の高いシステムを構築するという概念は、古くからある
  - 無線通信にある誤り訂正符号
    - 電波障害によって多少のビットが時おり狂うようなことがあっても、通信チャネルを通じてデジタルデータを正確に転送できる
  - TCP
    - IPではパケットのドロップ、遅延、重複、順序の変化などが生じる
    - TCPはより信頼性の高い転送レイヤをIP上で提供する。
    - TCPは失われたパケットが再送され、重複は排除され、送信された順序でパケットが再構成されることを保証する
- 高い信頼性を持つことができますが、その信頼性には上限が常にある
  - 無線通信にある誤り訂正符号
    - 通信チャンネルに通せるデータ量に限界がある
  - TCP
    - ネットワーク遅延などはなかったことにはできない。

### 8.2 信頼性の低いネットワーク

- シェアードナッシングな分散システムが通信する手段はネットワーク。
  - それ以外のシステムでは特別なハードや高価なモノが必要
  - 地理的に分散できるので支配的なアーキになった。
- 一般的に信頼性は低い。
  - イーサネットやインターネットは非同期パケットネットワーク
  - いつ到達するか、そもそも到達するかを保証しない。
  - 配信されたのかどうかさえ分からない。（レスポンスが消えてる場合と見分けがつかない）
  - 一般的にこれらのケースではタイムアウトで対処

#### 8.2.1 ネットワークのフォールトの実際

- 制御された環境でもフォールトが発生
  - 1企業が運営するデータセンターのような環境下でもネットワークのフォールトが月12回は発生するという研究あり。
  - 結局は人間の設定ミスなどのため、期待するほどフォールトは減らないとも。
- ソフトウェアでの対処の必要性
  - フォールトは避けられないためソフトウェアでの対処が必須
  - 対処とは耐えられなければならないということではなく、エラーをユーザに表示することが妥当であるケースも。

#### 8.2.2 フォールトの検出

- 自動検出は困難である。
  - ネットワークが不確実なので動作しているのか知ることは難しい。
  - たとえTCPによってパケットが配信されたことが確認されても、アプリケーションがその後クラッシュする可能性もある。
  - 成功を確認したい場合は、適切なレスポンスを受信する以外にない。
- エラーレスポンスが必ず受け取れるわけではない
  - 前提としては受け取れないケースを想定する必要がある。
  - 一般的にリトライを何回か行い、タイムアウトになるまで待つ。

#### 8.2.3 タイムアウトと限度のない遅延

- タイムアウトの長さは判断の速さとのトレードオフ
- システムが高負荷に苦しんでいるだけの場合、タイムアウトと早期判断してしまうのは問題
  - 落ちているとみなすと残りのノードで処理するためにさらに高負荷に苦しむ
- 理想的なタイムアウト「2d + r」を保証するシステムはない

##### 8.2.3.1 ネットワークの輻輳とキューイング

- パケット遅延変動の主要因はキューイング
  - スイッチのキューが埋まってしまうことはネットワークに問題がなくとも発生
  - CPUがビジーな場合OSによるキューイングもある
  - VMの場合はさらにそこでもバッファリング（キューイング）される
  - TCPは自身の送信レートを制限するため、送信側でもキューイングは発生しうる（フロー制御）
- 他の要因
  - さらにTCPレベルのタイムアウトはアプリケーション側から区別できないため、これも遅延になる
- UDPとの比較
  - レイテンシに敏感なアプリケーションの場合UDPを使うものもある
  - 失われた時間は例えば音声の場合無音で埋められる
  - この場合、リトライは人間のレイヤで行われる（もう一度お願いします）
- リソースは一般的に共有されているケースも
  - パブリッククラウドやマルチテナントデータセンターの場合
  - この場合は他の利用者の使用状況は制御できず知ることもできないため、より計算することが困難
- タイムアウトの決め方
  - 経験的に決めるしかない。
  - 期待値を決定しアプリケーションの性格を考慮にいれて、適切なトレードオフを判断
  - さらに一定のタイムアウトではなく変動（ジッター）を計測して、自動的にタイムアウトを調整する方法も。

#### 8.2.4 同期ネットワークと非同期ネットワーク

- 同期ネットワークの例
  - 旧来の固定電話回線で、回路交換ネットワークとも。
  - きわめて信頼性が高く、遅延や断線はまれ。
  - 接続時に双方に固定レートが保証されるため、キューイングの影響はない。
  - 接続が確立されている間は他社が利用できない。

##### 8.2.4.1 単純に、ネットワークの遅延を予測できるようにはできないのか？

- TCPはこれらとは大きく違い、利用できる帯域は機会があれば利用し、アイドル状態ではまったく利用しない
- なぜ回線交換型のネットワークではなくパケット交換プロトコルなのか
  - バースト性を持つトラフィックに最適化されているため
  - 様々な用途に無駄なく帯域を使える
- 回路交換とパケット交換のハイブリット型
  - ATMでは使用されており、これを構築しようという試みもあった。
  - しかしキューイングの必要性が低いだけで、完全には無くならない
  - くわえてISP事業者間で合意し、BGP(プロトコル)で接続を確立するなどが必要で有効にすることが難しい。
- より一般化すれば帯域を動的にすることは、リソースを動的にすること
  - CPUリソースを動的にすることとも同じこと。
  - リソースを静的にパーティショニングできる場合、遅延などを保証しやすくなるが、利用効率は下がる（コストが上昇する）

### 8.3 信頼性の低いクロック

- 分散システムにおいては時間は複雑なもの
  - 通信は瞬時に行われるものではないため
  - それぞれのマシンは完璧に時刻が正しいわけではない
  - 一般的にはNTPサーバーである程度同期させている。

#### 8.3.1 単調増加のクロックと時刻のクロック

##### 8.3.1.1 時刻のクロック

- 時刻のクロックが行うのは現在の日付時刻を返すこと
- これは通常NTPと同期される。

##### 8.3.1.2 単調増加のクロック

- 単調増加のクロックは、タイムアウトやサービスのレスポンスタイムなど、期間（時間の間隔）を計測するのに適したもの
- 単調増加クロックの絶対値には意味はない
  - 特に、別々のコンピュータの単調増加のクロックの値が意味するものは異なっている
- 複数のCPUソケットを持つサーバー上では、CPUごとに個別のタイマーがあり、OSが埋め合わせているものの鵜呑みにしない方が賢明
- クロックのslewing
  - NTPはマシン自身が早く・遅く進んでいることを検出した場合、単調増加のクロックを進ませる頻度を調整することがある
  - NTPは単調増加のクロックを前後の時間にジャンプさせることはできない
- 単調増加のクロックの解像度
  - 解像度は非常に高く、ほとんどのシステムでは時間の間隔をマイクロ秒あるいはそれ以下で計測
- 分散システムでは、経過時間（たとえばタイムアウト）を計測する上で単調増加のクロックを使っても問題ない

#### 8.3.2 クロックの同期と正確性

- 我々が望むような信頼性や正確性は実際には持っていない。
  - 水晶発振器はそれほど正確ではなく、変動する（温度などに依存）
  - Google はサーバーに200ppmの変動を見込んでおり、1日1回に同期を取り戻す場合、17秒に相当する。
  - TPの同期の精度はネットワークの遅延に制限される、また通信できなくなる場合もある
  - NTPサーバーによっては時刻が狂っていたり設定に問題があり、複数のサーバーに問い合わせてはずれ値を無視している
  - 閏秒を考慮に入れずに設計されたシステムではタイミングの前提が崩れる
- リソースに十分投資すれば正確にできる事例はある
  - 欧州第2 次金融商品市場指令のドラフトでは、100msec以内の動機を求めている
  - こういった精度はGPS受信機、Precision Time Protocol（PTP）[52]、そして注意深いデプロイとモニタリングによって実現可能

#### 8.3.3 同期クロックへの依存

- ネットワークと同様、クロックに対してもソフトウェアでの対処が必要
  - クロックもほとんどの時間はうまく動作するが、時折問題が発生するため
- ネットワークと違いそのミスに気づかれにくい
  - 徐々に実際の時間からずれていく
  - 劇的なクラッシュではなく、きづかないほどわずかにデータが損失する

##### 8.3.3.1 順序関係を持つイベントのタイムスタンプ

- LWW(last write wins)がタイムスタンプを基準としていると厳しい
- 「最近」の定義はローカルの時刻のクロックに依存するものであり、不正確であることを認識しておくことが重要
- 論理クロックはそういった理由から、イベントの順序づけに関しては安全性が高い選択肢

##### 8.3.3.2 クロックの値には信頼区間がある

- 誤差は簡単に100ミリ以上になることから、読み出したクロックの値をある時刻を指すものと考えるのは理にかなってない
- むしろそれらはある信頼区間を持つような形。
- 不確実性が公開されていれば、信頼区間を得られるが一般的に公開されていない。
  - 例外としては、Google のSpanner におけるTrueTime APIがあり、明示的にローカルのクロックの信頼区間を報告
  - 信頼区間を問い合わせると、[earliest, latest] という2 つの値が得られます。
  - これらはありえる最も早いタイムスタンプと、ありえる最も遅いタイムスタンプ

##### 8.3.3.3 グローバルなスナップショットのための同期クロック

2023-01-31 : p.319(343/660)まで +3
2023-01-18 : p.316(340/660)まで +15
2023-01-17 : p.301(325/660)まで +0
2023-01-16 : p.301(325/660)まで +0
2023-01-15 : p.301(325/660)まで +0
2023-01-14 : p.301(325/660)まで +33
2023-01-13 : p.268(292/660)まで +0
2023-01-12 : p.268(292/660)まで +12
2023-01-11 : p.256(280/660)まで +30
2023-01-10 : p.226(250/660)まで +30
2023-01-09 : p.196(220/660)まで +30
2023-01-08 : p.166(190/660)まで +33
2023-01-06 : p.133(157/660)まで +11
2023-01-05 : p.122(146/660)まで +15
2023-01-04 : p.107(131/660)まで
