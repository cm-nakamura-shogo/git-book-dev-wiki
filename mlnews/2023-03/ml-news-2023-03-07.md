### 2023-03-07のまとめ

#### ----- Google Cloud -----

- なし

#### ----- AWS -----

- [【ブログ】SageMaker Trainingで大規模言語モデル(LLM)の学習を成功させるためのヒントとベストプラクティス](https://aws.amazon.com/jp/blogs/machine-learning/training-large-language-models-on-amazon-sagemaker-best-practices/)
  - 正攻法でガチ学習する場合の話が書いてあります。
- [【アプデ】Forecastが251か国の休日データをサポートするようになりました](https://aws.amazon.com/jp/about-aws/whats-new/2023/02/amazon-forecast-built-in-holiday-data-251-countries/)
  - そもそも日本はもともと対応されてたようです。

#### ----- その他 -----

- なし

#### ---- [Weekly Kaggle News #168](https://weeklykagglenews.substack.com/p/weekly-kaggle-news-168) -----

- [Kaggleが「Models」ページを公開](https://www.kaggle.com/models)
  - TensorFlow Hub と連携し、約 2000 モデルが整備されているらしい
- [深層学習用 GPU の比較記事](https://timdettmers.com/2023/01/30/which-gpu-for-deep-learning/)
  - めちゃめちゃ詳しく書いてあり参考になります。
- [「OTTO – Multi-Objective Recommender System」のコンペ概要や上位解法を紹介](https://medium.com/towards-data-science/building-a-recommender-system-using-machine-learning-2eefba9a692e)
  - OTTOは多目的レコメンドシステムで、Click、カート追加、注文の３つを予測するタスク
  - 一般的なベースラインは候補生成と再ランクの２段階
    - 今日のZOZOTOWNの事例もこの２ステージだったので一般的なのだろう
    - 選択するアイテム数が多いことが要因らしい
  - 1ステージ目はアイテム削減が目的のためシンプルであり、Co-Visitation行列などが使用される
  - 2ステージ目の再ランキングにはGBDT(勾配ブースティング決定, xgboostやlightGBMなど)が使用
    - そもそもXGBRankerやLGBMRankerというものがライブラリにある
  - 今後も「[Kaggle Blueprints](https://medium.com/@iamleonie/list/the-kaggle-blueprints-unlocking-winning-approaches-79c4222f75fb)」として、さまざまなコンペを紹介予定らしい
- [ディープラーニングによる自然言語処理が5/8に発売](https://www.kyoritsu-pub.co.jp/book/b10030620.html)
  - 事前学習済み言語モデル「LUKE」の著者でもある山田育矢さんらの書籍