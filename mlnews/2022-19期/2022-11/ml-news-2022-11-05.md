
### 2022-11-05

#### Google Cloud

- Vertex AIのアップデート
  - https://cloud.google.com/release-notes#November_04_2022
  - Vertex AI Predictionで、A2 machine typeがserve predictionsに使用可能になった。
  - Vertex ML Metadataで、フィルタリングが可能に。

#### AWS

- SageMakerでTensorFlowのオブジェクト検出のビルトインアルゴリズムの提供開始
  - https://aws.amazon.com/jp/blogs/machine-learning/transfer-learning-for-tensorflow-object-detection-models-in-amazon-sagemaker/
  - あれ？今までなかったんや？という気持ちになっている。
  - 例としているのはResNet50だが、対応モデルは以下に記載されている。
    - https://docs.aws.amazon.com/sagemaker/latest/dg/object-detection-tensorflow-Models.html
  - YOLOとかはない様子（なぜだろう？）
- SageMakerでTensorFlowのテキスト分類のビルトインアルゴリズムの提供開始
  - https://aws.amazon.com/jp/blogs/machine-learning/transfer-learning-for-tensorflow-text-classification-models-in-amazon-sagemaker/
  - 上記と同様。対応モデルは以下に記載。
    - https://docs.aws.amazon.com/sagemaker/latest/dg/text-classification-tensorflow-Models.html
- GitHubのビルド済みサンプルフローを使って、Data Wranglerを理解するブログ
  - https://aws.amazon.com/jp/blogs/machine-learning/use-github-samples-with-amazon-sagemaker-data-wrangler/
- SageMaker Deep Learning Container(DLC)の紹介
  - https://aws.amazon.com/jp/blogs/machine-learning/deploy-bloom-176b-and-opt-30b-on-amazon-sagemaker-with-large-model-inference-deep-learning-containers-and-deepspeed/
  - 大規模モデルの推論を数分で開始するための機能。
  - 大規模とはGPUメモリが数百必要な事例の話。

#### その他

- Kaggle新コンペ「OTTO – Multi-Objective Recommender System」が11/1に開始
  - https://www.kaggle.com/competitions/otto-recommender-system
  - eコマースのレコメンドのコンペ。
  - クリック・カートに入れる・購入するなど複数のアクションに対する多目的最適化タスク。
  - データセットはjsonlの構造化データのみで、12GB程度。
- Kaggle Days Championshipの１位解法概要が公開
  - https://twitter.com/tkm2261/status/1586641230842236929
  - distilroberta-baseのモデルとLightGBMを組み合わせたアンサンブル。
  - LightGBM自体のスコアが低くてもアンサンブルする意味はあるみたい。
  - オーバーフィットしやすいデータの場合、おおきなBERT系のモデル使ってもうまくいかないらしい。
- Nishikaのボケ判定AIの１位解法
  - https://qiita.com/z-lai/items/2d51ce123d77ad7100e8
  - 笑いを２種類に分別しているのが面白い。
    - 画像とテキストの結びつけが面白いことにより成立するボケ (multimodal boke)
    - 画像が適当だけどテキスト単体が面白いことにより成立するボケ (unimodal boke)
  - 今年３月に公開されたJGLUEの論文を参考にベースのNLPモデルを選定
    - nlp-waseda/robertaが結構良さげに見える
  - largeは学習が不安定なのでハイパーパラメータの細かい調整が必要など書いてあり参考になる。
  - 学習を安定化させるTipsとしては以下が挙げられている。
    - weight decayとwarmup stepsは必ず入れる
    - 事前学習モデル部分の重みに対する学習率を下げる（もしくはフリーズ）
    - 相対的に分類ヘッドの学習率を上げる
    - 大きなバッチサイズで学習する
      - これはメモリが許せばだが、まあ想定の範囲内
- Google Smartphone Decimeter Challengeの2nd place solution
  - https://www.youtube.com/watch?v=C-lSXptkHOM
  - やっぱしカルマンフィルタが登場してる。
