
## AWS

### [2023-06-02 Fraud Detector、Amazon EventBridgeによるイベントオーケストレーションの提供を発表](https://aws.amazon.com/jp/about-aws/whats-new/2023/06/amazon-fraud-detector-event-orchestration-eventbridge/)

- 様々なビジネスニーズに対してAFDからのイベントをオーケストレーションすることができるように
- 例えば、Amazon Simple Notification Serviceを使用して関係者に通知を送信し、特定の不正イベントが発生した際にタイムリーに対応するなど

## Google

### [2023-06-01 Vertex AIバッチ予測リクエストの入力または出力として、マルチリージョンのBigQueryテーブルを指定できるように](https://cloud.google.com/release-notes#June_01_2023)

## その他

### [2023-05-14 FLAIR: オンラインで会話型ボットを検出するためのフレームワーク](https://elith.substack.com/i/121117725/論文)

- 人間とボットを効果的に区別するための質問シナリオ
  - 人間にとっては簡単だがボットにとっては難しいもの
  - ボットにとっては簡単だが人間にとっては難しいもの
- LLMの弱点
  - 文字のカウント、文字の置換、文字の配置
  - ランダム編集、ノイズ侵害
  - ASCIIアート
- LLMが得意なもの
  - 記憶、計算の一部
- 元論文
  - [[2305.06424] Bot or Human? Detecting ChatGPT Imposters with A Single Question](https://arxiv.org/abs/2305.06424)

### [2023-05-21 第1回 LLM 勉強会 - NII(国立情報学研究所)](http://llm-jp.nii.ac.jp/llm/2023/05/21/first-study-group.html)

- 特に以下の資料が正しくまとまっており有用
  - [Recent_LLMs.pptx - Google スライド](https://docs.google.com/presentation/d/178Nk6flxqS59E2J9SSj5ZhyZ8YD_3tWJ/edit#slide=id.p1)
- ここで言及されている各モデルのライセンス等のまとめも有用そう
  - [Mooler0410/LLMsPracticalGuide: A curated list of practical guide resources of LLMs (LLMs Tree, Examples, Papers)](https://github.com/Mooler0410/LLMsPracticalGuide#Usage-and-Restrictions)

### [2023-05-27 オープンLLMリーダーボードで上位を占めるLLM、Falcon-40B & 7Bをリリース](https://huggingface.co/tiiuae/falcon-40b)

- リリース当初は商用利用には報酬が必要であったが、Apache 2.0化された
- LLaMAの代替なるかといったところ（LLaMAは7B～65B）

### [2023-05-29 SEAHORSE: 多面的に要約を評価するツール・大規模データセット](https://twitter.com/AIBoom_net/status/1663043297596813319)

- 6つの軸に沿った評価済み要約96k個のデータセットで、「要約システム」や「要約そのもの」を評価する技術に貢献する
- 要約を評価する軸は以下の6つ。
  - 理解がしやすい、繰り返しになっていない、文法が正しい、正確、重要なポイントを抽出している、簡潔である
- 日本語訳して流用すると良いかも

### [2023-06-01 生成系AI/LLM に関する 注目アップデート ~MS Build 2023 編~ - Speaker Deck](https://speakerdeck.com/yujioshima/llm-niguan-suru-zhu-mu-atupudeto-ms-build-2023-bian)

- こちら社内文書に基づくChatBotの作成に有用そうでした。
- MS Build 2023のまとめから始まり、Cognitive Search + LLMでRetrieval Augmented Generationを実現する方法、
- 実際にメルカリ社内での活用方法の紹介があります。
- 個人的にクエリとドキュメントは類似性があるか疑問に思っており、
- そこも最後の活用方法でTwo tower modelで解決しているところが面白かったです。
- Two tower modelはVertex AI Matching Engineでも実現できそう
  - [Two-Tower ModelでEmbeddingsを作成してMatching Engineで法人名寄せを試す（前半）｜Koji Iino](https://note.com/lizefield/n/n67eb32d2c1a4)

### [2023-06-03 QLoRA: メモリの消費量を激減させつつ少ないデータでトレーニングできる手法](https://gigazine.net/news/20230603-qlora-finetuning-llm/)

- LoRAでは、元のモデルのパラメーター行列を低ランク近似した新たな行列をトレーニング対象にすることで、トレーニングに必要なメモリの消費量を削減
  - [[2106.09685] LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)
- このLoRAをベースに、追加で3つのテクニックを利用することで650億(65B)パラメーターのモデルを48GBしかメモリを搭載していないGPUでトレーニング可能に
- 学習も24時間のトレーニングでChatGPTの99.3%に匹敵する性能を引き出すことに成功
- ３つのテクニック
  - NF4での量子化 : QLoRAでは4bitで量子化を行う
  - 二重量子化 : 量子化の際に用いる定数についても量子化を行う
  - ページ最適化 : GPUメモリが上限に達した際に、通常のメモリへとデータを退避させて計算に必要なメモリを確保（ピークの使用率を抑える）
