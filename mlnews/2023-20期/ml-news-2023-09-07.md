### ml-news-2023-09-07

- [2023-09-05 Amazon Personalize、列の制限を拡張して実装を簡素化](https://aws.amazon.com/jp/about-aws/whats-new/2023/09/amazon-personalize-implementation-column-limits/)
  - アイテムデータセットに2倍の列数（100列）、ユーザーデータセットに5倍の列数（25列）を持ち込むことができるように
- [2023-09-05 Amazon SageMakerの地理空間機能がGPUベースのインスタンスでノートブックをサポート](https://aws.amazon.com/jp/about-aws/whats-new/2023/09/amazon-sagemaker-geospatial-notebook-gpu-instances/)
  - 書いてある通り
- [2023-09-05 AWS Compute OptimizerがG4dnおよびP3インスタンスをサポート](https://aws.amazon.com/jp/about-aws/whats-new/2023/09/aws-compute-optimizer-rightsizing-g4dn-p3-instances/)
  - EC2インスタンスのライツサイジング推奨を使用して、既存のAcceleratedコンピューティングリソースに適切なAcceleratedコンピューティングインスタンスオプション、インスタンスタイプ、GPU数、およびプロビジョニングIOPS設定を特定できるように
- [2023-09-06 TIIのFalcon-180BがHuggingFaceで公開](https://huggingface.co/blog/falcon-180b)
  - こちらは5月末に公開されたFalcon-40Bのさらなる大規模版
  - MMLUではLlama 2(70B)やOpenAIのGPT-3.5に匹敵する性能
  - HuggingFaceのSpacesで試すことが可能。日本語も理解・回答してくれる（多少不自然な場合もある）
    - [Falcon-180B Demo - a Hugging Face Space by tiiuae](https://huggingface.co/spaces/tiiuae/falcon-180b-demo)
  - 40B,7BはApach 2.0 Licenseであったが、こちらは特殊なライセンスとなっておりその点は注意
    - [LICENSE.txt · tiiuae/falcon-180b-license at main](https://huggingface.co/spaces/tiiuae/falcon-180b-license/blob/main/LICENSE.txt)
- [2023-09-06 Amazon Kendra で独自文書に対するセマンティック検索（自然言語での検索）を実現する - Taste of Tech Topics](https://acro-engineer.hatenablog.com/entry/2023/09/06/110000)
  - Kendraの紹介記事で画面や具体的な使用例が示されており分かりやすい
- [2023-09-06 「機械学習〜推薦システムを題材に〜」講義資料(生成AIの話題も含む)｜masa_kazama](https://note.com/masa_kazama/n/na630a2f46058)
  - 概要推薦(パーソナライズなし)、コンテンツベース、協調フィルタリングなどそれぞれを詳しく説明
  - 単にアイテムを推薦するたけでなくユーザー毎にサムネイルを最適化している点などがあり興味深かった
  - part2では新しいLLMを使った手法にも触れられている（微分可能な検索インデックスなど）
- [2023-08-26 「基礎からの新しいストレージ入門」は、2023年夏の課題図書](https://zenn.dev/tzkoba/articles/88ac78ad384d6c)
- [2021-07-08 マイクロサービスの認証・認可とJWT / Authentication and Authorization in Microservices and JWT - Speaker Deck](https://speakerdeck.com/oracle4engineer/authentication-and-authorization-in-microservices-and-jwt)
  - JWTどころかOAuth2、OpenID Connectについても分かりやすい
  - OAuth2は認可プロトコルの話で、認可コードグラントが推奨方式。認証目的であればOpenID Connectを使用する。
  - OAuth2ではアクセストークンが認可サーバーから発行されるが、認可コードグラント方式では認可コードを使ってクライアントがアクセストークンを取得する
  - OpenID ConnectにはIDトークンが含まれ、iss:誰が(IdP)、sub:誰を(ex. エンドユーザ)、aud:誰のために(Relying Party)などの認証に必要な項目を含む
  - アクセストークンとIDトークンの違いはaudが異なり、アクセストークンはリソースサーバーが、IDトークンはクライアント(Relying Party)がそれぞれaudとなる
  - 認可コードフローにより、アクセストークンとIDトークンをクライアント(Relying Party)で取得する
- [2023-09-06 Amazon SageMaker InferenceがPyTorchのMME(マルチモデルエンドポイント)をサポート](https://aws.amazon.com/jp/about-aws/whats-new/2023/09/amazon-sagemaker-inference-multi-model-endpoints-pytorch/)
  - TorchServe を使用してデプロイされた PyTorch モデルをサポートするように
  - [Run multiple generative AI models on GPU using Amazon SageMaker multi-model endpoints with TorchServe and save up to 75% in inference costs | AWS Machine Learning Blog](https://aws.amazon.com/jp/blogs/machine-learning/run-multiple-generative-ai-models-on-gpu-using-amazon-sagemaker-multi-model-endpoints-with-torchserve-and-save-up-to-75-in-inference-costs/)
- [2023-09-06 Amazon SageMaker JumpStartを使用して、Llama 2モデルをfine-tuningすることが可能に](https://aws.amazon.com/jp/blogs/machine-learning/fine-tune-llama-2-for-text-generation-on-amazon-sagemaker-jumpstart/)
  - LLMは巨大なモデルであるため、fine-tuningにもLoRAや量子化、データ並列化トレーニング(FSDP)などの設定がある
  - モデル規模により使用可能なfine-tuning方法に違いがある
  - fine-tuningにかかる時間なども記載されており、70Bで最大8時間程度の計測結果がある
- [2023-09-06 Amazon SageMaker JumpStartを使用したGenerative AIとRAGによる安全なエンタープライズアプリケーションの構築](https://aws.amazon.com/jp/blogs/machine-learning/build-a-secure-enterprise-application-with-generative-ai-and-rag-using-amazon-sagemaker-jumpstart/)
  - RAGにOpenSearchを使うアーキテクチャ例
- [2023-09-06 TSMixer：時系列予測のためのAll-MLPアーキテクチャ – Google Research Blog](https://blog.research.google/2023/09/tsmixer-all-mlp-architecture-for-time.html)
  - 長期予測ベンチマークで優れたパフォーマンスを発揮する高度な多変量モデルであるTSMixerを開発
  - 実証結果は、TSMixerが、PatchTST、Fedformer、Autoformer、DeepAR、TFTなどの最新モデルを凌駕する
  - [google-research/tsmixer at master · google-research/google-research · GitHub](https://github.com/google-research/google-research/tree/master/tsmixer)