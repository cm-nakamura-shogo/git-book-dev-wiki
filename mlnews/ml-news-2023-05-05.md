### 2023-05-05のまとめ

## AWS

### [2023-05-04 Amazon Forecastのデプロイ自動化をコードなしで反復する方法 (AWS)](https://aws.amazon.com/jp/blogs/machine-learning/automate-the-deployment-of-an-amazon-forecast-time-series-forecasting-model/)

- StepFunctionsなどの例が示されていて分かりやすいかも

### [2023-05-04 SageMaker JumpStartを使ってGenerative AIをAWSで始める方法 (AWS)](https://aws.amazon.com/jp/blogs/machine-learning/get-started-with-generative-ai-on-aws-using-amazon-sagemaker-jumpstart/)

### [2023-05-03 画像や動画に対するコンテンツモデレーションの精度を改善 (AWS)](https://aws.amazon.com/jp/about-aws/whats-new/2023/05/amazon-rekognition-content-moderation-images-videos/)

### [2023-05-03 SageMakerドメイン復旧についてのソリューション事例 (AWS)](https://aws.amazon.com/jp/blogs/machine-learning/implement-backup-and-recovery-using-an-event-driven-serverless-architecture-with-amazon-sagemaker-studio/)

### [2023-05-03 SageMakerのデプロイインスタンスにml.inf2とml.trn1が対応 (AWS)](https://aws.amazon.com/jp/about-aws/whats-new/2023/05/sagemaker-ml-inf2-ml-trn1-instances-model-deployment/)

- 公式ブログでも
  - [2023-05-04 SageMaker上でInferentia2とTrainiumを使ってGenerative AIの推論を低コスト・高パフォーマンスで実現する方法](https://aws.amazon.com/jp/blogs/machine-learning/achieve-high-performance-with-lowest-cost-for-generative-ai-inference-using-aws-inferentia2-and-aws-trainium-on-amazon-sagemaker/)

### [2023-05-03 LLMの独自コンテキスト拡張(RAG)にKendraを使用する例の紹介 (AWS)](https://aws.amazon.com/jp/blogs/machine-learning/quickly-build-high-accuracy-generative-ai-applications-on-enterprise-data-using-amazon-kendra-langchain-and-large-language-models/)

### [2023-05-03 Gravitonプロセッサに対するPyTorch 2.0の推論性能の最適化を実現 (AWS)](https://aws.amazon.com/jp/blogs/machine-learning/optimized-pytorch-2-0-inference-with-aws-graviton-processors/)

### [2023-05-02 Sagemaker Autopilotがサンプルの重みと追加の目的指標を持つトレーニングモデルをサポート (AWS)](https://awsapichanges.info/archive/changes/bd55ca-api.sagemaker.html)

## Google Cloud

### [2023-05-03 BigQueryのテーブルクローン機能がGA (Google Cloud)](https://cloud.google.com/bigquery/docs/release-notes#May_03_2023)

## その他

### [2023-05-04 CLIP ViT-L/14がリリース (Hugging Face)](https://huggingface.co/laion/CLIP-ViT-L-14-DataComp.XL-s13B-b90K)

- ImageNetで79.2%のゼロショット精度を実現したCLIP ViT-L/14がリリース
- CLIPを大きく上回り、LAION-2Bで学習させた大型モデル（ViT-g/14）を凌駕

### [2023-05-04 StarCoder: コード生成用の最先端のLLM (Hugging Face)](https://huggingface.co/blog/starcoder)

- ベンチマークでは、OpenAIのcode-cushman-001 (12B)モデルも凌駕している
- VSCodeの拡張機能「HF Code Autocomplete」として使える（APIキーとかは必要そう）
  - [HF Code Autocomplete - Visual Studio Marketplace](https://marketplace.visualstudio.com/items?itemName=HuggingFace.huggingface-vscode)

### [2023-05-04 MaMMUT: マルチモーダルのための新しい基盤モデルアーキテクチャの研究 (Google Research)](https://ai.googleblog.com/2023/05/mammut-simple-vision-encoder-text.html)

- 視覚言語基盤モデルは、一般的にはCLIPなどに代表される対照学習と次トークン予測の２つの主要なシナリオが一般的
- 前者と後者で得意な下流タスクがことなるため課題
- MaMMUTはこれを解決するアーキテクチャとなっており、更に先行研究よりも多くの画像フレームを扱えるため、動画処理にもメリットがある
- 要するにImage側のエンコーダ結果をテキスト側のデコーダのCross Attentionとして使うところがポイントっぽい

### [2023-05-03 SpikeGPT: より軽量な環境で動かすことが可能な言語モデルの可能性](https://zenn.dev/octu0/scraps/0078b6e9925674)
